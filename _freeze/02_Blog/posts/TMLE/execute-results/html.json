{
  "hash": "4e7260632c377c8d98e51a966258a0a5",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Targeted Maximum Likelihood Estimation / Targeted Minimum Loss-based Estimation (TMLE)\"\ndate: 2024-02-22\ncategories: [Causal Inference]\nexecute: \n  cache: false\n---\n\n\n\n# TMLEの手順\n\n参考ページ：https://www.khstats.com/blog/tmle/tutorial-pt2\n\n## 下準備\n\n### ライブラリなどの読み込み\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(sl3)\nlibrary(tmle3)\nlibrary(future)\n\nkable <- partial(\n  knitr::kable,\n  digits = 3\n)\n\nplan(multisession, workers = 8)\n\nset.seed(7)\n```\n:::\n\n\n\n### データ作成\n\n- 元のサイトでは$Y$が2値変数になっていて、推定が真値と一致しているかどうかの評価が難しいので、ここでは$Y$を連続変数としている。\n\n- ATEの真値は0.3に設定\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngenerate_data <- function(n){ \n    W1 <- rbinom(n, size=1, prob=0.2) # binary confounder\n    W2 <- rbinom(n, size=1, prob=0.5) # binary confounder\n    W3 <- round(runif(n, min=2, max=7)) # continuous confounder\n    W4 <- round(runif(n, min=0, max=4)) # continuous confounder\n    A  <- rbinom(n, size=1, prob= plogis(-0.2 + 0.2*W2 + log(0.1*W3) + 0.3*W4 + 0.2*W1*W4)) # binary treatment depends on confounders\n    Y <- -1 + 0.3*A - 0.1*W1 + 0.2*W2 + 0.3*W3 - 0.1*W4 + sin(0.1*W2*W4) # continuous outcome depends on confounders\n    return(tibble(Y, W1, W2, W3, W4, A))\n}\n\nn <- 1000\ndat_obs <- generate_data(n) # generate a data set with n observations\n\n\ndat_obs |> \n  summarise(samplemean = mean(Y), .by = A)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 2\n      A samplemean\n  <int>      <dbl>\n1     1      0.686\n2     0      0.237\n```\n\n\n:::\n:::\n\n\n\n### 機械学習ライブラリの設定\n\n- glm、Lasso、Random forest、Multivariate adaptive regression splineをスタッキング\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsl_libs <-\n  Lrnr_sl$new(\n    learners = Stack$new(\n      Lrnr_glm$new(), \n      Lrnr_glmnet$new(alpha = 1), \n      Lrnr_ranger$new(num.trees = 2000, max.depth = 3), \n      Lrnr_earth$new()\n    )\n  )\n```\n:::\n\n\n## Step1：アウトカムの予測\n\n### アウトカムの条件付き期待値関数を推定\n\n$$\n{\\mathrm E}[Y | A, W]\n$$\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntask <- \n  sl3_Task$new(\n    dat_obs, \n    covariates = select(dat_obs, !Y) |> names(), \n    outcome = 'Y', \n    outcome_type = 'continuous', \n    folds = 8L\n  )\n\ntask_A1 <- \n  sl3_Task$new(\n    dat_obs |> mutate(A = 1), \n    covariates = select(dat_obs, !Y) |> names(), \n    outcome = 'Y', \n    outcome_type = 'continuous', \n    folds = 8L\n  )\n\ntask_A0 <- \n  sl3_Task$new(\n    dat_obs |> mutate(A = 0), \n    covariates = select(dat_obs, !Y) |> names(), \n    outcome = 'Y', \n    outcome_type = 'continuous', \n    folds = 8L\n  )\n\n# 全サンプルで学習\nsl_fit <- sl_libs$train(task)\n```\n:::\n\n\n- 以下の3つの予測値を算出\n\n- $A := 1$は全サンプルでAを1にする（$A = 1$はサンプルのうちのAが1となる部分集団）\n\n\\begin{align}\n&{\\mathrm E}[Y | A, W] \\\\\n&{\\mathrm E}[Y | A := 1, W] \\\\\n&{\\mathrm E}[Y | A := 0, W]\n\\end{align}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat_tmle1 <- \n  dat_obs |> \n  mutate(\n    # 観測サンプルについての予測値\n    Q_A = sl_fit$predict(task),\n    # 全てのサンプルでA = 1に固定したときの予測値\n    Q_A1 = sl_fit$predict(task_A1),\n    # 全てのサンプルでA = 0に固定した時の予測値\n    Q_A0 = sl_fit$predict(task_A0)\n  )\n```\n:::\n\n\n\n- standardization（g-computation）によるATE\n\n$$\nATE_{g \\mathrm{-}comp} = {\\mathrm E}[ {\\mathrm E}[Y | A := 1, W] - {\\mathrm E}[Y | A := 0, W]]\n$$\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat_tmle1 |> \n  summarise(ATE = mean(Q_A1 - Q_A0))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 1\n    ATE\n  <dbl>\n1 0.300\n```\n\n\n:::\n:::\n\n\n\n## Step2：処置確率（傾向スコア）の予測\n\n- 傾向スコアを機械学習モデルにより予測\n\n$$\n\\mathrm{Pr}(A = 1 | W)\n$$\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntask_g <- \n  sl3_Task$new(\n    data = dat_obs, \n    covariates = select(dat_obs, !c(Y, A)) |> names(), \n    outcome = 'A', \n    outcome_type = 'binomial', \n    folds = 8\n  )\n\nsl_fit_g <- sl_libs$train(task_g)\n```\n:::\n\n\n\n### Clever Covariateの作成\n\n- 傾向スコアからClever Covariateと呼ばれる情報を作成（IPWに似ている）\n\n\\begin{align}\n&H(A,W) &= \\frac{A}{\\mathrm{Pr}(A = 1 | W)} - \\frac{1 - A}{1 - \\mathrm{Pr}(A = 1 | W)} \\\\\n&H(1,W) &= \\frac{A}{\\mathrm{Pr}(A = 1 | W)} \\\\\n&H(0,W) &= - \\frac{1 - A}{1 - \\mathrm{Pr}(A = 1 | W)} \n\\end{align}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat_tmle2 <- \n  dat_tmle1 |> \n  mutate(\n    # Propensity Scoreの予測\n    ps = sl_fit_g$predict(task_g),\n    # ipw (Inverse Probability Weight)\n    ipw = case_when(\n      A == 1 ~ 1 / ps, \n      A == 0 ~ 1 / (1 - ps)\n    ),\n    # Clever Covariates\n    H_A = case_when(\n      A == 1 ~ 1 / ps,\n      A == 0 ~ -1 / (1 - ps)\n    ),\n    H_A1 = case_when(\n      A == 1 ~ H_A,\n      A == 0 ~ 0\n    ),\n    H_A0 = case_when(\n      A == 1 ~ 0,\n      A == 0 ~ H_A\n    )\n  )\n```\n:::\n\n\n\n- IPWによるATE\n\n$$\nATE_{ipw} = {\\mathrm E}[\\frac{A}{\\mathrm{Pr}(A = 1 | W)}Y - \\frac{1 - A}{1 - \\mathrm{Pr}(A = 1 | W)}Y]\n$$\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat_tmle2 |> \n  summarise(CFmean = sum(Y*ipw) / sum(ipw), .by = A) |> \n  arrange(A) |> \n  summarise(ATE = diff(CFmean))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 1\n    ATE\n  <dbl>\n1 0.298\n```\n\n\n:::\n:::\n\n\n- Augumented IPWによるATE\n  - （ほんとは関数推定時にcross-fitをする）\n\n$$\nATE_{aipw} = \\mathrm E[\\mathrm E[Y | A := 1, W] - \\mathrm E[Y | A := 0, W] + \\frac{A}{\\mathrm Pr(A = 1 | W)}(Y - {\\mathrm E}[Y | A := 1, W]) - \\frac{1 - A}{1 - \\mathrm Pr(A = 1 | W)}(Y - \\mathrm E[Y | A := 0, W])]\n$$\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat_tmle2 |> \n  summarise(ATE = mean(Q_A1 - Q_A0 + ipw*A*(Y - Q_A1) - ipw*(1 - A)*(Y - Q_A0)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 1\n    ATE\n  <dbl>\n1 0.300\n```\n\n\n:::\n:::\n\n\n\n\n## Step3：変動パラメータの推定\n\n\n- AIPWの問題点：統計的最適化がターゲットのパラメータ（ATE）に対してではなく、母平均関数${\\mathrm E}[Y | A,W]$および傾向スコア関数$\\mathrm{Pr}(A = 1 | W)$のパラメータについて最適化されている点\n- 推定したいパラメータ（ATE）のEIF（Efficient Influence Function）を解くことがこのステップのポイントらしい\n- 具体的には、Step1で推定した$\\mathrm{E}[Y | A, W]$と、Step2で推定したClever Covariate$H(A, W)$を用いて、以下の回帰式の$\\epsilon$（変動パラメータ）を推定する\n\n$$\nY = \\mathrm{E}[Y | A, W] + \\epsilon H(A,W)\n$$\n\n- 切片が0で、Step1の推定値の係数を1に固定するために、-1と`offset`を利用する\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit <- glm(Y ~ -1 + offset(Q_A) + H_A, data = dat_tmle2, family = gaussian())\n```\n:::\n\n\n- 変動パラメータの推定値\n\n\n::: {.cell}\n\n```{.r .cell-code}\nepsilon <- coef(fit)\n\nepsilon\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        H_A \n6.40031e-07 \n```\n\n\n:::\n:::\n\n\n## Step4：アウトカムの予測値を更新\n\n- 推定したepsilonと$Y$の予測値をもとに、$Y$の予測値を更新\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat_tmle3 <- \n  dat_tmle2 |> \n  mutate(\n    Q_A_update = Q_A + epsilon*H_A,\n    Q_A1_update = Q_A1 + epsilon*H_A1,\n    Q_A0_update = Q_A0 + epsilon*H_A0,\n  )\n```\n:::\n\n\n\n## Step5：推定したい統計量を推定\n\n- 更新されたアウトカムの予測値を用いて、Standardizationの要領でATEを推定\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat_tmle3 |> \n  summarise(ATE = mean(Q_A1_update - Q_A0_update))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 1\n    ATE\n  <dbl>\n1 0.300\n```\n\n\n:::\n\n```{.r .cell-code}\nATE <- mean(dat_tmle3$Q_A1_update - dat_tmle3$Q_A0_update)\n```\n:::\n\n\n\n## Step6：標準誤差の推定\n\n- TMLEではbootstrapによらずとも標準誤差を算出できる（！）\n- まずは、Influence Functionを推定する\n- Influence Function：各サンプルがどれだけATEに影響をあたえるか？\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat_tmle4 <- \n  dat_tmle3 |> \n  mutate(\n    IF = (Y - Q_A_update)*H_A + Q_A1_update - Q_A0_update - ATE\n  )\n```\n:::\n\n\nATEの標準誤差はIFを用いて\n\n$$\nSE = \\sqrt{\\frac{\\mathrm{var}(IF)}{N}}\n$$\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat_tmle4 |> \n  summarise(SE = sqrt(var(IF) / 1000))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 1\n         SE\n      <dbl>\n1 0.0000217\n```\n\n\n:::\n:::\n\n\n\n\n# `tmle3`パッケージによる実行\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnode_list <- \n  list(\n    W = dat_obs |> select(!c(A, Y)) |> names(),\n    A = 'A',\n    Y = 'Y'\n  )\n    \n\ntmle3_fit <- \n  tmle3(\n    tmle_spec = tmle_ATE(treatment_level = 1, control_level = 0), \n    data = dat_obs, \n    node_list = node_list, \n    learner_list = list(A = sl_libs, Y = sl_libs)\n  )\n```\n:::\n\n\n大体正しく推定できている\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntmle3_fit\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nA tmle3_Fit that took 1 step(s)\n     type                param  init_est  tmle_est          se     lower\n   <char>               <char>     <num>     <num>       <num>     <num>\n1:    ATE ATE[Y_{A=1}-Y_{A=0}] 0.2999748 0.2996603 0.000206224 0.2992561\n       upper psi_transformed lower_transformed upper_transformed\n       <num>           <num>             <num>             <num>\n1: 0.3000645       0.2996603         0.2992561         0.3000645\n```\n\n\n:::\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}