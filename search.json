[
  {
    "objectID": "Causal_Inference/Causal_Decomposition.html",
    "href": "Causal_Inference/Causal_Decomposition.html",
    "title": "Causal Decomposition Analysis",
    "section": "",
    "text": "library(tidyverse)\nlibrary(cfdecomp)\nlibrary(gapclosing)\nlibrary(causal.decomp)\n\n\nd &lt;- \n  sMIDUS |&gt; \n  transmute(Y = health |&gt; as.numeric(),  # outcome\n            T = edu |&gt; as.numeric(),   # treatment (continuous)\n            T2 = edu |&gt; case_match(4:6 ~ 0,   # treatment (binary)\n                                   7:9 ~ 1,\n                                   .default = NA) |&gt; factor(),\n            X = racesex |&gt; factor(levels = c(\"1\", \"4\", \"2\", \"3\")),  # note!\n            L1 = lowchildSES |&gt; as.numeric(),\n            L2 = abuse |&gt; as.numeric(),\n            C1 = age |&gt; as.numeric(),\n            C2 = stroke |&gt; as.numeric(),\n            C3 = T2DM |&gt; as.numeric(),\n            C4 = heart |&gt; as.numeric()) |&gt; \n  mutate(across(L1:C4, \\(.x){.x - mean(.x, na.rm = TRUE)})) |&gt; \n  tibble()"
  },
  {
    "objectID": "Causal_Inference/Causal_Decomposition.html#前準備",
    "href": "Causal_Inference/Causal_Decomposition.html#前準備",
    "title": "Causal Decomposition Analysis",
    "section": "",
    "text": "library(tidyverse)\nlibrary(cfdecomp)\nlibrary(gapclosing)\nlibrary(causal.decomp)\n\n\nd &lt;- \n  sMIDUS |&gt; \n  transmute(Y = health |&gt; as.numeric(),  # outcome\n            T = edu |&gt; as.numeric(),   # treatment (continuous)\n            T2 = edu |&gt; case_match(4:6 ~ 0,   # treatment (binary)\n                                   7:9 ~ 1,\n                                   .default = NA) |&gt; factor(),\n            X = racesex |&gt; factor(levels = c(\"1\", \"4\", \"2\", \"3\")),  # note!\n            L1 = lowchildSES |&gt; as.numeric(),\n            L2 = abuse |&gt; as.numeric(),\n            C1 = age |&gt; as.numeric(),\n            C2 = stroke |&gt; as.numeric(),\n            C3 = T2DM |&gt; as.numeric(),\n            C4 = heart |&gt; as.numeric()) |&gt; \n  mutate(across(L1:C4, \\(.x){.x - mean(.x, na.rm = TRUE)})) |&gt; \n  tibble()"
  },
  {
    "objectID": "Causal_Inference/Causal_Decomposition.html#cfdecomp",
    "href": "Causal_Inference/Causal_Decomposition.html#cfdecomp",
    "title": "Causal Decomposition Analysis",
    "section": "cfdecomp",
    "text": "cfdecomp\n\nSudharsanan and Bijlsma (2021) の方法。mediatorの値をシミュレーションで複数生成するのが特徴\n\n\n# cfd.mean\nfit_cfdecomp &lt;-\n  cfdecomp::cfd.mean(\n    formula.y = 'Y ~ X + T + X:T + L1 + L2 + C1 + C2 + C3 + C4',\n    formula.m = 'T ~ X + C1 + C2 + C3 + C4',\n    mediator = 'T',\n    group = 'X',\n    data = d |&gt; data.frame(),\n    family.y = 'gaussian',\n    family.m = 'gaussian',\n    bs.size = 50,\n    mc.size = 10,\n    alpha = 0.05\n  )\n\n\nfit_cfdecomp\n\n$out_nc_m\n          1        4        2        3\n1  7.704393 5.878511 7.100601 6.334574\n2  7.684907 5.893482 7.078147 6.504993\n3  7.667103 5.792513 7.064230 6.331297\n4  7.691473 5.832407 7.094649 6.448269\n5  7.702502 5.867025 7.084150 6.302879\n6  7.707649 5.881564 7.073169 6.348328\n7  7.698524 5.863295 7.094253 6.304080\n8  7.665428 5.934105 7.077088 6.393080\n9  7.687736 5.849997 7.069869 6.413267\n10 7.691149 5.867068 7.097231 6.395584\n11 7.717062 5.875149 7.083784 6.311631\n12 7.714051 5.841621 7.081830 6.427463\n13 7.720328 5.865250 7.084813 6.451981\n14 7.716165 5.781157 7.065282 6.483936\n15 7.701326 5.931750 7.110469 6.480753\n16 7.713160 5.909803 7.113075 6.471447\n17 7.717832 5.793296 7.100073 6.395649\n18 7.721109 5.873599 7.059873 6.346348\n19 7.706348 5.847319 7.067505 6.412451\n20 7.711646 5.830050 7.082999 6.401420\n21 7.692526 5.825116 7.072620 6.470432\n22 7.720256 5.818086 7.109367 6.362887\n23 7.687258 5.858049 7.071935 6.427542\n24 7.779983 5.917717 7.064179 6.440181\n25 7.687712 5.880672 7.070538 6.392827\n26 7.681760 5.874938 7.072690 6.387883\n27 7.723142 5.922137 7.074596 6.458075\n28 7.745995 5.868351 7.109323 6.394674\n29 7.675111 5.874267 7.080646 6.400481\n30 7.704231 5.869453 7.084789 6.438085\n31 7.703909 5.828574 7.069695 6.400884\n32 7.723633 5.923871 7.101306 6.442652\n33 7.704270 5.940003 7.073209 6.348928\n34 7.690021 5.863546 7.141164 6.350132\n35 7.713714 5.935156 7.068561 6.379914\n36 7.701639 5.803481 7.090905 6.380944\n37 7.663258 5.898129 7.065360 6.451323\n38 7.700254 5.880185 7.038593 6.495530\n39 7.740043 5.857458 7.052444 6.229921\n40 7.675637 5.903854 7.053284 6.370314\n41 7.720521 5.804465 7.127913 6.454960\n42 7.722971 5.832164 7.059062 6.404134\n43 7.718257 5.865445 7.110773 6.377804\n44 7.710481 5.916549 7.056362 6.351512\n45 7.693268 5.862944 7.063951 6.447388\n46 7.678086 5.846925 7.073370 6.380035\n47 7.723755 5.911177 7.075948 6.456036\n48 7.677894 5.949973 7.048513 6.405711\n49 7.718471 5.880965 7.061658 6.505462\n50 7.676996 5.882907 7.099679 6.596813\n\n$out_cf_m\n          1        4        2        3\n1  7.695237 7.698664 7.714275 7.698376\n2  7.692037 7.735170 7.708061 7.714884\n3  7.662060 7.669484 7.678900 7.629026\n4  7.692625 7.730004 7.692944 7.727442\n5  7.703100 7.676401 7.707583 7.680340\n6  7.692301 7.704222 7.711041 7.705918\n7  7.694546 7.747794 7.717056 7.698930\n8  7.662324 7.658380 7.681429 7.643902\n9  7.672684 7.694050 7.695842 7.699512\n10 7.666396 7.665698 7.676986 7.634029\n11 7.731539 7.728737 7.725253 7.689596\n12 7.718414 7.677185 7.721105 7.718532\n13 7.727804 7.725323 7.732888 7.713343\n14 7.700645 7.691775 7.700034 7.693086\n15 7.693895 7.700015 7.690966 7.660751\n16 7.705284 7.730238 7.701869 7.703176\n17 7.715922 7.735389 7.729652 7.730373\n18 7.729834 7.708581 7.740596 7.716448\n19 7.727479 7.681980 7.725371 7.715341\n20 7.718810 7.699963 7.723112 7.709143\n21 7.690483 7.710145 7.714376 7.704448\n22 7.710002 7.708093 7.729129 7.701050\n23 7.676221 7.676947 7.689312 7.657328\n24 7.786247 7.790737 7.794220 7.774169\n25 7.686563 7.700556 7.700895 7.692829\n26 7.685353 7.692371 7.698384 7.675953\n27 7.725504 7.745133 7.735270 7.716700\n28 7.747605 7.728458 7.759898 7.731043\n29 7.679783 7.643081 7.686112 7.665649\n30 7.704130 7.699832 7.720783 7.721398\n31 7.701701 7.710872 7.713634 7.678175\n32 7.735713 7.726618 7.730834 7.721670\n33 7.696643 7.697963 7.702217 7.712556\n34 7.704079 7.707784 7.701126 7.679178\n35 7.716738 7.713484 7.721166 7.720744\n36 7.702610 7.693181 7.710453 7.682322\n37 7.686079 7.703241 7.703538 7.694242\n38 7.695140 7.706969 7.701521 7.704428\n39 7.726793 7.741675 7.727676 7.697694\n40 7.670291 7.660500 7.683376 7.666899\n41 7.713675 7.707159 7.724040 7.713228\n42 7.723779 7.761976 7.739969 7.705372\n43 7.712476 7.713428 7.715712 7.685084\n44 7.704671 7.722937 7.717555 7.698043\n45 7.684617 7.724854 7.705855 7.684188\n46 7.700869 7.696082 7.695639 7.658616\n47 7.735210 7.731111 7.732692 7.722269\n48 7.690006 7.728187 7.703978 7.687452\n49 7.710897 7.730958 7.717505 7.715660\n50 7.684738 7.694305 7.692742 7.681102\n\n$out_nc_quantile_m\n             1        4        2        3\n2.5%  7.665805 5.792689 7.049397 6.303149\n50%   7.704251 5.868902 7.075272 6.401152\n97.5% 7.744656 5.938913 7.124575 6.505357\n\n$out_cf_quantile_m\n             1        4        2        3\n2.5%  7.663240 7.658857 7.679469 7.636250\n50%   7.702156 7.707471 7.712338 7.699221\n97.5% 7.744930 7.758785 7.755555 7.730893\n\n$out_nc_y\n          1        4        2        3\n1  7.609364 6.718684 7.341827 7.002085\n2  7.628486 6.719177 7.294239 7.052794\n3  7.595029 6.716448 7.336332 6.978392\n4  7.636489 6.711188 7.305312 6.893564\n5  7.579541 6.660990 7.316580 6.975759\n6  7.580596 6.700170 7.288052 6.998277\n7  7.595183 6.711373 7.332799 7.012144\n8  7.551420 6.644580 7.306586 7.013259\n9  7.582326 6.707404 7.296872 6.987075\n10 7.589799 6.677915 7.328206 6.971000\n11 7.614199 6.720911 7.339720 6.868702\n12 7.586114 6.689092 7.351506 7.026881\n13 7.622678 6.682343 7.325961 6.977455\n14 7.603960 6.650643 7.338078 7.110627\n15 7.579252 6.724402 7.317658 6.975170\n16 7.611930 6.774817 7.335448 6.945007\n17 7.585327 6.647768 7.341476 6.935480\n18 7.576620 6.666223 7.322488 6.842111\n19 7.575377 6.692609 7.320514 7.008304\n20 7.642213 6.704353 7.360568 6.919576\n21 7.578390 6.697662 7.305599 7.009812\n22 7.623193 6.667674 7.323701 6.975625\n23 7.603659 6.720316 7.327473 6.977863\n24 7.624144 6.785678 7.330341 6.934198\n25 7.591184 6.740607 7.296366 7.087310\n26 7.580709 6.697542 7.305766 6.971515\n27 7.596846 6.736814 7.309232 6.977437\n28 7.626346 6.705490 7.365605 6.953224\n29 7.588795 6.682027 7.350720 7.021670\n30 7.598572 6.671137 7.332563 7.050782\n31 7.663353 6.674185 7.316531 6.956455\n32 7.638913 6.654991 7.315467 6.943161\n33 7.583040 6.744792 7.315880 7.050680\n34 7.587131 6.745807 7.315240 7.040534\n35 7.629869 6.777861 7.320697 6.987942\n36 7.633211 6.690888 7.296936 7.042209\n37 7.586876 6.690698 7.304879 7.001249\n38 7.605630 6.641577 7.312473 7.054762\n39 7.578209 6.702098 7.346856 6.861277\n40 7.615791 6.693407 7.275010 6.904633\n41 7.619558 6.629054 7.335635 6.968320\n42 7.597071 6.722200 7.301024 6.909407\n43 7.653331 6.665569 7.320626 6.992004\n44 7.608243 6.725489 7.387248 7.060275\n45 7.563750 6.676142 7.308825 6.848430\n46 7.614122 6.674357 7.303363 6.924984\n47 7.596193 6.694495 7.306756 6.901427\n48 7.606979 6.738551 7.326250 7.075850\n49 7.593624 6.698417 7.340698 7.080536\n50 7.543299 6.771432 7.302208 6.976786\n\n$out_cf_y\n          1        4        2        3\n1  7.607414 7.241627 7.491580 7.233985\n2  7.629989 7.175883 7.438672 7.212483\n3  7.593847 7.247442 7.492807 7.189440\n4  7.636815 7.247154 7.413713 7.170191\n5  7.579692 7.455758 7.484690 7.201809\n6  7.576799 7.155861 7.400021 7.370657\n7  7.594346 7.214699 7.435885 7.201016\n8  7.550623 7.151088 7.492054 7.540559\n9  7.578009 7.233869 7.443912 7.093525\n10 7.584653 7.362574 7.447179 7.274074\n11 7.617740 7.272451 7.504440 7.298578\n12 7.587291 7.339500 7.496442 7.085919\n13 7.624523 7.385366 7.442322 7.356935\n14 7.599690 7.218713 7.469391 7.235977\n15 7.577244 7.383210 7.436937 7.392177\n16 7.609999 7.393361 7.470339 7.195733\n17 7.584932 7.420018 7.491768 7.261574\n18 7.578244 7.254377 7.458756 7.223458\n19 7.580229 7.269746 7.464720 7.195017\n20 7.643505 7.154219 7.509154 7.172263\n21 7.577978 7.352014 7.478805 7.269683\n22 7.620846 7.433312 7.479854 7.302266\n23 7.600740 7.173616 7.491643 7.306826\n24 7.625785 7.277016 7.473609 7.127676\n25 7.591045 7.274529 7.444616 7.291411\n26 7.581488 7.264959 7.465865 7.008815\n27 7.597288 7.414300 7.441858 7.187059\n28 7.626680 7.300014 7.496952 7.040814\n29 7.589927 7.158867 7.460670 7.154384\n30 7.598552 7.174590 7.487488 7.261998\n31 7.662883 7.166347 7.477567 7.374014\n32 7.641907 7.242302 7.487043 7.168227\n33 7.581495 7.313520 7.479004 7.389798\n34 7.590214 7.370308 7.474344 7.297419\n35 7.630770 7.325501 7.430298 7.332733\n36 7.633509 7.268941 7.415104 7.397309\n37 7.592106 7.105822 7.426028 7.199093\n38 7.604581 7.172293 7.460978 7.311739\n39 7.575793 7.168612 7.484473 7.132777\n40 7.614368 7.213674 7.416985 7.150848\n41 7.617792 7.227409 7.463862 7.133855\n42 7.597207 7.443558 7.387119 7.053380\n43 7.651733 7.302834 7.477593 7.174305\n44 7.606760 7.238119 7.544200 7.360466\n45 7.561765 7.197407 7.476156 7.196137\n46 7.621505 7.178989 7.430615 7.389787\n47 7.599247 7.233115 7.424585 7.335626\n48 7.610071 7.200206 7.495498 7.266563\n49 7.592124 7.159419 7.512929 7.217993\n50 7.545320 7.359588 7.435835 7.243914\n\n$out_nc_quantile_y\n             1        4        2        3\n2.5%  7.554194 6.642253 7.289444 6.851321\n50%   7.596958 6.698040 7.320570 6.977659\n97.5% 7.650829 6.777176 7.364472 7.085786\n\n$out_cf_quantile_y\n             1        4        2        3\n2.5%  7.553130 7.151793 7.403101 7.043642\n50%   7.597920 7.247298 7.469865 7.228721\n97.5% 7.649881 7.441253 7.512079 7.396155\n\n$mediation\n        4         2         3 \n0.6283431 0.5154469 0.4166421 \n\n$mediation_quantile\n              4         2         3\n2.5%  0.4836956 0.3327377 0.1110938\n97.5% 0.8243395 0.7036244 0.6867509\n\n$mc_conv_info_m\n          [,1]     [,2]     [,3]     [,4]\n [1,] 7.689496 5.829198 7.091873 6.309873\n [2,] 7.703785 5.850273 7.113070 6.306611\n [3,] 7.701987 5.858959 7.098913 6.314230\n [4,] 7.705048 5.868545 7.101568 6.324108\n [5,] 7.706593 5.870395 7.098734 6.339593\n [6,] 7.703159 5.875024 7.099050 6.323742\n [7,] 7.703515 5.879523 7.098054 6.317544\n [8,] 7.702621 5.878158 7.101698 6.315582\n [9,] 7.706594 5.879419 7.100027 6.332194\n[10,] 7.704393 5.878511 7.100601 6.334574\n\n$mc_conv_info_y\n          [,1]     [,2]     [,3]     [,4]\n [1,] 7.606192 6.704516 7.339697 6.997885\n [2,] 7.609234 6.710571 7.344870 6.997330\n [3,] 7.608851 6.713067 7.341415 6.998625\n [4,] 7.609503 6.715821 7.342063 7.000305\n [5,] 7.609832 6.716352 7.341372 7.002938\n [6,] 7.609101 6.717682 7.341449 7.000243\n [7,] 7.609177 6.718975 7.341206 6.999189\n [8,] 7.608986 6.718583 7.342095 6.998855\n [9,] 7.609832 6.718945 7.341687 7.001680\n[10,] 7.609364 6.718684 7.341827 7.002085\n\nmean(fit_cfdecomp$out_nc_y[,2] - fit_cfdecomp$out_nc_y[,1])\n\n[1] -0.9007578\n\nmean(fit_cfdecomp$out_cf_y[,2] - fit_cfdecomp$out_nc_y[,1])\n\n[1] -0.3357567\n\nmean(fit_cfdecomp$out_nc_y[,2] - fit_cfdecomp$out_cf_y[,2])\n\n[1] -0.5650011\n\nfit_cfdecomp$mediation\n\n        4         2         3 \n0.6283431 0.5154469 0.4166421 \n\nmean(fit_cfdecomp$out_nc_y[,3] - fit_cfdecomp$out_nc_y[,1])\n\n[1] -0.2795149\n\nmean(fit_cfdecomp$out_cf_y[,3] - fit_cfdecomp$out_nc_y[,1])\n\n[1] -0.1373915\n\nmean(fit_cfdecomp$out_nc_y[,3] - fit_cfdecomp$out_cf_y[,3])\n\n[1] -0.1421234\n\nmean(fit_cfdecomp$out_nc_y[,4] - fit_cfdecomp$out_nc_y[,1])\n\n[1] -0.6208383\n\nmean(fit_cfdecomp$out_cf_y[,4] - fit_cfdecomp$out_nc_y[,1])\n\n[1] -0.361873\n\nmean(fit_cfdecomp$out_nc_y[,4] - fit_cfdecomp$out_cf_y[,4])\n\n[1] -0.2589653"
  },
  {
    "objectID": "Causal_Inference/Causal_Decomposition.html#causal.decomp",
    "href": "Causal_Inference/Causal_Decomposition.html#causal.decomp",
    "title": "Causal Decomposition Analysis",
    "section": "causal.decomp",
    "text": "causal.decomp\n\nPark, Kang, and Lee (2023) の方法。\n\n\n# smi \nfit.y &lt;- lm(Y ~ X + T + X:T + L1 + L2 + C1 + C2 + C3 + C4, data = d)\nfit.m &lt;- lm(T ~ X + C1 + C2 + C3 + C4, data = d)\n\nfit_smi &lt;- smi(fit.y = fit.y,\n    fit.m = fit.m,\n    treat = \"X\", \n    sims = 100, \n    conf.level = .95,\n    conditional = TRUE,\n    covariates = 1,\n    # baseline covariatesを調整できる\n    #covariates = c(\"C1\", \"C2\", \"C3\", \"C4\"),\n    seed = 227,\n    )\n\nfit_smi\n\n\nResults:\n\n                               estimate 95% CI Lower 95% CI Upper\nInitial Disparity   (1 vs 4) -0.8993401   -0.9925190  -0.79813505\nDisparity Remaining (1 vs 4) -0.3384430   -0.4880863  -0.14795873\nDisparity Reduction (1 vs 4) -0.5608971   -0.7346168  -0.42650061\nInitial Disparity   (1 vs 2) -0.2749659   -0.3378874  -0.19366549\nDisparity Remaining (1 vs 2) -0.1213246   -0.2203441  -0.05458727\nDisparity Reduction (1 vs 2) -0.1536412   -0.1896213  -0.10521328\nInitial Disparity   (1 vs 3) -0.6137425   -0.7326095  -0.47793913\nDisparity Remaining (1 vs 3) -0.3500123   -0.5151994  -0.08733348\nDisparity Reduction (1 vs 3) -0.2637302   -0.4880614  -0.11574038\n\n\n\nsensitivity analysis(Park et al. 2023)\n\n\nsensRes &lt;- sensitivity(boot.res = fit_smi, fit.m = fit.m, fit.y = fit.y, \n                       mediator = \"T\",\n                       covariates = c(\"C1\", \"C2\", \"C3\", \"C4\"), \n                       treat = \"X\",\n                       sel.lev.treat = \"4\", \n                       max.rsq = 0.3)\nplot(sensRes)"
  },
  {
    "objectID": "Causal_Inference/Causal_Decomposition.html#cfdecomp-1",
    "href": "Causal_Inference/Causal_Decomposition.html#cfdecomp-1",
    "title": "Causal Decomposition Analysis",
    "section": "cfdecomp",
    "text": "cfdecomp\n\n# cfd.mean\nset.seed(123456)\nfit_cfdecomp_b &lt;-\n  cfd.mean(\n    formula.y = 'Y ~ X + T2 + X:T2 + L1 + L2 + C1 + C2 + C3 + C4',\n    formula.m = 'T2 ~ X + C1 + C2 + C3 + C4',\n    mediator = 'T2',\n    group = 'X',\n    data = d |&gt; mutate(T2 = as.numeric(T2) - 1) |&gt; data.frame(),\n    family.y = 'gaussian',\n    family.m = 'binomial',\n    bs.size = 50,\n    mc.size = 10,\n    alpha = 0.05\n  )\nmean(fit_cfdecomp_b$out_nc_y[,\"4\"] - fit_cfdecomp_b$out_nc_y[,\"1\"])\n\n[1] -0.8981701\n\nmean(fit_cfdecomp_b$out_cf_y[,\"4\"] - fit_cfdecomp_b$out_nc_y[,\"1\"])\n\n[1] -0.5546351\n\nmean(fit_cfdecomp_b$out_nc_y[,\"4\"] - fit_cfdecomp_b$out_cf_y[,\"4\"])\n\n[1] -0.343535\n\nfit_cfdecomp_b$mediation\n\n        4         2         3 \n0.3828971 0.1298177 0.2061071 \n\nmean(fit_cfdecomp_b$out_nc_y[,\"2\"] - fit_cfdecomp_b$out_nc_y[,\"1\"])\n\n[1] -0.2774845\n\nmean(fit_cfdecomp_b$out_cf_y[,\"2\"] - fit_cfdecomp_b$out_nc_y[,\"1\"])\n\n[1] -0.2419289\n\nmean(fit_cfdecomp_b$out_nc_y[,\"2\"] - fit_cfdecomp_b$out_cf_y[,\"2\"])\n\n[1] -0.03555558\n\nmean(fit_cfdecomp_b$out_nc_y[,\"3\"] - fit_cfdecomp_b$out_nc_y[,\"1\"])\n\n[1] -0.5849521\n\nmean(fit_cfdecomp_b$out_cf_y[,\"3\"] - fit_cfdecomp_b$out_nc_y[,\"1\"])\n\n[1] -0.4655888\n\nmean(fit_cfdecomp_b$out_nc_y[,\"3\"] - fit_cfdecomp_b$out_cf_y[,\"3\"])\n\n[1] -0.1193632"
  },
  {
    "objectID": "Causal_Inference/Causal_Decomposition.html#causal.decomp-1",
    "href": "Causal_Inference/Causal_Decomposition.html#causal.decomp-1",
    "title": "Causal Decomposition Analysis",
    "section": "causal.decomp",
    "text": "causal.decomp\n\n# smi\nfit.y &lt;- lm(Y ~ X + T2 + X:T2 + L1 + L2 + C1 + C2 + C3 + C4, data = d)\nfit.m &lt;- glm(T2 ~ X + C1 + C2 + C3 + C4, data = d, family = binomial(link = \"logit\"))\n\nfit_smi_b &lt;- smi(fit.y = fit.y,\n               fit.m = fit.m,\n               treat = \"X\", \n               sims = 100, \n               conf.level = .95,\n               conditional = TRUE,\n               # covariates = 1,\n               covariates = c(\"C1\", \"C2\", \"C3\", \"C4\"),\n               seed = 123456)\nfit_smi_b\n\n\nResults:\n\n                                estimate 95% CI Lower 95% CI Upper\nInitial Disparity   (1 vs 4) -0.95667843  -1.02938841  -0.88175003\nDisparity Remaining (1 vs 4) -0.61262729  -0.77966809  -0.46844380\nDisparity Reduction (1 vs 4) -0.34405113  -0.49650368  -0.21581205\nInitial Disparity   (1 vs 2) -0.31394841  -0.38058533  -0.26000373\nDisparity Remaining (1 vs 2) -0.27995004  -0.34624791  -0.22226331\nDisparity Reduction (1 vs 2) -0.03399837  -0.05003528  -0.02022012\nInitial Disparity   (1 vs 3) -0.59968604  -0.69525994  -0.49987857\nDisparity Remaining (1 vs 3) -0.48148718  -0.61387973  -0.32105036\nDisparity Reduction (1 vs 3) -0.11819886  -0.24676548  -0.03361808\n\nsensRes &lt;- sensitivity(boot.res = fit_smi_b, \n                       fit.m = fit.m, \n                       fit.y = fit.y, \n                       mediator = \"T2\",\n                       covariates = c(\"C1\", \"C2\", \"C3\", \"C4\"), \n                       treat = \"X\",\n                       sel.lev.treat = \"4\", \n                       max.rsq = 0.3)\nplot(sensRes)"
  },
  {
    "objectID": "Causal_Inference/Causal_Decomposition.html#gapclosing",
    "href": "Causal_Inference/Causal_Decomposition.html#gapclosing",
    "title": "Causal Decomposition Analysis",
    "section": "gapclosing",
    "text": "gapclosing\n\nLundberg (2022)\n\n\n# gapclosing - regression\n# stochastic intervention\n# treatmentの割り当て確率の予測値を算出\nfit_glm &lt;- glm(T2 ~ X + C1 + C2 + C3, data = d, family = binomial(link = \"logit\"))\n\n# 全員のtreatmentが1だった時の予測値\nassing_prob &lt;- predict(fit_glm, newdata = d |&gt; mutate(X = \"1\"), type = \"response\")\n\n# 予測値をもとにrandom draw\ndraw &lt;- rbinom(n = nrow(d), size = 1, prob = assing_prob)\n\nfit_gapclosing &lt;- \n  gapclosing(\n    data = d |&gt; mutate(T2 = as.numeric(T2) - 1),\n    outcome_formula = Y ~ T2 * X + C1 + C2 + C3 + C4 + L1 + L2,\n    treatment_name = \"T2\",\n    category_name = \"X\",\n    counterfactual_assignments = draw # random draw\n  )\n\nfit_gapclosing\n\n\nFactual mean outcomes:\n# A tibble: 4 × 2\n  X     estimate\n  &lt;fct&gt;    &lt;dbl&gt;\n1 1         7.60\n2 4         6.70\n3 2         7.32\n4 3         6.98\n\nCounterfactual mean outcomes (post-intervention means):\n# A tibble: 4 × 2\n  X     estimate\n  &lt;fct&gt;    &lt;dbl&gt;\n1 1         7.60\n2 4         7.03\n3 2         7.36\n4 3         7.11\n\nFactual disparities:\n# A tibble: 12 × 2\n   X     estimate\n   &lt;chr&gt;    &lt;dbl&gt;\n 1 1 - 4    0.899\n 2 1 - 2    0.275\n 3 1 - 3    0.614\n 4 4 - 1   -0.899\n 5 4 - 2   -0.624\n 6 4 - 3   -0.286\n 7 2 - 1   -0.275\n 8 2 - 4    0.624\n 9 2 - 3    0.339\n10 3 - 1   -0.614\n11 3 - 4    0.286\n12 3 - 2   -0.339\n\nCounterfactual disparities (gap-closing estimands):\n# A tibble: 12 × 2\n   X     estimate\n   &lt;chr&gt;    &lt;dbl&gt;\n 1 1 - 4   0.566 \n 2 1 - 2   0.241 \n 3 1 - 3   0.483 \n 4 4 - 1  -0.566 \n 5 4 - 2  -0.325 \n 6 4 - 3  -0.0830\n 7 2 - 1  -0.241 \n 8 2 - 4   0.325 \n 9 2 - 3   0.242 \n10 3 - 1  -0.483 \n11 3 - 4   0.0830\n12 3 - 2  -0.242 \n\nAdditive gap closed: Counterfactual - Factual\n# A tibble: 12 × 2\n   X     estimate\n   &lt;chr&gt;    &lt;dbl&gt;\n 1 1 - 4   0.334 \n 2 1 - 2   0.0339\n 3 1 - 3   0.131 \n 4 4 - 1  -0.334 \n 5 4 - 2  -0.300 \n 6 4 - 3  -0.203 \n 7 2 - 1  -0.0339\n 8 2 - 4   0.300 \n 9 2 - 3   0.0972\n10 3 - 1  -0.131 \n11 3 - 4   0.203 \n12 3 - 2  -0.0972\n\nProportional gap closed: (Counterfactual - Factual) / Factual\n# A tibble: 12 × 2\n   X     estimate\n   &lt;chr&gt;    &lt;dbl&gt;\n 1 1 - 4    0.371\n 2 1 - 2    0.123\n 3 1 - 3    0.214\n 4 4 - 1    0.371\n 5 4 - 2    0.480\n 6 4 - 3    0.710\n 7 2 - 1    0.123\n 8 2 - 4    0.480\n 9 2 - 3    0.287\n10 3 - 1    0.214\n11 3 - 4    0.710\n12 3 - 2    0.287\n\ndisparityplot(fit_gapclosing, category_A = \"1\", category_B = \"4\")\n\n\n\ndisparityplot(fit_gapclosing, category_A = \"1\", category_B = \"2\")\n\n\n\ndisparityplot(fit_gapclosing, category_A = \"1\", category_B = \"3\")\n\n\n\n\n\n機械学習をつかったdoubly robustな方法も使える\n\n\n# gapclosing - ranger, doubly robust\nfit_gapclosing_ranger &lt;- \n  gapclosing(\n  data = d |&gt; mutate(T2 = as.numeric(T2) - 1),\n  outcome_formula = Y ~ T2 + X + C1 + C2 + C3 + C4 + L1 + L2,\n  treatment_formula = T2 ~ X + C1 + C2 + C3 + C4 + L1 + L2, \n  treatment_name = \"T2\",\n  treatment_algorithm = \"ranger\",\n  outcome_algorithm = \"ranger\",\n  category_name = \"X\",\n  counterfactual_assignments = rbinom(n = nrow(d), size = 1, prob = assing_prob) \n)\n\nfit_gapclosing\n\n\nFactual mean outcomes:\n# A tibble: 4 × 2\n  X     estimate\n  &lt;fct&gt;    &lt;dbl&gt;\n1 1         7.60\n2 4         6.70\n3 2         7.32\n4 3         6.98\n\nCounterfactual mean outcomes (post-intervention means):\n# A tibble: 4 × 2\n  X     estimate\n  &lt;fct&gt;    &lt;dbl&gt;\n1 1         7.60\n2 4         7.03\n3 2         7.36\n4 3         7.11\n\nFactual disparities:\n# A tibble: 12 × 2\n   X     estimate\n   &lt;chr&gt;    &lt;dbl&gt;\n 1 1 - 4    0.899\n 2 1 - 2    0.275\n 3 1 - 3    0.614\n 4 4 - 1   -0.899\n 5 4 - 2   -0.624\n 6 4 - 3   -0.286\n 7 2 - 1   -0.275\n 8 2 - 4    0.624\n 9 2 - 3    0.339\n10 3 - 1   -0.614\n11 3 - 4    0.286\n12 3 - 2   -0.339\n\nCounterfactual disparities (gap-closing estimands):\n# A tibble: 12 × 2\n   X     estimate\n   &lt;chr&gt;    &lt;dbl&gt;\n 1 1 - 4   0.566 \n 2 1 - 2   0.241 \n 3 1 - 3   0.483 \n 4 4 - 1  -0.566 \n 5 4 - 2  -0.325 \n 6 4 - 3  -0.0830\n 7 2 - 1  -0.241 \n 8 2 - 4   0.325 \n 9 2 - 3   0.242 \n10 3 - 1  -0.483 \n11 3 - 4   0.0830\n12 3 - 2  -0.242 \n\nAdditive gap closed: Counterfactual - Factual\n# A tibble: 12 × 2\n   X     estimate\n   &lt;chr&gt;    &lt;dbl&gt;\n 1 1 - 4   0.334 \n 2 1 - 2   0.0339\n 3 1 - 3   0.131 \n 4 4 - 1  -0.334 \n 5 4 - 2  -0.300 \n 6 4 - 3  -0.203 \n 7 2 - 1  -0.0339\n 8 2 - 4   0.300 \n 9 2 - 3   0.0972\n10 3 - 1  -0.131 \n11 3 - 4   0.203 \n12 3 - 2  -0.0972\n\nProportional gap closed: (Counterfactual - Factual) / Factual\n# A tibble: 12 × 2\n   X     estimate\n   &lt;chr&gt;    &lt;dbl&gt;\n 1 1 - 4    0.371\n 2 1 - 2    0.123\n 3 1 - 3    0.214\n 4 4 - 1    0.371\n 5 4 - 2    0.480\n 6 4 - 3    0.710\n 7 2 - 1    0.123\n 8 2 - 4    0.480\n 9 2 - 3    0.287\n10 3 - 1    0.214\n11 3 - 4    0.710\n12 3 - 2    0.287\n\ndisparityplot(fit_gapclosing_ranger, category_A = \"1\", category_B = \"4\")\n\n\n\ndisparityplot(fit_gapclosing_ranger, category_A = \"1\", category_B = \"2\")\n\n\n\ndisparityplot(fit_gapclosing_ranger, category_A = \"1\", category_B = \"3\")"
  },
  {
    "objectID": "Causal_Inference/TMLE.html",
    "href": "Causal_Inference/TMLE.html",
    "title": "Targeted Maximum Likelihood Estimation / Targeted Minimum Loss-based Estimation (TMLE)",
    "section": "",
    "text": "参考ページ：https://www.khstats.com/blog/tmle/tutorial-pt2\n\n\n\n\n\nlibrary(tidyverse)\nlibrary(sl3)\nlibrary(tmle3)\nlibrary(future)\n\nkable &lt;- partial(\n  knitr::kable,\n  digits = 3\n)\n\nplan(multisession, workers = 8)\n\nset.seed(7)\n\n\n\n\n\n元のサイトでは\\(Y\\)が2値変数になっていて、推定が真値と一致しているかどうかの評価が難しいので、ここでは\\(Y\\)を連続変数としている。\nATEの真値は0.3に設定\n\n\ngenerate_data &lt;- function(n){ \n    W1 &lt;- rbinom(n, size=1, prob=0.2) # binary confounder\n    W2 &lt;- rbinom(n, size=1, prob=0.5) # binary confounder\n    W3 &lt;- round(runif(n, min=2, max=7)) # continuous confounder\n    W4 &lt;- round(runif(n, min=0, max=4)) # continuous confounder\n    A  &lt;- rbinom(n, size=1, prob= plogis(-0.2 + 0.2*W2 + log(0.1*W3) + 0.3*W4 + 0.2*W1*W4)) # binary treatment depends on confounders\n    Y &lt;- -1 + 0.3*A - 0.1*W1 + 0.2*W2 + 0.3*W3 - 0.1*W4 + sin(0.1*W2*W4) # continuous outcome depends on confounders\n    return(tibble(Y, W1, W2, W3, W4, A))\n}\n\nn &lt;- 1000\ndat_obs &lt;- generate_data(n) # generate a data set with n observations\n\n\ndat_obs |&gt; \n  summarise(samplemean = mean(Y), .by = A)\n\n# A tibble: 2 × 2\n      A samplemean\n  &lt;int&gt;      &lt;dbl&gt;\n1     1      0.686\n2     0      0.237\n\n\n\n\n\n\nglm、Lasso、Random forest、Multivariate adaptive regression splineをスタッキング\n\n\nsl_libs &lt;-\n  Lrnr_sl$new(\n    learners = Stack$new(\n      Lrnr_glm$new(), \n      Lrnr_glmnet$new(alpha = 1), \n      Lrnr_ranger$new(num.trees = 2000, max.depth = 3), \n      Lrnr_earth$new()\n    )\n  )\n\n\n\n\n\n\n\n\\[\n{\\mathrm E}[Y | A, W]\n\\]\n\ntask &lt;- \n  sl3_Task$new(\n    dat_obs, \n    covariates = select(dat_obs, !Y) |&gt; names(), \n    outcome = 'Y', \n    outcome_type = 'continuous', \n    folds = 8L\n  )\n\ntask_A1 &lt;- \n  sl3_Task$new(\n    dat_obs |&gt; mutate(A = 1), \n    covariates = select(dat_obs, !Y) |&gt; names(), \n    outcome = 'Y', \n    outcome_type = 'continuous', \n    folds = 8L\n  )\n\ntask_A0 &lt;- \n  sl3_Task$new(\n    dat_obs |&gt; mutate(A = 0), \n    covariates = select(dat_obs, !Y) |&gt; names(), \n    outcome = 'Y', \n    outcome_type = 'continuous', \n    folds = 8L\n  )\n\n# 全サンプルで学習\nsl_fit &lt;- sl_libs$train(task)\n\n\n以下の3つの予測値を算出\n\\(A := 1\\)は全サンプルでAを1にする（\\(A = 1\\)はサンプルのうちのAが1となる部分集団）\n\n\\[\\begin{align}\n&{\\mathrm E}[Y | A, W] \\\\\n&{\\mathrm E}[Y | A := 1, W] \\\\\n&{\\mathrm E}[Y | A := 0, W]\n\\end{align}\\]\n\ndat_tmle1 &lt;- \n  dat_obs |&gt; \n  mutate(\n    # 観測サンプルについての予測値\n    Q_A = sl_fit$predict(task),\n    # 全てのサンプルでA = 1に固定したときの予測値\n    Q_A1 = sl_fit$predict(task_A1),\n    # 全てのサンプルでA = 0に固定した時の予測値\n    Q_A0 = sl_fit$predict(task_A0)\n  )\n\n\nstandardization（g-computation）によるATE\n\n\\[\nATE_{g \\mathrm{-}comp} = {\\mathrm E}[ {\\mathrm E}[Y | A := 1, W] - {\\mathrm E}[Y | A := 0, W]]\n\\]\n\ndat_tmle1 |&gt; \n  summarise(ATE = mean(Q_A1 - Q_A0))\n\n# A tibble: 1 × 1\n    ATE\n  &lt;dbl&gt;\n1 0.300\n\n\n\n\n\n\n\n傾向スコアを機械学習モデルにより予測\n\n\\[\n\\mathrm{Pr}(A = 1 | W)\n\\]\n\ntask_g &lt;- \n  sl3_Task$new(\n    data = dat_obs, \n    covariates = select(dat_obs, !c(Y, A)) |&gt; names(), \n    outcome = 'A', \n    outcome_type = 'binomial', \n    folds = 8\n  )\n\nsl_fit_g &lt;- sl_libs$train(task_g)\n\n\n\n\n傾向スコアからClever Covariateと呼ばれる情報を作成（IPWに似ている）\n\n\\[\\begin{align}\n&H(A,W) &= \\frac{A}{\\mathrm{Pr}(A = 1 | W)} - \\frac{1 - A}{1 - \\mathrm{Pr}(A = 1 | W)} \\\\\n&H(1,W) &= \\frac{A}{\\mathrm{Pr}(A = 1 | W)} \\\\\n&H(0,W) &= - \\frac{1 - A}{1 - \\mathrm{Pr}(A = 1 | W)}\n\\end{align}\\]\n\ndat_tmle2 &lt;- \n  dat_tmle1 |&gt; \n  mutate(\n    # Propensity Scoreの予測\n    ps = sl_fit_g$predict(task_g),\n    # ipw (Inverse Probability Weight)\n    ipw = case_when(\n      A == 1 ~ 1 / ps, \n      A == 0 ~ 1 / (1 - ps)\n    ),\n    # Clever Covariates\n    H_A = case_when(\n      A == 1 ~ 1 / ps,\n      A == 0 ~ -1 / (1 - ps)\n    ),\n    H_A1 = case_when(\n      A == 1 ~ H_A,\n      A == 0 ~ 0\n    ),\n    H_A0 = case_when(\n      A == 1 ~ 0,\n      A == 0 ~ H_A\n    )\n  )\n\n\nIPWによるATE\n\n\\[\nATE_{ipw} = {\\mathrm E}[\\frac{A}{\\mathrm{Pr}(A = 1 | W)}Y - \\frac{1 - A}{1 - \\mathrm{Pr}(A = 1 | W)}Y]\n\\]\n\ndat_tmle2 |&gt; \n  summarise(CFmean = sum(Y*ipw) / sum(ipw), .by = A) |&gt; \n  arrange(A) |&gt; \n  summarise(ATE = diff(CFmean))\n\n# A tibble: 1 × 1\n    ATE\n  &lt;dbl&gt;\n1 0.306\n\n\n\nAugumented IPWによるATE\n\n（ほんとは関数推定時にcross-fitをする）\n\n\n\\[\nATE_{aipw} = {\\mathrm E[{\\mathrm E}[Y | A := 1, W] - {\\mathrm E}[Y | A := 0, W] + \\frac{A}{{\\mathrm Pr}(A = 1 | W)}(Y - {\\mathrm E}[Y | A := 1, W]) - \\frac{1 - A}{1 - {\\mathrm Pr}(A = 1 | W)}(Y - {\\mathrm E}[Y | A := 0, W])]}\n\\]\n\ndat_tmle2 |&gt; \n  summarise(ATE = mean(Q_A1 - Q_A0 + ipw*A*(Y - Q_A1) - ipw*(1 - A)*(Y - Q_A0)))\n\n# A tibble: 1 × 1\n    ATE\n  &lt;dbl&gt;\n1 0.300\n\n\n\n\n\n\n\nAIPWの問題点：統計的最適化がターゲットのパラメータ（ATE）に対してではなく、母平均関数\\({\\mathrm E}[Y | A,W]\\)および傾向スコア関数\\(\\mathrm{Pr}(A = 1 | W)\\)のパラメータについて最適化されている点\n推定したいパラメータ（ATE）のEIF（Efficient Influence Function）を解くことがこのステップのポイントらしい\n具体的には、Step1で推定した\\(\\mathrm{E}[Y | A, W]\\)と、Step2で推定したClever Covariate\\(H(A, W)\\)を用いて、以下の回帰式の\\(\\epsilon\\)（変動パラメータ）を推定する\n\n\\[\nY = \\mathrm{E}[Y | A, W] + \\epsilon H(A,W)\n\\]\n\n切片が0で、Step1の推定値の係数を1に固定するために、-1とoffsetを利用する\n\n\nfit &lt;- glm(Y ~ -1 + offset(Q_A) + H_A, data = dat_tmle2, family = gaussian())\n\n\n変動パラメータの推定値\n\n\nepsilon &lt;- coef(fit)\n\nepsilon\n\n         H_A \n1.386454e-06 \n\n\n\n\n\n\n推定したepsilonと\\(Y\\)の予測値をもとに、\\(Y\\)の予測値を更新\n\n\ndat_tmle3 &lt;- \n  dat_tmle2 |&gt; \n  mutate(\n    Q_A_update = Q_A + epsilon*H_A,\n    Q_A1_update = Q_A1 + epsilon*H_A1,\n    Q_A0_update = Q_A0 + epsilon*H_A0,\n  )\n\n\n\n\n\n更新されたアウトカムの予測値を用いて、Standardizationの要領でATEを推定\n\n\ndat_tmle3 |&gt; \n  summarise(ATE = mean(Q_A1_update - Q_A0_update))\n\n# A tibble: 1 × 1\n    ATE\n  &lt;dbl&gt;\n1 0.300\n\nATE &lt;- mean(dat_tmle3$Q_A1_update - dat_tmle3$Q_A0_update)\n\n\n\n\n\nTMLEではbootstrapによらずとも標準誤差を算出できる（！）\nまずは、Influence Functionを推定する\nInfluence Function：各サンプルがどれだけATEに影響をあたえるか？\n\n\ndat_tmle4 &lt;- \n  dat_tmle3 |&gt; \n  mutate(\n    IF = (Y - Q_A_update)*H_A + Q_A1_update - Q_A0_update - ATE\n  )\n\nATEの標準誤差はIFを用いて\n\\[\nSE = \\sqrt{\\frac{\\mathrm{var}(IF)}{N}}\n\\]\n\ndat_tmle4 |&gt; \n  summarise(SE = sqrt(var(IF) / 1000))\n\n# A tibble: 1 × 1\n         SE\n      &lt;dbl&gt;\n1 0.0000215"
  },
  {
    "objectID": "Causal_Inference/TMLE.html#下準備",
    "href": "Causal_Inference/TMLE.html#下準備",
    "title": "Targeted Maximum Likelihood Estimation / Targeted Minimum Loss-based Estimation (TMLE)",
    "section": "",
    "text": "library(tidyverse)\nlibrary(sl3)\nlibrary(tmle3)\nlibrary(future)\n\nkable &lt;- partial(\n  knitr::kable,\n  digits = 3\n)\n\nplan(multisession, workers = 8)\n\nset.seed(7)\n\n\n\n\n\n元のサイトでは\\(Y\\)が2値変数になっていて、推定が真値と一致しているかどうかの評価が難しいので、ここでは\\(Y\\)を連続変数としている。\nATEの真値は0.3に設定\n\n\ngenerate_data &lt;- function(n){ \n    W1 &lt;- rbinom(n, size=1, prob=0.2) # binary confounder\n    W2 &lt;- rbinom(n, size=1, prob=0.5) # binary confounder\n    W3 &lt;- round(runif(n, min=2, max=7)) # continuous confounder\n    W4 &lt;- round(runif(n, min=0, max=4)) # continuous confounder\n    A  &lt;- rbinom(n, size=1, prob= plogis(-0.2 + 0.2*W2 + log(0.1*W3) + 0.3*W4 + 0.2*W1*W4)) # binary treatment depends on confounders\n    Y &lt;- -1 + 0.3*A - 0.1*W1 + 0.2*W2 + 0.3*W3 - 0.1*W4 + sin(0.1*W2*W4) # continuous outcome depends on confounders\n    return(tibble(Y, W1, W2, W3, W4, A))\n}\n\nn &lt;- 1000\ndat_obs &lt;- generate_data(n) # generate a data set with n observations\n\n\ndat_obs |&gt; \n  summarise(samplemean = mean(Y), .by = A)\n\n# A tibble: 2 × 2\n      A samplemean\n  &lt;int&gt;      &lt;dbl&gt;\n1     1      0.686\n2     0      0.237\n\n\n\n\n\n\nglm、Lasso、Random forest、Multivariate adaptive regression splineをスタッキング\n\n\nsl_libs &lt;-\n  Lrnr_sl$new(\n    learners = Stack$new(\n      Lrnr_glm$new(), \n      Lrnr_glmnet$new(alpha = 1), \n      Lrnr_ranger$new(num.trees = 2000, max.depth = 3), \n      Lrnr_earth$new()\n    )\n  )"
  },
  {
    "objectID": "Causal_Inference/TMLE.html#step1アウトカムの予測",
    "href": "Causal_Inference/TMLE.html#step1アウトカムの予測",
    "title": "Targeted Maximum Likelihood Estimation / Targeted Minimum Loss-based Estimation (TMLE)",
    "section": "",
    "text": "\\[\n{\\mathrm E}[Y | A, W]\n\\]\n\ntask &lt;- \n  sl3_Task$new(\n    dat_obs, \n    covariates = select(dat_obs, !Y) |&gt; names(), \n    outcome = 'Y', \n    outcome_type = 'continuous', \n    folds = 8L\n  )\n\ntask_A1 &lt;- \n  sl3_Task$new(\n    dat_obs |&gt; mutate(A = 1), \n    covariates = select(dat_obs, !Y) |&gt; names(), \n    outcome = 'Y', \n    outcome_type = 'continuous', \n    folds = 8L\n  )\n\ntask_A0 &lt;- \n  sl3_Task$new(\n    dat_obs |&gt; mutate(A = 0), \n    covariates = select(dat_obs, !Y) |&gt; names(), \n    outcome = 'Y', \n    outcome_type = 'continuous', \n    folds = 8L\n  )\n\n# 全サンプルで学習\nsl_fit &lt;- sl_libs$train(task)\n\n\n以下の3つの予測値を算出\n\\(A := 1\\)は全サンプルでAを1にする（\\(A = 1\\)はサンプルのうちのAが1となる部分集団）\n\n\\[\\begin{align}\n&{\\mathrm E}[Y | A, W] \\\\\n&{\\mathrm E}[Y | A := 1, W] \\\\\n&{\\mathrm E}[Y | A := 0, W]\n\\end{align}\\]\n\ndat_tmle1 &lt;- \n  dat_obs |&gt; \n  mutate(\n    # 観測サンプルについての予測値\n    Q_A = sl_fit$predict(task),\n    # 全てのサンプルでA = 1に固定したときの予測値\n    Q_A1 = sl_fit$predict(task_A1),\n    # 全てのサンプルでA = 0に固定した時の予測値\n    Q_A0 = sl_fit$predict(task_A0)\n  )\n\n\nstandardization（g-computation）によるATE\n\n\\[\nATE_{g \\mathrm{-}comp} = {\\mathrm E}[ {\\mathrm E}[Y | A := 1, W] - {\\mathrm E}[Y | A := 0, W]]\n\\]\n\ndat_tmle1 |&gt; \n  summarise(ATE = mean(Q_A1 - Q_A0))\n\n# A tibble: 1 × 1\n    ATE\n  &lt;dbl&gt;\n1 0.300"
  },
  {
    "objectID": "Causal_Inference/TMLE.html#step2処置確率傾向スコアの予測",
    "href": "Causal_Inference/TMLE.html#step2処置確率傾向スコアの予測",
    "title": "Targeted Maximum Likelihood Estimation / Targeted Minimum Loss-based Estimation (TMLE)",
    "section": "",
    "text": "傾向スコアを機械学習モデルにより予測\n\n\\[\n\\mathrm{Pr}(A = 1 | W)\n\\]\n\ntask_g &lt;- \n  sl3_Task$new(\n    data = dat_obs, \n    covariates = select(dat_obs, !c(Y, A)) |&gt; names(), \n    outcome = 'A', \n    outcome_type = 'binomial', \n    folds = 8\n  )\n\nsl_fit_g &lt;- sl_libs$train(task_g)\n\n\n\n\n傾向スコアからClever Covariateと呼ばれる情報を作成（IPWに似ている）\n\n\\[\\begin{align}\n&H(A,W) &= \\frac{A}{\\mathrm{Pr}(A = 1 | W)} - \\frac{1 - A}{1 - \\mathrm{Pr}(A = 1 | W)} \\\\\n&H(1,W) &= \\frac{A}{\\mathrm{Pr}(A = 1 | W)} \\\\\n&H(0,W) &= - \\frac{1 - A}{1 - \\mathrm{Pr}(A = 1 | W)}\n\\end{align}\\]\n\ndat_tmle2 &lt;- \n  dat_tmle1 |&gt; \n  mutate(\n    # Propensity Scoreの予測\n    ps = sl_fit_g$predict(task_g),\n    # ipw (Inverse Probability Weight)\n    ipw = case_when(\n      A == 1 ~ 1 / ps, \n      A == 0 ~ 1 / (1 - ps)\n    ),\n    # Clever Covariates\n    H_A = case_when(\n      A == 1 ~ 1 / ps,\n      A == 0 ~ -1 / (1 - ps)\n    ),\n    H_A1 = case_when(\n      A == 1 ~ H_A,\n      A == 0 ~ 0\n    ),\n    H_A0 = case_when(\n      A == 1 ~ 0,\n      A == 0 ~ H_A\n    )\n  )\n\n\nIPWによるATE\n\n\\[\nATE_{ipw} = {\\mathrm E}[\\frac{A}{\\mathrm{Pr}(A = 1 | W)}Y - \\frac{1 - A}{1 - \\mathrm{Pr}(A = 1 | W)}Y]\n\\]\n\ndat_tmle2 |&gt; \n  summarise(CFmean = sum(Y*ipw) / sum(ipw), .by = A) |&gt; \n  arrange(A) |&gt; \n  summarise(ATE = diff(CFmean))\n\n# A tibble: 1 × 1\n    ATE\n  &lt;dbl&gt;\n1 0.306\n\n\n\nAugumented IPWによるATE\n\n（ほんとは関数推定時にcross-fitをする）\n\n\n\\[\nATE_{aipw} = {\\mathrm E[{\\mathrm E}[Y | A := 1, W] - {\\mathrm E}[Y | A := 0, W] + \\frac{A}{{\\mathrm Pr}(A = 1 | W)}(Y - {\\mathrm E}[Y | A := 1, W]) - \\frac{1 - A}{1 - {\\mathrm Pr}(A = 1 | W)}(Y - {\\mathrm E}[Y | A := 0, W])]}\n\\]\n\ndat_tmle2 |&gt; \n  summarise(ATE = mean(Q_A1 - Q_A0 + ipw*A*(Y - Q_A1) - ipw*(1 - A)*(Y - Q_A0)))\n\n# A tibble: 1 × 1\n    ATE\n  &lt;dbl&gt;\n1 0.300"
  },
  {
    "objectID": "Causal_Inference/TMLE.html#step3変動パラメータの推定",
    "href": "Causal_Inference/TMLE.html#step3変動パラメータの推定",
    "title": "Targeted Maximum Likelihood Estimation / Targeted Minimum Loss-based Estimation (TMLE)",
    "section": "",
    "text": "AIPWの問題点：統計的最適化がターゲットのパラメータ（ATE）に対してではなく、母平均関数\\({\\mathrm E}[Y | A,W]\\)および傾向スコア関数\\(\\mathrm{Pr}(A = 1 | W)\\)のパラメータについて最適化されている点\n推定したいパラメータ（ATE）のEIF（Efficient Influence Function）を解くことがこのステップのポイントらしい\n具体的には、Step1で推定した\\(\\mathrm{E}[Y | A, W]\\)と、Step2で推定したClever Covariate\\(H(A, W)\\)を用いて、以下の回帰式の\\(\\epsilon\\)（変動パラメータ）を推定する\n\n\\[\nY = \\mathrm{E}[Y | A, W] + \\epsilon H(A,W)\n\\]\n\n切片が0で、Step1の推定値の係数を1に固定するために、-1とoffsetを利用する\n\n\nfit &lt;- glm(Y ~ -1 + offset(Q_A) + H_A, data = dat_tmle2, family = gaussian())\n\n\n変動パラメータの推定値\n\n\nepsilon &lt;- coef(fit)\n\nepsilon\n\n         H_A \n1.386454e-06"
  },
  {
    "objectID": "Causal_Inference/TMLE.html#step4アウトカムの予測値を更新",
    "href": "Causal_Inference/TMLE.html#step4アウトカムの予測値を更新",
    "title": "Targeted Maximum Likelihood Estimation / Targeted Minimum Loss-based Estimation (TMLE)",
    "section": "",
    "text": "推定したepsilonと\\(Y\\)の予測値をもとに、\\(Y\\)の予測値を更新\n\n\ndat_tmle3 &lt;- \n  dat_tmle2 |&gt; \n  mutate(\n    Q_A_update = Q_A + epsilon*H_A,\n    Q_A1_update = Q_A1 + epsilon*H_A1,\n    Q_A0_update = Q_A0 + epsilon*H_A0,\n  )"
  },
  {
    "objectID": "Causal_Inference/TMLE.html#step5推定したい統計量を推定",
    "href": "Causal_Inference/TMLE.html#step5推定したい統計量を推定",
    "title": "Targeted Maximum Likelihood Estimation / Targeted Minimum Loss-based Estimation (TMLE)",
    "section": "",
    "text": "更新されたアウトカムの予測値を用いて、Standardizationの要領でATEを推定\n\n\ndat_tmle3 |&gt; \n  summarise(ATE = mean(Q_A1_update - Q_A0_update))\n\n# A tibble: 1 × 1\n    ATE\n  &lt;dbl&gt;\n1 0.300\n\nATE &lt;- mean(dat_tmle3$Q_A1_update - dat_tmle3$Q_A0_update)"
  },
  {
    "objectID": "Causal_Inference/TMLE.html#step6標準誤差の推定",
    "href": "Causal_Inference/TMLE.html#step6標準誤差の推定",
    "title": "Targeted Maximum Likelihood Estimation / Targeted Minimum Loss-based Estimation (TMLE)",
    "section": "",
    "text": "TMLEではbootstrapによらずとも標準誤差を算出できる（！）\nまずは、Influence Functionを推定する\nInfluence Function：各サンプルがどれだけATEに影響をあたえるか？\n\n\ndat_tmle4 &lt;- \n  dat_tmle3 |&gt; \n  mutate(\n    IF = (Y - Q_A_update)*H_A + Q_A1_update - Q_A0_update - ATE\n  )\n\nATEの標準誤差はIFを用いて\n\\[\nSE = \\sqrt{\\frac{\\mathrm{var}(IF)}{N}}\n\\]\n\ndat_tmle4 |&gt; \n  summarise(SE = sqrt(var(IF) / 1000))\n\n# A tibble: 1 × 1\n         SE\n      &lt;dbl&gt;\n1 0.0000215"
  },
  {
    "objectID": "Others/Multilevel.html",
    "href": "Others/Multilevel.html",
    "title": "マルチレベルモデルは点推定値に影響を及ぼすか？",
    "section": "",
    "text": "library(tidyverse)\nlibrary(lmerTest)\nlibrary(broom)\nlibrary(broom.mixed)"
  },
  {
    "objectID": "Others/Multilevel.html#マルチレベルモデルにおけるよくある誤解",
    "href": "Others/Multilevel.html#マルチレベルモデルにおけるよくある誤解",
    "title": "マルチレベルモデルは点推定値に影響を及ぼすか？",
    "section": "マルチレベルモデルにおけるよくある誤解",
    "text": "マルチレベルモデルにおけるよくある誤解\n「マルチレベルモデルでやんなきゃ係数にバイアスが…」←ホント？"
  },
  {
    "objectID": "Others/Multilevel.html#データ生成",
    "href": "Others/Multilevel.html#データ生成",
    "title": "マルチレベルモデルは点推定値に影響を及ぼすか？",
    "section": "データ生成",
    "text": "データ生成\nマルチレベルのデータを考える サンプルサイズ1000、グループ数50のデータ\n\\[\\begin{align}\ny_{ig} \\sim 0.5x_{ig} + \\mathrm{Normal}(\\theta_g, 1) \\\\\n\\theta_g \\sim \\mathrm{Normal}(0, 3) \\\\\nx_i \\sim \\mathrm{Normal}(0, 1)\n\\end{align}\\]\n\ndgp &lt;- function(samplesize = 1000) {\n  tibble(\n    id = 1:samplesize,\n    group = rep(1:50, 20),\n    x = rnorm(samplesize, mean = 0, sd = 1),\n  ) |&gt; \n    group_by(group) |&gt; \n    mutate(group_mean = rnorm(1, mean = 0, sd = 3)) |&gt; \n    ungroup() |&gt; \n    mutate(y = 0.5*x + rnorm(samplesize, mean = group_mean, sd = 1))\n}\n\ndata &lt;- dgp()\n\n\nlm(y ~ x, data = data) |&gt; \n  summary()\n\n\nCall:\nlm(formula = y ~ x, data = data)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-7.204 -2.101 -0.026  2.141  8.066 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.38089    0.09163  -4.157 3.51e-05 ***\nx            0.50111    0.09371   5.347 1.11e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.895 on 998 degrees of freedom\nMultiple R-squared:  0.02785,   Adjusted R-squared:  0.02688 \nF-statistic: 28.59 on 1 and 998 DF,  p-value: 1.107e-07\n\nlmer(y ~ x + (1|group), data = data) |&gt; \n  summary()\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: y ~ x + (1 | group)\n   Data: data\n\nREML criterion at convergence: 3140.8\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-2.8801 -0.6852 -0.0267  0.6389  3.3316 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n group    (Intercept) 7.464    2.732   \n Residual             1.053    1.026   \nNumber of obs: 1000, groups:  group, 50\n\nFixed effects:\n             Estimate Std. Error        df t value Pr(&gt;|t|)    \n(Intercept)  -0.37872    0.38773  49.00115  -0.977    0.333    \nx             0.45115    0.03431 949.88893  13.148   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n  (Intr)\nx -0.004"
  },
  {
    "objectID": "Others/Multilevel.html#シミュレーション",
    "href": "Others/Multilevel.html#シミュレーション",
    "title": "マルチレベルモデルは点推定値に影響を及ぼすか？",
    "section": "シミュレーション",
    "text": "シミュレーション\nデータを1000個作成\n\ndata_list &lt;- \n  map(1:1000, \\(x) dgp(1000)) |&gt; \n  enframe()\n\nOLSと変量効果モデルで推定\n\nresult &lt;- \n  data_list |&gt; \n  mutate(\n    lm = map(value, \\(data) lm(y ~ x, data = data)),\n    lmer = map(value, \\(data) lmer(y ~ x + (1|group), data = data))\n  )\n\nxの係数のみ取り出す\n\nres2 &lt;- \n  result |&gt; \n  mutate(\n    lm_res = map(lm, \\(model) {\n      tidy(model) |&gt; \n        select(term, estimate)\n    }),\n    lmer_res = map(lmer, \\(model) {\n      tidy(model) |&gt; \n        select(term, estimate)\n    })\n  ) |&gt; \n  select(name, lm_res, lmer_res) |&gt; \n  pivot_longer(!name, names_to = 'model', values_to = 'value') |&gt; \n  unnest(value) |&gt; \n  filter(term == 'x')\n\n結果を図示\n\nどちらの点推定値も真の値の0.5を中心に分布＝バイアスはない\nOLSによる点推定値はバリアンスが大きい\n変量効果（マルチレベル）モデルによる点推定値はバリアンスが小さい\n\n\nres2 |&gt; \n  ggplot(aes(estimate, fill = model))+\n  geom_vline(xintercept = 0.5, linetype = 'dashed', alpha = 0.5)+\n  geom_histogram(alpha = 0.3, color = 'black', binwidth = 0.02, position = 'identity')+\n  scale_x_continuous(breaks = seq(0, 1, 0.2))"
  },
  {
    "objectID": "Others/tensorflow_with_Rstudio.html",
    "href": "Others/tensorflow_with_Rstudio.html",
    "title": "tensorflowをRstudioで",
    "section": "",
    "text": "pythonをRStudioで使うのはreticulateパッケージによりだいぶ楽になった。\nただpythonにおけるdeep learningの代表的パッケージであるtensorflowをRStudioで使う際に少しつまづいたのでメモです。"
  },
  {
    "objectID": "Others/tensorflow_with_Rstudio.html#pythonの環境をどうやって構築するか",
    "href": "Others/tensorflow_with_Rstudio.html#pythonの環境をどうやって構築するか",
    "title": "tensorflowをRstudioで",
    "section": "pythonの環境をどうやって構築するか？",
    "text": "pythonの環境をどうやって構築するか？\nRではrenvパッケージを使っている。ではpythonでは？\npythonはRよりも環境が複雑で変化も早そうなので、環境構築は重要。\n今回はpython公式が推奨しているっぽいvenvを使う。\n\n\n\n\n\n\nNote\n\n\n\n最初global環境でそのままやろうとしたが上手くいかず挫折。Dockerとかも有力だと思う。"
  },
  {
    "objectID": "Others/tensorflow_with_Rstudio.html#venvを使って仮想環境を用意",
    "href": "Others/tensorflow_with_Rstudio.html#venvを使って仮想環境を用意",
    "title": "tensorflowをRstudioで",
    "section": "venvを使って仮想環境を用意",
    "text": "venvを使って仮想環境を用意\n\npythonがインストールされているか確認\n\nhomebrewなりでインストールしておく\nPATHを通しておく\n\n作業ディレクトリにて、terminalで以下のコマンドを実行し.venvフォルダを作成\n\n\n\n\nterminal\n\npython3 -m venv .venv"
  },
  {
    "objectID": "Others/tensorflow_with_Rstudio.html#pythonライブラリをインストール",
    "href": "Others/tensorflow_with_Rstudio.html#pythonライブラリをインストール",
    "title": "tensorflowをRstudioで",
    "section": "pythonライブラリをインストール",
    "text": "pythonライブラリをインストール\n\n以下のコマンドを実行し、仮想環境を起動\n\npythonの場所が.venv内のフォルダになっているのを確認\n\n\n\n\n\nterminal\n\nsource .venv/bin/activate\nwhich python\n\n\n/Users/kamaken/Documents/R/Project/Kentaro-Kamada/.venv/bin/python\n\n\n\n\n\n\n\n\nNote\n\n\n\nsource .venv/bin/activateは、ライブラリをインストールするときは毎回実行する必要があります！\nまちがってglobal環境を汚さないように！\n\n\n\nライブラリをインストール\n\ntensorflowのほか、numpyも必須（tensorflowをインストールしたときに入る）\nM1 macでGPUを使いたい場合はtensorflow-metalもインストールする\n\n\n\n\n\nterminal\n\npip install tensorflow-macos\npip install tensorflow-metal\n\n\n\n仮想環境を抜けるときは以下のコマンドを実行\n\n\n\n\nterminal\n\ndeactivate"
  },
  {
    "objectID": "Others/tensorflow_with_Rstudio.html#rstudioでpythonを使う",
    "href": "Others/tensorflow_with_Rstudio.html#rstudioでpythonを使う",
    "title": "tensorflowをRstudioで",
    "section": "RStudioでpythonを使う",
    "text": "RStudioでpythonを使う\n\npythonの場所を指定\n\nRStudioでpythonを使うには、RStudioにpythonの場所を教えてやる必要がある\n\n今回は.venv内のpythonを使いたいので、作業ディレクトリに.Rprofileを作成し、以下のコードを書く\n\n\n\n\n\n.Rprofile\n\nSys.setenv(RETICULATE_PYTHON = \".venv/bin/python\")\n\n\n\n\npythonを起動\n\nRStudioでpythonのコードを実行すると、reticulate::repl_python()が走ってコンソールがpythonになる\n\n\n\n\npython\n\nimport sys\nprint(sys.version)\n\n\n3.11.7 (main, Dec  4 2023, 18:10:11) [Clang 15.0.0 (clang-1500.1.0.2.5)]\n\n\n\ntensorflowを起動して、GPUが使えるかを確認\nphysical_deviceでCPUとGPUの両方が表示されてればOK\n\n\n\n\npython\n\nimport tensorflow as tf\ntf.config.list_physical_devices()\n\n\n[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]"
  },
  {
    "objectID": "Others/tensorflow_with_Rstudio.html#ニューラルネットをやってみる",
    "href": "Others/tensorflow_with_Rstudio.html#ニューラルネットをやってみる",
    "title": "tensorflowをRstudioで",
    "section": "ニューラルネットをやってみる",
    "text": "ニューラルネットをやってみる\n\nチュートリアルとして、手書きのアルファベットを分類するモデルを作成してみる\nRでデータを読み込んで、pythonに渡してtensorflowで分析\n参考\n\n\n\n\nR\n\nlibrary(tidyverse)\nlibrary(rsample)\nlibrary(reticulate)\n\n\n\nデータのダウンロード\n\n\n\nR\n\n# データのダウンロード\nif (!dir.exists('data')) dir.create('data')\ndownload.file('https://ai.stanford.edu/~btaskar/ocr/letter.data.gz', destfile = 'data/letter.data')\n\n\n\n\nデータの読み込み\n\n列名はhttps://ai.stanford.edu/~btaskar/ocr/letter.namesより取得できる\n詳細は以下の通り\n\nid: each letter is assigned a unique integer id\nletter: a-z\nnext_id: id for next letter in the word, -1 if last letter\nword_id: each word is assigned a unique integer id (not used)\nposition: position of letter in the word (not used)\nfold: 0-9 – cross-validation fold\np_i_j: 0/1 – value of pixel in row i, column j\n\nletterとp_i_j（pixel）のみ使うのでそれ以外は削除\n\n\n\n\nR\n\ncolname &lt;- read_lines('https://ai.stanford.edu/~btaskar/ocr/letter.names')\ndata &lt;- \n  read_tsv('data/letter.data', col_names = colname) |&gt; \n  # letterとpixelだけ残す\n  select(id, letter, matches('p_\\\\d+_\\\\d+'))\n\n\n\n\nデータの前処理\n\nletterを数値に変換\n\n\n\n\nR\n\ndf &lt;- \n  data |&gt; \n  mutate(\n    letter = factor(letter),\n    # pythonは0からはじまるので-1する（Rは1から）\n    letter_num = as.numeric(letter) - 1\n  )\n\n\n\n訓練データとテストデータに分割\n\n\n\n\nR\n\nsplit &lt;- rsample::initial_split(df, prop = 0.8)\ntrain &lt;- rsample::training(split)\ntest &lt;- rsample::testing(split)\n\n\n\npythonに渡せる形にデータを変換\n\nreticulate::np_array()であらかじめndarrayにしておくと便利\n\n\n\n\n\nR\n\ntrain_x &lt;- \n  train |&gt; \n  select(!c(id, letter, letter_num)) |&gt;\n  as.matrix() |&gt; \n  np_array()\n\ntrain_y &lt;-\n  train |&gt;\n  pull(letter_num) |&gt;\n  np_array()\n\ntest_x &lt;-\n  test |&gt; \n  select(!c(id, letter, letter_num)) |&gt;\n  as.matrix() |&gt; \n  np_array()\n\ntest_y &lt;-\n  test |&gt;\n  pull(letter_num) |&gt;\n  np_array()\n\n\n\n\nニューラルネットで学習\n\n中間層が1つのニューラルネットを作成\n\n入力層は16x8=128（ピクセル）\n中間層は64次元\n出力層はa-zの26分類\n\n\n\n\n\npython\n\nimport tensorflow as tf\nimport numpy as np\n\n# モデルの作成\nmodel = tf.keras.Sequential([\n  tf.keras.layers.Dense(64, activation='relu'),\n  tf.keras.layers.Dense(26, activation='softmax')\n])\n\nmodel.compile(\n  optimizer='adam',\n  loss='sparse_categorical_crossentropy',\n  metrics=['accuracy']\n)\n\n\n\nrで作成したデータをpythonに渡して学習\nr.train_xでRのtrain_xを参照できる\npythonのオブジェクトを渡すときはpy$train_x\n\n\n\n\npython\n\n# モデルの学習\nmodel.fit(r.train_x, r.train_y, epochs=10, verbose=2)\n\n\nEpoch 1/10\n1304/1304 - 1s - loss: 1.3103 - accuracy: 0.6443 - 674ms/epoch - 517us/step\nEpoch 2/10\n1304/1304 - 1s - loss: 0.8173 - accuracy: 0.7754 - 544ms/epoch - 417us/step\nEpoch 3/10\n1304/1304 - 1s - loss: 0.6954 - accuracy: 0.8087 - 544ms/epoch - 417us/step\nEpoch 4/10\n1304/1304 - 1s - loss: 0.6173 - accuracy: 0.8277 - 540ms/epoch - 414us/step\nEpoch 5/10\n1304/1304 - 1s - loss: 0.5613 - accuracy: 0.8428 - 541ms/epoch - 415us/step\nEpoch 6/10\n1304/1304 - 1s - loss: 0.5192 - accuracy: 0.8514 - 543ms/epoch - 416us/step\nEpoch 7/10\n1304/1304 - 1s - loss: 0.4876 - accuracy: 0.8605 - 542ms/epoch - 416us/step\nEpoch 8/10\n1304/1304 - 1s - loss: 0.4605 - accuracy: 0.8670 - 540ms/epoch - 414us/step\nEpoch 9/10\n1304/1304 - 1s - loss: 0.4390 - accuracy: 0.8725 - 539ms/epoch - 413us/step\nEpoch 10/10\n1304/1304 - 1s - loss: 0.4211 - accuracy: 0.8768 - 541ms/epoch - 415us/step\n&lt;keras.src.callbacks.History object at 0x2848e8d90&gt;\n\n\n\n\nテストデータで評価\n\nテストデータでの正答率は77%くらい\n\n\n\n\npython\n\n# モデルの評価\nmodel.evaluate(r.test_x, r.test_y, verbose=2)\n\n\n326/326 - 0s - loss: 0.5127 - accuracy: 0.8534 - 142ms/epoch - 434us/step\n[0.5126842856407166, 0.853417694568634]"
  },
  {
    "objectID": "Others/rlang学習.html",
    "href": "Others/rlang学習.html",
    "title": "rlang学習",
    "section": "",
    "text": "Advanced R"
  },
  {
    "objectID": "Others/rlang学習.html#参考",
    "href": "Others/rlang学習.html#参考",
    "title": "rlang学習",
    "section": "",
    "text": "Advanced R"
  },
  {
    "objectID": "Others/rlang学習.html#下準備",
    "href": "Others/rlang学習.html#下準備",
    "title": "rlang学習",
    "section": "下準備",
    "text": "下準備\n\nlibrary(rlang)\nlibrary(lobstr)"
  },
  {
    "objectID": "Others/rlang学習.html#expression",
    "href": "Others/rlang学習.html#expression",
    "title": "rlang学習",
    "section": "expression",
    "text": "expression\n\nexpr：コードをそのまま捕まえる\n\n作成されたオブジェクト：expression\ncall, symbol, constant, pairlistの4つからなる\n\n\n\nexpr(mean(x, na.rm = TRUE))\n\nmean(x, na.rm = TRUE)\n\nexpr(10 + 100 + 1000)\n\n10 + 100 + 1000\n\n\n\nexprは関数の引数なども書いてあるまま保持する\n\n\n# これはxになる\ncapture_it &lt;- function(x) {\n  expr(x)\n}\n\ncapture_it(a + b + c)\n\nx\n\n\n\n引数は評価した上でコードを保持したい場合はenexprを使う\n\n\ncapture_it &lt;- function(x) {\n  enexpr(x)\n}\ncapture_it(a + b + c)\n\na + b + c"
  },
  {
    "objectID": "Others/rlang学習.html#abstract-syntax-tree-ast-による表現",
    "href": "Others/rlang学習.html#abstract-syntax-tree-ast-による表現",
    "title": "rlang学習",
    "section": "abstract syntax tree (AST) による表現",
    "text": "abstract syntax tree (AST) による表現\n\ncodeはtree形式で表現できる\nfがcall, aがsymbol, “b”がconstantらしい\n\n\nlobstr::ast(f(a, \"b\"))\n\n█─f \n├─a \n└─\"b\" \n\n\n\n+や*も関数であることがわかる\n\n\nlobstr::ast(1 + 2 * 3)\n\n█─`+` \n├─1 \n└─█─`*` \n  ├─2 \n  └─3"
  },
  {
    "objectID": "Others/rlang学習.html#codeを生成するcode",
    "href": "Others/rlang学習.html#codeを生成するcode",
    "title": "rlang学習",
    "section": "codeを生成するcode",
    "text": "codeを生成するcode\n\ncall2\n\n\ncall2('f', 1, 2)\n\nf(1, 2)\n\ncall2('+', 1, call2('*', 2, 3))\n\n1 + 2 * 3\n\n\n\n!!を使ったcodeの生成\n\n捕まえておいたコードを評価したいところで評価する\n\n\n\nxx &lt;- expr(x + x)\nyy &lt;- expr(y + y)\n\nexpr(!!xx / !!yy)\n\n(x + x)/(y + y)"
  },
  {
    "objectID": "Others/rlang学習.html#codeの評価",
    "href": "Others/rlang学習.html#codeの評価",
    "title": "rlang学習",
    "section": "codeの評価",
    "text": "codeの評価\n\neval(expr, env)：exprをenvで評価\n\n\neval(expr(x + y), env(x = 1, y = 10))\n\n[1] 11\n\neval(expr(x + y), env(x = 2, y = 100))\n\n[1] 102\n\n\n\nenvを指定しないときは現在のenvironmentで評価\n\n\nx &lt;- 10\ny &lt;- 100\n\neval(expr(x + y))\n\n[1] 110\n\n\n\nデータを環境として扱う\neval_tidy(expr, data)：exprをdata内部で評価\n\ndataでcurrent environmentを上書きするのでdata-mask呼ばれる\n\nevalでもできないことはないが、落とし穴があるらしい\n\n\ndf &lt;- data.frame(x = 1:5, y = sample(5))\neval_tidy(expr(x + y), df)\n\n[1]  2  5  5  8 10\n\n\n\ndplyrっぽい関数\n\n\nwith2 &lt;- function(df, expr) {\n  eval_tidy(enexpr(expr), df)\n}\n\nwith2(df, x + y)\n\n[1]  2  5  5  8 10"
  },
  {
    "objectID": "Others/rlang学習.html#quosure",
    "href": "Others/rlang学習.html#quosure",
    "title": "rlang学習",
    "section": "Quosure",
    "text": "Quosure\n\nexprだけを使う方法は問題がでてくる\n関数の内部でaを定義してみる\n\n\nwith2 &lt;- function(df, expr) {\n  a &lt;- 1000\n  eval_tidy(enexpr(expr), df)\n}\n\n\nさらに関数の外部（global environment）でもaを定義する\n\n\ndf &lt;- data.frame(x = 1:3)\na &lt;- 10\n\n\nこの状態でwith2を使うと問題が発生\n\nglobal environmentのaではなく、関数内部で定義したaが優先的に使われる\n\n\n\nwith2(df, x + a)\n\n[1] 1001 1002 1003\n\n\n\nquosure：expressionとenvironmentをセットで保持する\n\ndata-maskでは、eval_tidyのdata -&gt; quosureのenv -&gt; global envの順で評価される\n\n\n\nwith2 &lt;- function(df, expr) {\n  a &lt;- 1000\n  eval_tidy(enquo(expr), df)\n}\n\nx &lt;- 10\n\nwith2(df, x + a)\n\n[1] 11 12 13"
  },
  {
    "objectID": "Others/度数から対数尤度を計算.html",
    "href": "Others/度数から対数尤度を計算.html",
    "title": "観測度数・期待度数から対数尤度を計算する方法",
    "section": "",
    "text": "\\(f_{ij}\\)：観測度数\n\n\n\n\n\nB\nB’\n\n\n\n\nA\n\\(f_{11}\\)\n\\(f_{12}\\)\n\n\nA’\n\\(f_{21}\\)\n\\(f_{22}\\)\n\n\n\n\n\\(p_{ij}\\)：セル確率\n\nモデルから計算されたセルの確率\nAとBに関連がないモデルにおいては、期待度数をサンプルサイズで割ったもの\n\n\n\n\n\n\nB\nB’\n\n\n\n\nA\n\\(p_{11}\\)\n\\(p_{12}\\)\n\n\nA’\n\\(p_{21}\\)\n\\(p_{22}\\)\n\n\n\n尤度関数を\\(L\\)とすると\n\\[\nL = \\prod_i \\prod_j p_{ij}^{f_{ij}} =  p_{11}^{f_{11}} \\times p_{12}^{f_{12}} \\times p_{21}^{f_{21}} \\times p_{22}^{f_{22}}\n\\]\n各セルの確率を(観測度数)乗したものを全部掛け算\n対数尤度はこれの対数を取ったもの\n\\[\n\\log L = \\sum_i \\sum_j f_{ij} \\log p_{ij} = f_{11} \\log p_{11} + f_{12} \\log p_{12} + f_{21} \\log p_{21} + f_{22} \\log p_{22}\n\\]\n掛け算が足し算になるので計算しやすい"
  },
  {
    "objectID": "Others/度数から対数尤度を計算.html#観測度数とセル確率から対数尤度を計算",
    "href": "Others/度数から対数尤度を計算.html#観測度数とセル確率から対数尤度を計算",
    "title": "観測度数・期待度数から対数尤度を計算する方法",
    "section": "",
    "text": "\\(f_{ij}\\)：観測度数\n\n\n\n\n\nB\nB’\n\n\n\n\nA\n\\(f_{11}\\)\n\\(f_{12}\\)\n\n\nA’\n\\(f_{21}\\)\n\\(f_{22}\\)\n\n\n\n\n\\(p_{ij}\\)：セル確率\n\nモデルから計算されたセルの確率\nAとBに関連がないモデルにおいては、期待度数をサンプルサイズで割ったもの\n\n\n\n\n\n\nB\nB’\n\n\n\n\nA\n\\(p_{11}\\)\n\\(p_{12}\\)\n\n\nA’\n\\(p_{21}\\)\n\\(p_{22}\\)\n\n\n\n尤度関数を\\(L\\)とすると\n\\[\nL = \\prod_i \\prod_j p_{ij}^{f_{ij}} =  p_{11}^{f_{11}} \\times p_{12}^{f_{12}} \\times p_{21}^{f_{21}} \\times p_{22}^{f_{22}}\n\\]\n各セルの確率を(観測度数)乗したものを全部掛け算\n対数尤度はこれの対数を取ったもの\n\\[\n\\log L = \\sum_i \\sum_j f_{ij} \\log p_{ij} = f_{11} \\log p_{11} + f_{12} \\log p_{12} + f_{21} \\log p_{21} + f_{22} \\log p_{22}\n\\]\n掛け算が足し算になるので計算しやすい"
  },
  {
    "objectID": "Others/度数から対数尤度を計算.html#尤度比カイ2乗値との関係",
    "href": "Others/度数から対数尤度を計算.html#尤度比カイ2乗値との関係",
    "title": "観測度数・期待度数から対数尤度を計算する方法",
    "section": "尤度比カイ2乗値との関係",
    "text": "尤度比カイ2乗値との関係\n尤度比カイ2乗値（\\(G^2\\)）と観測度数（\\(f\\)）・期待度数（\\(F\\)）の関係は以下の式で表される(Hout 1983; McCutcheon 1987)\n\\[\nG^2 = 2 \\sum_i \\sum_j f_{ij} \\log \\frac{f_{ij}}{F_{ij}}\n\\]\nなぜこうなるのか？対数尤度の比から考えてみる。\nまず、AとBに関連がないというモデル（独立モデル）のもとで対数尤度を計算する。この条件ではセル確率\\(p_{ij}\\)は期待度数\\(F_{ij}\\)をサンプルサイズ\\(N\\)で割ったものになる。\n\\[\n\\log L_1 = \\sum_i \\sum_j f_{ij} \\log p_{ij} = \\sum_i \\sum_j f_{ij} \\log \\frac{F_{ij}}{N} = \\sum_i \\sum_j f_{ij} \\log F_{ij} - \\sum_i \\sum_j f_{ij} \\log N\n\\]\n次に、AとBに関連があるというモデル（飽和モデル）のもとで対数尤度を計算する。この条件ではセル確率\\(p_{ij}\\)は観測度数\\(f_{ij}\\)をサンプルサイズ\\(N\\)で割ったものになる。\n\\[\n\\log L_2 = \\sum_i \\sum_j f_{ij} \\log p_{ij} = \\sum_i \\sum_j f_{ij} \\log \\frac{f_{ij}}{N} = \\sum_i \\sum_j f_{ij} \\log f_{ij} - \\sum_i \\sum_j f_{ij} \\log N\n\\]\n対数尤度の差（＝尤度の比）を計算すると\n\\[\\begin{align}\n\\log L_2 - \\log L_1 &= \\sum_i \\sum_j f_{ij} \\log f_{ij} - \\sum_i \\sum_j f_{ij} \\log F_{ij} \\\\\n\\log \\frac{L_2}{L_1} &= \\sum_i \\sum_j (f_{ij} \\log f_{ij} - f_{ij} \\log F_{ij}) \\\\\n\\log \\frac{L_2}{L_1} &= \\sum_i \\sum_j f_{ij} \\log \\frac{f_{ij}}{F_{ij}} \\\\\n\\end{align}\\]\nそれっぽい形になった。"
  },
  {
    "objectID": "Causal_Inference/L-TMLE.html",
    "href": "Causal_Inference/L-TMLE.html",
    "title": "Longitudinal TMLE",
    "section": "",
    "text": "Hernán and Robins (2020) では、TMLEは時間依存交絡などに対処するG-Methodの一般化として提示されている。ここではltmleパッケージを用いた時間依存交絡への対処を中心に記述する。\n\n\n\n\n\nlibrary(tidyverse)\nlibrary(SuperLearner)\nlibrary(ltmle)\nlibrary(randomForest)\nlibrary(ggdag)\nlibrary(future)\n\noptions(mc.cores = 8)\nplan(strategy = 'multisession')\n\nset.seed(95)\n\n\n\n\n以下のDAGを考える\n\ndagify(\n  L1 ~ A0 + U1,\n  A1 ~ A0 + L1,\n  Y ~ U1,\n  exposure = c('A0', 'A1'), \n  outcome = 'Y', \n  coords = list(x = c(A0 = 0, L1 = 1, A1 = 2, U1 = 1, Y = 3),\n                y = c(A0 = 0, L1 = 0, A1 = 0, U1 = -1, Y = 0))\n) %&gt;% \n  ggdag()+\n  theme_dag()\n\n\n\n\nTreatmentはA0とA1であるが、ここからYへのpathはないので、因果効果は全ての組み合わせについて0になる\n具体的には、potential outcomeのすべての組み合わせについて、差分を取った値が0になる\n\\[\\begin{eqnarray}\n\\mathrm{E}[Y^{0, 0} - Y^{1, 0}] &= 0\\\\\n\\mathrm{E}[Y^{0, 0} - Y^{0, 1}] &= 0\\\\\n\\mathrm{E}[Y^{0, 0} - Y^{1, 1}] &= 0\\\\\n\\mathrm{E}[Y^{1, 0} - Y^{0, 1}] &= 0\\\\\n\\mathrm{E}[Y^{1, 0} - Y^{1, 1}] &= 0\\\\\n\\mathrm{E}[Y^{0, 1} - Y^{1, 1}] &= 0\n\\end{eqnarray}\\]\n\ngenerate_data &lt;- function(n){ \n    A0 &lt;- rbinom(n, size=1, prob=0.3) # binary treatment\n    U1 &lt;- rnorm(n, mean = 0, sd = 1) # latent continuous confounder\n    L1 &lt;- 0.1 + 0.3*A0 + 0.3*U1 + rnorm(n, sd = 0.1)\n    A1 &lt;- rbinom(n, size=1, prob = plogis(-0.2 + 0.3*L1 + 0.02*L1^2)) # binary treatment\n    Y &lt;- -0.2 + 0.5*U1 + rnorm(n, sd = 0.1) # continuous outcome depends on confounders\n    return(tibble(Y, A0, A1, L1, U1))\n}\n\ndata_obs &lt;- generate_data(1000)\n\nA0の因果効果\n\ndata_obs %&gt;% \n  lm(Y ~ A0, data = .) %&gt;% \n  summary()\n\n\nCall:\nlm(formula = Y ~ A0, data = .)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.0295 -0.3631  0.0081  0.3323  1.8426 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.20740    0.01898 -10.925   &lt;2e-16 ***\nA0           0.02873    0.03575   0.804    0.422    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5087 on 998 degrees of freedom\nMultiple R-squared:  0.0006469, Adjusted R-squared:  -0.0003545 \nF-statistic: 0.646 on 1 and 998 DF,  p-value: 0.4217\n\n\n\ncollider bias\n\n\\(L_1\\)を条件づけると、\\(A_0 \\to L_1 \\gets U_1 \\to Y\\)というパスが開いて、バイアスをもたらす\n\ndata_obs %&gt;% \n  lm(Y ~ A0 + L1, data = .) %&gt;% \n  summary()\n\n\nCall:\nlm(formula = Y ~ A0 + L1, data = .)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.55477 -0.13977 -0.00681  0.12914  0.54753 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.352169   0.007096  -49.63   &lt;2e-16 ***\nA0          -0.469341   0.014311  -32.80   &lt;2e-16 ***\nL1           1.539818   0.018919   81.39   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1841 on 997 degrees of freedom\nMultiple R-squared:  0.8693,    Adjusted R-squared:  0.869 \nF-statistic:  3315 on 2 and 997 DF,  p-value: &lt; 2.2e-16\n\n\n\ndata_obs %&gt;% \n  lm(Y ~ A0*A1, data = .) %&gt;% \n  summary()\n\n\nCall:\nlm(formula = Y ~ A0 * A1, data = .)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.01955 -0.36365  0.00781  0.33180  1.85253 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.21730    0.02598  -8.364   &lt;2e-16 ***\nA0           0.04341    0.05053   0.859    0.390    \nA1           0.02128    0.03809   0.559    0.576    \nA0:A1       -0.03064    0.07162  -0.428    0.669    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5091 on 996 degrees of freedom\nMultiple R-squared:  0.0009839, Adjusted R-squared:  -0.002025 \nF-statistic: 0.327 on 3 and 996 DF,  p-value: 0.8059\n\n\n\n\n\n\nHernán and Robins (2020) にて紹介されている方法\n以下のウェイトを作成する\n\\[\nW^{A_0, A_1} = \\frac{1}{f(A_0 | L_0)} \\times \\frac{1}{f(A_1 | A_0, L_0, L_1)}\n\\]\n今回はベースライン共変量がない（\\(L_0 = \\varnothing\\)）ので\n\\[\nW^{A_0, A_1} = \\frac{1}{f(A_0)} \\times \\frac{1}{f(A_1 | A_0, L_1)}\n\\]\n\nres_ipw &lt;- \n  broom::augment_columns(\n    glm(A0 ~ 1, data = data_obs, family = 'binomial'), \n    data = data_obs, \n    type.predict = 'response'\n  ) %&gt;% \n  # A0に対するウェイト\n  mutate(\n    ipw_A0 = case_when(A0 == 1 ~ 1 / .fitted,\n                       A0 == 0 ~ 1 / (1 - .fitted))\n  ) %&gt;% \n  select(Y:U1, ipw_A0) %&gt;% \n  broom::augment_columns(\n    glm(A1 ~ A0 + L1 + I(L1^2), data = data_obs, family = 'binomial'),\n    data = .,\n    type.predict = 'response'\n  ) %&gt;% \n  mutate(\n    # A1に対するウェイト\n    ipw_A1 = case_when(A1 == 1 ~ 1 / .fitted,\n                       A1 == 0 ~ 1 / (1 - .fitted))\n  ) %&gt;% \n  select(Y:U1, ipw_A0, ipw_A1) %&gt;% \n  # A0に対するウェイトとA1に対するウェイトをかけ算\n  mutate(ipw = ipw_A0*ipw_A1) %&gt;% \n  # 重み付け推定\n  summarise(CFmean = weighted.mean(Y, ipw), .by = c(A0, A1)) %&gt;% \n  arrange(A0, A1)\n\nipwによる各treatment strategyにおけるcounterfactual mean\n\nres_ipw\n\n# A tibble: 4 × 3\n     A0    A1 CFmean\n  &lt;int&gt; &lt;int&gt;  &lt;dbl&gt;\n1     0     0 -0.208\n2     0     1 -0.207\n3     1     0 -0.166\n4     1     1 -0.189\n\n\ncounterfactual meanの差分\n\ncross_join(\n  res_ipw %&gt;% \n    mutate(A0_A1 = str_c(A0, ',', A1)) %&gt;% \n    select(A0_A1, CFmean),\n  res_ipw %&gt;% \n    mutate(A0_A1 = str_c(A0, ',', A1)) %&gt;% \n    select(A0_A1, CFmean)\n  ) %&gt;% \n  filter(A0_A1.x != A0_A1.y) %&gt;% \n  mutate(ATE = CFmean.y - CFmean.x)\n\n# A tibble: 12 × 5\n   A0_A1.x CFmean.x A0_A1.y CFmean.y       ATE\n   &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;      &lt;dbl&gt;     &lt;dbl&gt;\n 1 0,0       -0.208 0,1       -0.207  0.000654\n 2 0,0       -0.208 1,0       -0.166  0.0419  \n 3 0,0       -0.208 1,1       -0.189  0.0190  \n 4 0,1       -0.207 0,0       -0.208 -0.000654\n 5 0,1       -0.207 1,0       -0.166  0.0412  \n 6 0,1       -0.207 1,1       -0.189  0.0184  \n 7 1,0       -0.166 0,0       -0.208 -0.0419  \n 8 1,0       -0.166 0,1       -0.207 -0.0412  \n 9 1,0       -0.166 1,1       -0.189 -0.0229  \n10 1,1       -0.189 0,0       -0.208 -0.0190  \n11 1,1       -0.189 0,1       -0.207 -0.0184  \n12 1,1       -0.189 1,0       -0.166  0.0229  \n\n\n\n\n\n\nltmleによる実装\n\n\nres_tmle &lt;- \n  ltmle::ltmle(\n    # データセットの列の順番が大切\n    data = data_obs %&gt;% select(A0, L1, A1, Y), \n    Anodes = c('A0', 'A1'), \n    Lnodes = 'L1',\n    Ynodes = 'Y',\n    abar = list(treatment = c(1, 1), control = c(0, 0)), \n    # Yrange = c(-100, 100), \n    SL.library = c('SL.glm', 'SL.gam', 'SL.randomForest')\n  )\n\n\nsummary(res_tmle)\n\nEstimator:  tmle \nCall:\nltmle::ltmle(data = data_obs %&gt;% select(A0, L1, A1, Y), Anodes = c(\"A0\", \n    \"A1\"), Lnodes = \"L1\", Ynodes = \"Y\", abar = list(treatment = c(1, \n    1), control = c(0, 0)), SL.library = c(\"SL.glm\", \"SL.gam\", \n    \"SL.randomForest\"))\n\nTreatment Estimate:\n   Parameter Estimate:  -0.17554 \n    Estimated Std Err:  0.033541 \n              p-value:  &lt;2e-16 \n    95% Conf Interval: (-0.24128, -0.1098) \n\nControl Estimate:\n   Parameter Estimate:  -0.20361 \n    Estimated Std Err:  0.019777 \n              p-value:  &lt;2e-16 \n    95% Conf Interval: (-0.24237, -0.16485) \n\nAdditive Treatment Effect:\n   Parameter Estimate:  0.028069 \n    Estimated Std Err:  0.038937 \n              p-value:  0.47099 \n    95% Conf Interval: (-0.048247, 0.10438) \n\n\n\n\n\nltmleはメジャー（？）だが、あんまり汎用性が高いと言えず、開発も盛んなようには見えない。ほかにlongitudinal TMLEを実装しているパッケージとしてlmtpやstremrがある。lmtpはCRANにも登録してあり、試してみる価値あり。stremrはさまざまなアプローチの統合を目指す非常に意欲的なパッケージだが、開発は止まっている？っぽい。"
  },
  {
    "objectID": "Causal_Inference/L-TMLE.html#下準備",
    "href": "Causal_Inference/L-TMLE.html#下準備",
    "title": "Longitudinal TMLE",
    "section": "",
    "text": "library(tidyverse)\nlibrary(SuperLearner)\nlibrary(ltmle)\nlibrary(randomForest)\nlibrary(ggdag)\nlibrary(future)\n\noptions(mc.cores = 8)\nplan(strategy = 'multisession')\n\nset.seed(95)\n\n\n\n\n以下のDAGを考える\n\ndagify(\n  L1 ~ A0 + U1,\n  A1 ~ A0 + L1,\n  Y ~ U1,\n  exposure = c('A0', 'A1'), \n  outcome = 'Y', \n  coords = list(x = c(A0 = 0, L1 = 1, A1 = 2, U1 = 1, Y = 3),\n                y = c(A0 = 0, L1 = 0, A1 = 0, U1 = -1, Y = 0))\n) %&gt;% \n  ggdag()+\n  theme_dag()\n\n\n\n\nTreatmentはA0とA1であるが、ここからYへのpathはないので、因果効果は全ての組み合わせについて0になる\n具体的には、potential outcomeのすべての組み合わせについて、差分を取った値が0になる\n\\[\\begin{eqnarray}\n\\mathrm{E}[Y^{0, 0} - Y^{1, 0}] &= 0\\\\\n\\mathrm{E}[Y^{0, 0} - Y^{0, 1}] &= 0\\\\\n\\mathrm{E}[Y^{0, 0} - Y^{1, 1}] &= 0\\\\\n\\mathrm{E}[Y^{1, 0} - Y^{0, 1}] &= 0\\\\\n\\mathrm{E}[Y^{1, 0} - Y^{1, 1}] &= 0\\\\\n\\mathrm{E}[Y^{0, 1} - Y^{1, 1}] &= 0\n\\end{eqnarray}\\]\n\ngenerate_data &lt;- function(n){ \n    A0 &lt;- rbinom(n, size=1, prob=0.3) # binary treatment\n    U1 &lt;- rnorm(n, mean = 0, sd = 1) # latent continuous confounder\n    L1 &lt;- 0.1 + 0.3*A0 + 0.3*U1 + rnorm(n, sd = 0.1)\n    A1 &lt;- rbinom(n, size=1, prob = plogis(-0.2 + 0.3*L1 + 0.02*L1^2)) # binary treatment\n    Y &lt;- -0.2 + 0.5*U1 + rnorm(n, sd = 0.1) # continuous outcome depends on confounders\n    return(tibble(Y, A0, A1, L1, U1))\n}\n\ndata_obs &lt;- generate_data(1000)\n\nA0の因果効果\n\ndata_obs %&gt;% \n  lm(Y ~ A0, data = .) %&gt;% \n  summary()\n\n\nCall:\nlm(formula = Y ~ A0, data = .)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.0295 -0.3631  0.0081  0.3323  1.8426 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.20740    0.01898 -10.925   &lt;2e-16 ***\nA0           0.02873    0.03575   0.804    0.422    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5087 on 998 degrees of freedom\nMultiple R-squared:  0.0006469, Adjusted R-squared:  -0.0003545 \nF-statistic: 0.646 on 1 and 998 DF,  p-value: 0.4217\n\n\n\ncollider bias\n\n\\(L_1\\)を条件づけると、\\(A_0 \\to L_1 \\gets U_1 \\to Y\\)というパスが開いて、バイアスをもたらす\n\ndata_obs %&gt;% \n  lm(Y ~ A0 + L1, data = .) %&gt;% \n  summary()\n\n\nCall:\nlm(formula = Y ~ A0 + L1, data = .)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.55477 -0.13977 -0.00681  0.12914  0.54753 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.352169   0.007096  -49.63   &lt;2e-16 ***\nA0          -0.469341   0.014311  -32.80   &lt;2e-16 ***\nL1           1.539818   0.018919   81.39   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1841 on 997 degrees of freedom\nMultiple R-squared:  0.8693,    Adjusted R-squared:  0.869 \nF-statistic:  3315 on 2 and 997 DF,  p-value: &lt; 2.2e-16\n\n\n\ndata_obs %&gt;% \n  lm(Y ~ A0*A1, data = .) %&gt;% \n  summary()\n\n\nCall:\nlm(formula = Y ~ A0 * A1, data = .)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.01955 -0.36365  0.00781  0.33180  1.85253 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.21730    0.02598  -8.364   &lt;2e-16 ***\nA0           0.04341    0.05053   0.859    0.390    \nA1           0.02128    0.03809   0.559    0.576    \nA0:A1       -0.03064    0.07162  -0.428    0.669    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5091 on 996 degrees of freedom\nMultiple R-squared:  0.0009839, Adjusted R-squared:  -0.002025 \nF-statistic: 0.327 on 3 and 996 DF,  p-value: 0.8059"
  },
  {
    "objectID": "Causal_Inference/L-TMLE.html#ipw",
    "href": "Causal_Inference/L-TMLE.html#ipw",
    "title": "Longitudinal TMLE",
    "section": "",
    "text": "Hernán and Robins (2020) にて紹介されている方法\n以下のウェイトを作成する\n\\[\nW^{A_0, A_1} = \\frac{1}{f(A_0 | L_0)} \\times \\frac{1}{f(A_1 | A_0, L_0, L_1)}\n\\]\n今回はベースライン共変量がない（\\(L_0 = \\varnothing\\)）ので\n\\[\nW^{A_0, A_1} = \\frac{1}{f(A_0)} \\times \\frac{1}{f(A_1 | A_0, L_1)}\n\\]\n\nres_ipw &lt;- \n  broom::augment_columns(\n    glm(A0 ~ 1, data = data_obs, family = 'binomial'), \n    data = data_obs, \n    type.predict = 'response'\n  ) %&gt;% \n  # A0に対するウェイト\n  mutate(\n    ipw_A0 = case_when(A0 == 1 ~ 1 / .fitted,\n                       A0 == 0 ~ 1 / (1 - .fitted))\n  ) %&gt;% \n  select(Y:U1, ipw_A0) %&gt;% \n  broom::augment_columns(\n    glm(A1 ~ A0 + L1 + I(L1^2), data = data_obs, family = 'binomial'),\n    data = .,\n    type.predict = 'response'\n  ) %&gt;% \n  mutate(\n    # A1に対するウェイト\n    ipw_A1 = case_when(A1 == 1 ~ 1 / .fitted,\n                       A1 == 0 ~ 1 / (1 - .fitted))\n  ) %&gt;% \n  select(Y:U1, ipw_A0, ipw_A1) %&gt;% \n  # A0に対するウェイトとA1に対するウェイトをかけ算\n  mutate(ipw = ipw_A0*ipw_A1) %&gt;% \n  # 重み付け推定\n  summarise(CFmean = weighted.mean(Y, ipw), .by = c(A0, A1)) %&gt;% \n  arrange(A0, A1)\n\nipwによる各treatment strategyにおけるcounterfactual mean\n\nres_ipw\n\n# A tibble: 4 × 3\n     A0    A1 CFmean\n  &lt;int&gt; &lt;int&gt;  &lt;dbl&gt;\n1     0     0 -0.208\n2     0     1 -0.207\n3     1     0 -0.166\n4     1     1 -0.189\n\n\ncounterfactual meanの差分\n\ncross_join(\n  res_ipw %&gt;% \n    mutate(A0_A1 = str_c(A0, ',', A1)) %&gt;% \n    select(A0_A1, CFmean),\n  res_ipw %&gt;% \n    mutate(A0_A1 = str_c(A0, ',', A1)) %&gt;% \n    select(A0_A1, CFmean)\n  ) %&gt;% \n  filter(A0_A1.x != A0_A1.y) %&gt;% \n  mutate(ATE = CFmean.y - CFmean.x)\n\n# A tibble: 12 × 5\n   A0_A1.x CFmean.x A0_A1.y CFmean.y       ATE\n   &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;      &lt;dbl&gt;     &lt;dbl&gt;\n 1 0,0       -0.208 0,1       -0.207  0.000654\n 2 0,0       -0.208 1,0       -0.166  0.0419  \n 3 0,0       -0.208 1,1       -0.189  0.0190  \n 4 0,1       -0.207 0,0       -0.208 -0.000654\n 5 0,1       -0.207 1,0       -0.166  0.0412  \n 6 0,1       -0.207 1,1       -0.189  0.0184  \n 7 1,0       -0.166 0,0       -0.208 -0.0419  \n 8 1,0       -0.166 0,1       -0.207 -0.0412  \n 9 1,0       -0.166 1,1       -0.189 -0.0229  \n10 1,1       -0.189 0,0       -0.208 -0.0190  \n11 1,1       -0.189 0,1       -0.207 -0.0184  \n12 1,1       -0.189 1,0       -0.166  0.0229"
  },
  {
    "objectID": "Causal_Inference/L-TMLE.html#longitudinal-tmle",
    "href": "Causal_Inference/L-TMLE.html#longitudinal-tmle",
    "title": "Longitudinal TMLE",
    "section": "",
    "text": "ltmleによる実装\n\n\nres_tmle &lt;- \n  ltmle::ltmle(\n    # データセットの列の順番が大切\n    data = data_obs %&gt;% select(A0, L1, A1, Y), \n    Anodes = c('A0', 'A1'), \n    Lnodes = 'L1',\n    Ynodes = 'Y',\n    abar = list(treatment = c(1, 1), control = c(0, 0)), \n    # Yrange = c(-100, 100), \n    SL.library = c('SL.glm', 'SL.gam', 'SL.randomForest')\n  )\n\n\nsummary(res_tmle)\n\nEstimator:  tmle \nCall:\nltmle::ltmle(data = data_obs %&gt;% select(A0, L1, A1, Y), Anodes = c(\"A0\", \n    \"A1\"), Lnodes = \"L1\", Ynodes = \"Y\", abar = list(treatment = c(1, \n    1), control = c(0, 0)), SL.library = c(\"SL.glm\", \"SL.gam\", \n    \"SL.randomForest\"))\n\nTreatment Estimate:\n   Parameter Estimate:  -0.17554 \n    Estimated Std Err:  0.033541 \n              p-value:  &lt;2e-16 \n    95% Conf Interval: (-0.24128, -0.1098) \n\nControl Estimate:\n   Parameter Estimate:  -0.20361 \n    Estimated Std Err:  0.019777 \n              p-value:  &lt;2e-16 \n    95% Conf Interval: (-0.24237, -0.16485) \n\nAdditive Treatment Effect:\n   Parameter Estimate:  0.028069 \n    Estimated Std Err:  0.038937 \n              p-value:  0.47099 \n    95% Conf Interval: (-0.048247, 0.10438)"
  },
  {
    "objectID": "Causal_Inference/L-TMLE.html#他のパッケージ",
    "href": "Causal_Inference/L-TMLE.html#他のパッケージ",
    "title": "Longitudinal TMLE",
    "section": "",
    "text": "ltmleはメジャー（？）だが、あんまり汎用性が高いと言えず、開発も盛んなようには見えない。ほかにlongitudinal TMLEを実装しているパッケージとしてlmtpやstremrがある。lmtpはCRANにも登録してあり、試してみる価値あり。stremrはさまざまなアプローチの統合を目指す非常に意欲的なパッケージだが、開発は止まっている？っぽい。"
  },
  {
    "objectID": "Causal_Inference/Sudharsanan_Bijlsma_2021_読解.html",
    "href": "Causal_Inference/Sudharsanan_Bijlsma_2021_読解.html",
    "title": "Sudharsanan_Bijlsma_2021_Educational note 読解",
    "section": "",
    "text": "パッケージ\n\n\nlibrary(tidyverse)\nlibrary(cfdecomp)\n\nkable &lt;- partial(\n  knitr::kable,\n  digits = 3\n)\n\n\nデータ\n\n\n# the decomposition functions in our package are computationally intensive\n# to make the example run quick, I perform it on a subsample (n=125) of the data:\nset.seed(100)\ndata &lt;- \n  cfd.example.data[sample(1000),] |&gt; \n  select(SES, age, med.gauss, out.gauss, id) |&gt; \n  as_tibble()"
  },
  {
    "objectID": "Causal_Inference/Sudharsanan_Bijlsma_2021_読解.html#下準備",
    "href": "Causal_Inference/Sudharsanan_Bijlsma_2021_読解.html#下準備",
    "title": "Sudharsanan_Bijlsma_2021_Educational note 読解",
    "section": "",
    "text": "パッケージ\n\n\nlibrary(tidyverse)\nlibrary(cfdecomp)\n\nkable &lt;- partial(\n  knitr::kable,\n  digits = 3\n)\n\n\nデータ\n\n\n# the decomposition functions in our package are computationally intensive\n# to make the example run quick, I perform it on a subsample (n=125) of the data:\nset.seed(100)\ndata &lt;- \n  cfd.example.data[sample(1000),] |&gt; \n  select(SES, age, med.gauss, out.gauss, id) |&gt; \n  as_tibble()"
  },
  {
    "objectID": "Causal_Inference/Sudharsanan_Bijlsma_2021_読解.html#パッケージによる推定",
    "href": "Causal_Inference/Sudharsanan_Bijlsma_2021_読解.html#パッケージによる推定",
    "title": "Sudharsanan_Bijlsma_2021_Educational note 読解",
    "section": "パッケージによる推定",
    "text": "パッケージによる推定\n\ncfdecompパッケージで推定\n\n\n# cfd.mean \nmean.results.1 &lt;- \n  cfd.mean(\n    formula.y = out.gauss ~ SES * med.gauss * age,\n    formula.m = med.gauss ~ SES * age,\n    mediator = 'med.gauss',\n    group = 'SES',\n    data = as.data.frame(data),\n    family.y = 'gaussian',\n    family.m = 'gaussian',\n    bs.size=250,\n    mc.size=10,\n    alpha=0.05,\n    # cluster.sample=FALSE,\n    # cluster.name='id'\n  )\n\n\nSES2とSES3のmediatorの分布をSES1のmediatorの分布に揃える\n\n\ntibble(\n  category = c(\"SES1\", \"SES2\", \"SES3\"),\n  factual_mean = c(\n    mean(mean.results.1$out_nc_y[, 1]),\n    mean(mean.results.1$out_nc_y[, 2]),\n    mean(mean.results.1$out_nc_y[, 3])\n  ),\n  # and after giving the gaussian mediator of SES group 2 the distribution of the one in group 1\n  # the difference becomes:\n  counterfactual_mean = c(\n    mean(mean.results.1$out_cf_y[, 1]),\n    mean(mean.results.1$out_cf_y[, 2]),\n    mean(mean.results.1$out_cf_y[, 3])\n  )) |&gt; \n  kable()\n\n\n\n\ncategory\nfactual_mean\ncounterfactual_mean\n\n\n\n\nSES1\n4.306\n4.306\n\n\nSES2\n3.239\n3.511\n\n\nSES3\n2.212\n2.946"
  },
  {
    "objectID": "Causal_Inference/Sudharsanan_Bijlsma_2021_読解.html#自分でコードを書いてみる",
    "href": "Causal_Inference/Sudharsanan_Bijlsma_2021_読解.html#自分でコードを書いてみる",
    "title": "Sudharsanan_Bijlsma_2021_Educational note 読解",
    "section": "自分でコードを書いてみる",
    "text": "自分でコードを書いてみる"
  },
  {
    "objectID": "Causal_Inference/Sudharsanan_Bijlsma_2021_読解.html#step-1-regression-estimates",
    "href": "Causal_Inference/Sudharsanan_Bijlsma_2021_読解.html#step-1-regression-estimates",
    "title": "Sudharsanan_Bijlsma_2021_Educational note 読解",
    "section": "step 1: regression estimates",
    "text": "step 1: regression estimates\n\nmediatorとoutcomeのモデルをデータから推定\n\n\nmediator_model &lt;- lm(med.gauss ~ SES * age, data = data)\noutcome_model &lt;- lm(out.gauss ~ SES * age * med.gauss, data = data)"
  },
  {
    "objectID": "Causal_Inference/Sudharsanan_Bijlsma_2021_読解.html#step-2-simulate-the-natural-course-pseudo-population",
    "href": "Causal_Inference/Sudharsanan_Bijlsma_2021_読解.html#step-2-simulate-the-natural-course-pseudo-population",
    "title": "Sudharsanan_Bijlsma_2021_Educational note 読解",
    "section": "step 2: simulate the natural-course pseudo-population",
    "text": "step 2: simulate the natural-course pseudo-population\n\n推定したmediatorのモデルから、mediatorの「分布」を再現\n\n\\[\\begin{align*}\nMed_i = \\mathrm{E}[Med | X] + e_i \\\\\ne_i \\sim \\mathrm{N}(0, \\sigma)\n\\end{align*}\\]\n\nmediatorの分布のパラメータを取得\n\n\n# predict mediator\n# mediatorの「分布」のパラメータを取得\npred_mean_m &lt;- predict(mediator_model, newdata = data, type = \"response\")\nresidual_ref_m &lt;- mediator_model$residuals\nsd_ref_m &lt;- sd(residual_ref_m)\n\n\n推定したパラメータをもとに、mediatorの値をシミュレート\n\n\ndf_nc_med &lt;- \n  data |&gt; \n  mutate(\n    # ランダム性なし\n    pred_med = pred_mean_m,\n    # ランダム性をもたせる1（推定した標準偏差のパラメータを使用）\n    pred_med_draw_1 = rnorm(n(), mean = pred_mean_m, sd = sd_ref_m),\n    # ランダム性をもたせる2（残差からランダムにサンプリング）\n    pred_med_draw_2 = pred_mean_m + sample(residual_ref_m, n(), replace = TRUE)\n  )\n\ndf_nc_med |&gt; \n  summarise(\n    across(c(med.gauss, pred_med:pred_med_draw_2), mean),\n    .by = SES\n  ) |&gt; \n  arrange(SES) |&gt; \n  kable()\n\n\n\n\nSES\nmed.gauss\npred_med\npred_med_draw_1\npred_med_draw_2\n\n\n\n\n1\n8.428\n8.428\n8.418\n8.286\n\n\n2\n7.223\n7.223\n7.276\n7.454\n\n\n3\n5.410\n5.410\n5.579\n5.374\n\n\n\n\n\n\nシミュレートした値をoutcomeモデルに代入して予測値を計算・集計\n\n\ndf_nc_med |&gt; \n  mutate(\n    # ランダム性なし\n    pred_out = predict(\n      outcome_model, newdata = df_nc_med |&gt; mutate(med.gauss = pred_med)\n    ),\n    # ランダム性をもたせる1（推定した標準偏差のパラメータを使用）\n    pred_out_draw_1 = predict(\n      outcome_model, newdata = df_nc_med |&gt; mutate(med.gauss = pred_med_draw_1)\n    ),\n    # ランダム性をもたせる2（残差からランダムにサンプリング）\n    pred_out_draw_2 = predict(\n      outcome_model, newdata = df_nc_med |&gt; mutate(med.gauss = pred_med_draw_2)\n    )\n  ) |&gt; \n  summarise(\n    across(c(out.gauss, pred_out:pred_out_draw_2), mean),\n    .by = SES\n  ) |&gt; \n  arrange(SES) |&gt; \n  kable()\n\n\n\n\nSES\nout.gauss\npred_out\npred_out_draw_1\npred_out_draw_2\n\n\n\n\n1\n4.309\n4.309\n4.312\n4.286\n\n\n2\n3.237\n3.237\n3.249\n3.292\n\n\n3\n2.217\n2.217\n2.254\n2.214"
  },
  {
    "objectID": "Causal_Inference/Sudharsanan_Bijlsma_2021_読解.html#step-3-simulate-the-counterfactual-pseudo-population",
    "href": "Causal_Inference/Sudharsanan_Bijlsma_2021_読解.html#step-3-simulate-the-counterfactual-pseudo-population",
    "title": "Sudharsanan_Bijlsma_2021_Educational note 読解",
    "section": "step 3: simulate the counterfactual pseudo-population",
    "text": "step 3: simulate the counterfactual pseudo-population\n\n推定したmediatorのモデルにおいて、全員のSESが1だった場合のmediatorの分布を再現\n全員のSESを1にしてmediatorのパラメータを取得\n回帰モデルでは残差の部分は共変量に依存しない（SES間で分布が同じ、平均0・共通の標準偏差の正規分布）\n\nならばSESが1のグループの標準偏差を使わなくても良いのでは？（全体の標準偏差でもよい）\n標準偏差もグループによって異なる、といったモデルの場合にはどうなるか？\n\n\n\n# 平均\npred_mean_m_SES1 &lt;- predict(mediator_model, newdata = data |&gt; mutate(SES = '1'))\n# SES = 1のグループの残差\nresidual_ref_m_SES1 &lt;-\n  broom::augment(mediator_model)  |&gt;\n  filter(SES == '1') |&gt; \n  pull(.resid)\n# 標準偏差\nsd_ref_m_SES1 &lt;- sd(residual_ref_m_SES1)\n\n\n推定したパラメータをもとに、mediatorの値をシミュレート\n\n\ndf_cf_med &lt;- \n  data |&gt; \n  mutate(\n    # ランダム性なし\n    pred_med_SES1 = pred_mean_m_SES1,\n    # ランダム性をもたせる1（推定した標準偏差のパラメータを使用）\n    pred_med_draw_1_SES1 = rnorm(n(), mean = pred_mean_m_SES1, sd = sd_ref_m_SES1),\n    # ランダム性をもたせる2（残差からランダムにサンプリング）\n    pred_med_draw_2_SES1 = pred_mean_m_SES1 + sample(residual_ref_m_SES1, n(), replace = TRUE)\n  )\n\ndf_cf_med |&gt; \n  summarise(\n    across(c(med.gauss, pred_med_SES1:pred_med_draw_2_SES1), mean),\n    .by = SES\n  ) |&gt; \n  arrange(SES) |&gt; \n  kable()\n\n\n\n\n\n\n\n\n\n\n\nSES\nmed.gauss\npred_med_SES1\npred_med_draw_1_SES1\npred_med_draw_2_SES1\n\n\n\n\n1\n8.428\n8.428\n8.386\n8.632\n\n\n2\n7.223\n8.337\n8.258\n8.292\n\n\n3\n5.410\n8.442\n8.292\n8.593\n\n\n\n\n\n\nシミュレートした値をoutcomeモデルに代入して予測値を計算・集計\n\n\ndf_cf_med |&gt; \n  mutate(\n    # ランダム性なし\n    pred_out_SES1 = predict(\n      outcome_model, newdata = df_cf_med |&gt; mutate(med.gauss = pred_med_SES1)\n    ),\n    # ランダム性をもたせる1（推定した標準偏差のパラメータを使用）\n    pred_out_draw_1_SES1 = predict(\n      outcome_model, newdata = df_cf_med |&gt; mutate(med.gauss = pred_med_draw_1_SES1)\n    ),\n    # ランダム性をもたせる2（残差からランダムにサンプリング）\n    pred_out_draw_2_SES1 = predict(\n      outcome_model, newdata = df_cf_med |&gt; mutate(med.gauss = pred_med_draw_2_SES1)\n    )\n  ) |&gt; \n  summarise(\n    across(c(out.gauss, pred_out_SES1:pred_out_draw_2_SES1), mean),\n    .by = SES\n  ) |&gt; \n  arrange(SES) |&gt; \n  kable()\n\n\n\n\n\n\n\n\n\n\n\nSES\nout.gauss\npred_out_SES1\npred_out_draw_1_SES1\npred_out_draw_2_SES1\n\n\n\n\n1\n4.309\n4.309\n4.303\n4.347\n\n\n2\n3.237\n3.507\n3.485\n3.498\n\n\n3\n2.217\n2.949\n2.920\n2.979"
  },
  {
    "objectID": "Causal_Inference/Sudharsanan_Bijlsma_2021_読解.html#monte-carloとbootstrapの実装",
    "href": "Causal_Inference/Sudharsanan_Bijlsma_2021_読解.html#monte-carloとbootstrapの実装",
    "title": "Sudharsanan_Bijlsma_2021_Educational note 読解",
    "section": "monte carloとbootstrapの実装",
    "text": "monte carloとbootstrapの実装\n\n実際にはrandom drawは一回ではなく何回か行うことで不確実性を表現する\n標準誤差の推定のためにbootstrap法も必要\nまずはtreatmentとoutcomeのモデルを推定し、パラメータを取得\n\n\n# パラメータ推定\nestimate_model &lt;- function(data) {\n  \n  mediator_model &lt;- lm(med.gauss ~ SES * age, data = data)\n  outcome_model &lt;- lm(out.gauss ~ SES * age * med.gauss, data = data)\n  \n  pred_mean_m &lt;- predict(mediator_model, newdata = data, type = \"response\")\n  residual_ref_m &lt;- mediator_model$residuals\n  sd_ref_m &lt;- sd(residual_ref_m)\n\n  pred_mean_m_SES1 &lt;- predict(mediator_model, newdata = data |&gt; mutate(SES = '1'))\n  residual_ref_m_SES1 &lt;-\n    broom::augment(mediator_model)  |&gt;\n    filter(SES == '1') |&gt; \n    pull(.resid)\n  sd_ref_m_SES1 &lt;- sd(residual_ref_m_SES1)\n\n}\n\n\nパラメータをもとにmediatorをシミュレートするのを何回か繰り返す\n\n\nmontecarlo_sampling &lt;- function(data, mc = 10) {\n  # パラメータ推定\n  estimate_model(data)\n  # モンテカルロシミュレーション\n  map(1:mc, \\(mc) {\n    # mediatorサンプリング\n    boot_sample &lt;- \n      data |&gt; \n      mutate(\n        pred_med_draw_1 = rnorm(n(), mean = pred_mean_m, sd = sd_ref_m),\n        pred_med_draw_2 = pred_mean_m + sample(residual_ref_m, n(), replace = TRUE),\n        pred_med_draw_1_SES1 = rnorm(n(), mean = pred_mean_m_SES1, sd = sd_ref_m_SES1),\n        pred_med_draw_2_SES1 = pred_mean_m_SES1 + sample(residual_ref_m_SES1, n(), replace = TRUE),\n      )\n    # サンプリングしたものからoutcome予測\n    boot_sample |&gt; \n      mutate(\n        pred_out_draw_1 = predict(\n          outcome_model, newdata = boot_sample |&gt; mutate(med.gauss = pred_med_draw_1)\n        ),\n        pred_out_draw_2 = predict(\n          outcome_model, newdata = boot_sample |&gt; mutate(med.gauss = pred_med_draw_2)\n        ),\n        pred_out_draw_1_SES1 = predict(\n          outcome_model, newdata = boot_sample |&gt; mutate(med.gauss = pred_med_draw_1_SES1)\n        ),\n        pred_out_draw_2_SES1 = predict(\n          outcome_model, newdata = boot_sample |&gt; mutate(med.gauss = pred_med_draw_2_SES1)\n        )\n      ) |&gt; \n      group_by(SES) |&gt; \n      summarise(across(c(pred_out_draw_1:pred_out_draw_2_SES1), mean))\n    \n  }) |&gt; \n    list_rbind(names_to = 'mc') |&gt; \n    # シミュレーション結果を集計\n    group_by(SES) |&gt; \n    summarise(across(c(pred_out_draw_1:pred_out_draw_2_SES1), mean))\n  \n}\n\n\nこれをbootstrapで繰り返す\n\n\nresult &lt;- \n  map(1:250, \\(index) {\n    # bootstrapサンプル発生\n    bootsample &lt;- slice_sample(data, prop = 1, replace = TRUE)\n    montecarlo_sampling(bootsample, mc = 10) \n  }) |&gt; \n  list_rbind(names_to = 'index')\n\n\n結果を集計\n\n\nresult |&gt; \n  pivot_longer(\n    cols = c(pred_out_draw_1:pred_out_draw_2_SES1), \n    names_to = 'type', \n    values_to = 'value'\n  ) |&gt; \n  summarise(\n    mean = mean(value), \n    conf.low = quantile(value, 0.025),\n    conf.high = quantile(value, 0.975),\n    .by = c(SES, type)\n  ) |&gt; \n  arrange(type) |&gt; \n  kable()\n\n\n\n\nSES\ntype\nmean\nconf.low\nconf.high\n\n\n\n\n1\npred_out_draw_1\n4.015\n3.937\n4.091\n\n\n2\npred_out_draw_1\n3.183\n3.124\n3.251\n\n\n3\npred_out_draw_1\n2.645\n2.599\n2.688\n\n\n1\npred_out_draw_1_SES1\n4.276\n4.193\n4.351\n\n\n2\npred_out_draw_1_SES1\n3.513\n3.446\n3.582\n\n\n3\npred_out_draw_1_SES1\n2.970\n2.934\n3.000\n\n\n1\npred_out_draw_2\n4.015\n3.938\n4.094\n\n\n2\npred_out_draw_2\n3.182\n3.123\n3.253\n\n\n3\npred_out_draw_2\n2.646\n2.600\n2.687\n\n\n1\npred_out_draw_2_SES1\n4.276\n4.183\n4.358\n\n\n2\npred_out_draw_2_SES1\n3.515\n3.457\n3.584\n\n\n3\npred_out_draw_2_SES1\n2.970\n2.941\n2.998"
  }
]