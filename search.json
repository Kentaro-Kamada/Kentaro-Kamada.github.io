[
  {
    "objectID": "01_Causal_Inference/Borusyak_2024.html",
    "href": "01_Causal_Inference/Borusyak_2024.html",
    "title": "Borusyak et al. (2024)",
    "section": "",
    "text": "Borusyak, Kirill, Xavier Jaravel & Jann Spiess, 2024, “Revisiting Event-Study Designs: Robust and Efficient Estimation,” The Review of Economic Studies, (10.1093/restud/rdae007)."
  },
  {
    "objectID": "01_Causal_Inference/Borusyak_2024.html#文献",
    "href": "01_Causal_Inference/Borusyak_2024.html#文献",
    "title": "Borusyak et al. (2024)",
    "section": "",
    "text": "Borusyak, Kirill, Xavier Jaravel & Jann Spiess, 2024, “Revisiting Event-Study Designs: Robust and Efficient Estimation,” The Review of Economic Studies, (10.1093/restud/rdae007)."
  },
  {
    "objectID": "01_Causal_Inference/Borusyak_2024.html#ざっくりとした要約",
    "href": "01_Causal_Inference/Borusyak_2024.html#ざっくりとした要約",
    "title": "Borusyak et al. (2024)",
    "section": "ざっくりとした要約",
    "text": "ざっくりとした要約\nDiD推定をg-computationのようなアイデアで行う手法を提案した論文です。従来のTwo-way fixed effectによる推定では問題が生じることが指摘されてきた、処置のタイミングによる効果の異質性（staggerd DiD design）の問題などに対処しています。通常g-computationによる推定量の標準誤差はBootstrapで計算することが多いと思うのですが、この論文では推定量の漸近正規性などを示しつつ、Bootstrapを使わずに標準誤差を計算する方法を提案している点が興味深いです。"
  },
  {
    "objectID": "01_Causal_Inference/Borusyak_2024.html#estimand",
    "href": "01_Causal_Inference/Borusyak_2024.html#estimand",
    "title": "Borusyak et al. (2024)",
    "section": "Estimand",
    "text": "Estimand\n\n\\(Y_{it}(1), Y_{it}(0)\\)：個人\\(i\\)、時点\\(t\\)における介入を受けた場合と受けていない場合のpotential outcome\n\\(D_{it}\\)：介入を受けたかどうかのバイナリ変数\n\n個人レベルの因果効果\n\\[\\begin{align*}\n\\tau_{it} = Y_{it}(1) - Y_{it}(0) = Y_{it} - Y_{it}(0)\n\\end{align*}\\]\nこれを関心のある集団について平均する\n\nATT：\\(\\mathrm{E}[\\tau_{it}|D_{it} = 1] = \\mathrm{E}[Y_{it} - Y_{it}(0) | D_{it} = 1]\\)\nevent study：\\(\\mathrm{E}[\\tau_{it}|D_{it} = 1, T = t] = \\mathrm{E}[Y_{it} - Y_{it}(0) | D_{it} = 1, T = t]\\)\n特定の集団におけるATT：\\(\\mathrm{E}[\\tau_{it}|D_{it} = 1, Gender] = \\mathrm{E}[Y_{it} - Y_{it}(0) | D_{it} = 1, Gender]\\)\n\n論文ではウェイトを使った書き方をしているのでややこしいが、おそらくはこういうことだと思われる。"
  },
  {
    "objectID": "01_Causal_Inference/Borusyak_2024.html#識別",
    "href": "01_Causal_Inference/Borusyak_2024.html#識別",
    "title": "Borusyak et al. (2024)",
    "section": "識別",
    "text": "識別\n因果効果を識別するための仮定は以下の通り\n\nGeneral moodel of \\(Y(0)\\)：\\(\\mathrm{E}[Y_{it}(0)] = A'_{it}\\lambda_i + X'_{it}\\delta\\)の形で書けること\n\n\n\\(A'_{it}\\lambda_i\\)：個体の固定効果＋固定効果と共変量の交互作用\n\\(X'_{it}\\delta\\)：時点の固定効果＋時変の共変量\n要するに\\(\\mathrm{E}[Y_{it}(0)]\\)が適切のモデル化できていること\n\n\nNo-anticipation effects：\\(Y_{it} = Y_{it}(0) \\ if \\ D_{it} = 0\\)\n\n\nConsistencyと同じ？\n\n\nModel of causal effects\n\n\n効果の異質性がパラメトリックモデルで書けること（？）\n複雑すぎるとダメということ？このあたりちょっとわからなかったです"
  },
  {
    "objectID": "01_Causal_Inference/Borusyak_2024.html#推定",
    "href": "01_Causal_Inference/Borusyak_2024.html#推定",
    "title": "Borusyak et al. (2024)",
    "section": "推定",
    "text": "推定\n\n\\(\\mathrm{E}[Y_{it}(0)]\\)の推定\n\n\n処置を受けていないサンプル（\\(D_{it} = 0\\)）を用いて、以下のモデルをOLSで推定\n\n\\[\nY_{it} = A'_{it}\\lambda_i + X'_{it}\\delta + \\varepsilon_{it}\n\\]\n\n\\(\\hat{Y}_{it}(0) | D_{it} = 1\\) の推定\n\n\n推定したモデルに、処置を受けたサンプル（\\(D_{it} = 1\\)）を代入し、処置を受けたサンプルにおいて、処置を受けていなかった場合のpotential outcomeを予測\n\n\n\\(\\hat{\\tau}_{it}\\)の推定\n\n\n処置を受けたサンプルの、観察データ\\(Y_{it}\\)と予測したpotential outcome\\(\\hat{Y}_{it}(0)\\)の差を取る\n\n\n関心のある集団について平均値をとる\n\n\\[\n\\mathrm{ATT} = \\mathrm{E}[\\hat{\\tau}_{it}|D_{it} = 1]\n\\]\n標準誤差の推定も説明があるが、理解できなかった…"
  },
  {
    "objectID": "01_Causal_Inference/Borusyak_2024.html#分析例",
    "href": "01_Causal_Inference/Borusyak_2024.html#分析例",
    "title": "Borusyak et al. (2024)",
    "section": "分析例",
    "text": "分析例\n著者によるStataパッケージの他、RではKyle Buttsによるdidimputationパッケージがある。\nここではdidimputationパッケージのトイデータを用いて、パッケージによる結果と、手計算による結果を比較する。\n\n# remotes::install_github('kylebutts/didimputation')\nlibrary(tidyverse)\nlibrary(fixest)\nlibrary(didimputation)"
  },
  {
    "objectID": "01_Causal_Inference/Borusyak_2024.html#パッケージの結果",
    "href": "01_Causal_Inference/Borusyak_2024.html#パッケージの結果",
    "title": "Borusyak et al. (2024)",
    "section": "パッケージの結果",
    "text": "パッケージの結果\n\ndid_imputation(data = df_het, yname = \"dep_var\", gname = \"g\", tname = \"year\", idname = \"unit\") |&gt; \n  knitr::kable()\n\n\n\n\nterm\nestimate\nstd.error\nconf.low\nconf.high\n\n\n\n\ntreat\n2.262952\n0.0313968\n2.201414\n2.32449"
  },
  {
    "objectID": "01_Causal_Inference/Borusyak_2024.html#手計算",
    "href": "01_Causal_Inference/Borusyak_2024.html#手計算",
    "title": "Borusyak et al. (2024)",
    "section": "手計算",
    "text": "手計算\n\n# OLSで処置を受けていないサンプルにおけるモデルを推定\nmodel &lt;- feols(dep_var ~ 1 | year + unit, data = df_het |&gt; filter(!treat), combine.quick = TRUE)\n\nprediction &lt;- \n  # 処置を受けた集団の予測値（potential outcome）を計算\n  broom::augment(model, newdata = df_het |&gt; filter(treat)) |&gt; \n  # 差分を計算\n  mutate(tau = dep_var - .fitted) \n\n# 集計\nprediction |&gt; \n  summarise(\n    ATT = mean(tau)\n  ) |&gt; \n  knitr::kable()\n\n\n\n\nATT\n\n\n\n\n2.262952\n\n\n\n\n\nATTの点推定値は一致する。"
  },
  {
    "objectID": "01_Causal_Inference/Borusyak_2024.html#集団による異質性",
    "href": "01_Causal_Inference/Borusyak_2024.html#集団による異質性",
    "title": "Borusyak et al. (2024)",
    "section": "集団による異質性",
    "text": "集団による異質性\n次に、処置のタイミングにより効果が異なる場合の分析を考える。\nパッケージでは以下のように、処置のタイミングが同じグループごとにウェイトを作成してやることで、集団ごとの効果を推定できる。\n\ndf_het |&gt; \n  mutate(\n    group1 = case_when(\n      group == 'Group 1' & treat ~ 1,\n      .default = 0\n    ),\n    group2 = case_when(\n      group == 'Group 2' & treat ~ 1,\n      .default = 0\n    )\n  ) |&gt; \n  did_imputation(yname = \"dep_var\", gname = \"g\", tname = \"year\", idname = \"unit\", wtr = c('group1', 'group2')) |&gt; \n  knitr::kable()\n\n\n\n\nterm\nestimate\nstd.error\nconf.low\nconf.high\n\n\n\n\ngroup1\n2.513439\n0.0381686\n2.438629\n2.588249\n\n\ngroup2\n1.795048\n0.0389609\n1.718685\n1.871412\n\n\n\n\n\n手計算では、集計をグループごとに行えばよい。\n\n# 集計\nprediction |&gt; \n  summarise(\n    ATT = mean(tau),\n    .by = c(group)\n  ) |&gt; \n  knitr::kable()\n\n\n\n\ngroup\nATT\n\n\n\n\nGroup 1\n2.513439\n\n\nGroup 2\n1.795048\n\n\n\n\n\nこちらも点推定値は一致する。"
  },
  {
    "objectID": "01_Causal_Inference/Causal_Decomposition.html",
    "href": "01_Causal_Inference/Causal_Decomposition.html",
    "title": "Causal Decomposition Analysis",
    "section": "",
    "text": "library(tidyverse)\nlibrary(cfdecomp)\nlibrary(gapclosing)\nlibrary(causal.decomp)\n\n\nd &lt;- \n  sMIDUS |&gt; \n  transmute(Y = health |&gt; as.numeric(),  # outcome\n            T = edu |&gt; as.numeric(),   # treatment (continuous)\n            T2 = edu |&gt; case_match(4:6 ~ 0,   # treatment (binary)\n                                   7:9 ~ 1,\n                                   .default = NA) |&gt; factor(),\n            X = racesex |&gt; factor(levels = c(\"1\", \"4\", \"2\", \"3\")),  # note!\n            L1 = lowchildSES |&gt; as.numeric(),\n            L2 = abuse |&gt; as.numeric(),\n            C1 = age |&gt; as.numeric(),\n            C2 = stroke |&gt; as.numeric(),\n            C3 = T2DM |&gt; as.numeric(),\n            C4 = heart |&gt; as.numeric()) |&gt; \n  mutate(across(L1:C4, \\(.x){.x - mean(.x, na.rm = TRUE)})) |&gt; \n  tibble()"
  },
  {
    "objectID": "01_Causal_Inference/Causal_Decomposition.html#前準備",
    "href": "01_Causal_Inference/Causal_Decomposition.html#前準備",
    "title": "Causal Decomposition Analysis",
    "section": "",
    "text": "library(tidyverse)\nlibrary(cfdecomp)\nlibrary(gapclosing)\nlibrary(causal.decomp)\n\n\nd &lt;- \n  sMIDUS |&gt; \n  transmute(Y = health |&gt; as.numeric(),  # outcome\n            T = edu |&gt; as.numeric(),   # treatment (continuous)\n            T2 = edu |&gt; case_match(4:6 ~ 0,   # treatment (binary)\n                                   7:9 ~ 1,\n                                   .default = NA) |&gt; factor(),\n            X = racesex |&gt; factor(levels = c(\"1\", \"4\", \"2\", \"3\")),  # note!\n            L1 = lowchildSES |&gt; as.numeric(),\n            L2 = abuse |&gt; as.numeric(),\n            C1 = age |&gt; as.numeric(),\n            C2 = stroke |&gt; as.numeric(),\n            C3 = T2DM |&gt; as.numeric(),\n            C4 = heart |&gt; as.numeric()) |&gt; \n  mutate(across(L1:C4, \\(.x){.x - mean(.x, na.rm = TRUE)})) |&gt; \n  tibble()"
  },
  {
    "objectID": "01_Causal_Inference/Causal_Decomposition.html#cfdecomp",
    "href": "01_Causal_Inference/Causal_Decomposition.html#cfdecomp",
    "title": "Causal Decomposition Analysis",
    "section": "cfdecomp",
    "text": "cfdecomp\n\nSudharsanan and Bijlsma (2021) の方法。mediatorの値をシミュレーションで複数生成するのが特徴\n\n\n# cfd.mean\nfit_cfdecomp &lt;-\n  cfdecomp::cfd.mean(\n    formula.y = 'Y ~ X + T + X:T + L1 + L2 + C1 + C2 + C3 + C4',\n    formula.m = 'T ~ X + C1 + C2 + C3 + C4',\n    mediator = 'T',\n    group = 'X',\n    data = d |&gt; data.frame(),\n    family.y = 'gaussian',\n    family.m = 'gaussian',\n    bs.size = 50,\n    mc.size = 10,\n    alpha = 0.05\n  )\n\n\nfit_cfdecomp\n\n$out_nc_m\n          1        4        2        3\n1  7.731107 5.862364 7.096035 6.420827\n2  7.710372 5.874706 7.072613 6.418001\n3  7.729287 5.845363 7.091814 6.402172\n4  7.695194 5.835458 7.113750 6.333537\n5  7.709459 5.885473 7.089304 6.416817\n6  7.711736 5.892668 7.065101 6.396395\n7  7.718981 5.897499 7.053342 6.420924\n8  7.737605 5.867735 7.074898 6.385157\n9  7.743529 5.856016 7.053179 6.455156\n10 7.692660 5.873005 7.048532 6.402734\n11 7.700931 5.930579 7.073603 6.414454\n12 7.709857 5.868390 7.057170 6.412065\n13 7.660079 5.880086 7.090363 6.418543\n14 7.710393 5.895310 7.079329 6.504930\n15 7.713723 5.877004 7.073093 6.424146\n16 7.700547 5.882369 7.077779 6.355311\n17 7.704623 5.785021 7.080278 6.413351\n18 7.667512 5.873864 7.103714 6.395201\n19 7.697056 5.859268 7.129273 6.370715\n20 7.717567 5.856811 7.066087 6.427390\n21 7.718417 5.940837 7.056317 6.300270\n22 7.714834 5.917167 7.077442 6.428109\n23 7.687950 5.817567 7.091451 6.453549\n24 7.714199 5.896253 7.067718 6.378753\n25 7.716140 5.897198 7.072739 6.418608\n26 7.717260 5.851545 7.106499 6.322000\n27 7.728011 5.906222 7.093237 6.503905\n28 7.721451 5.880151 7.080311 6.317035\n29 7.704000 5.894500 7.082438 6.448184\n30 7.734425 5.894040 7.100767 6.348688\n31 7.655565 5.856737 7.071105 6.440338\n32 7.712734 5.880527 7.077625 6.417011\n33 7.688714 5.866698 7.039276 6.384534\n34 7.722024 5.871955 7.064584 6.461704\n35 7.712073 5.889282 7.075899 6.433744\n36 7.672385 5.868874 7.111429 6.325871\n37 7.701811 5.844496 7.083837 6.416046\n38 7.725763 5.897123 7.072589 6.451187\n39 7.702589 5.838610 7.095121 6.452029\n40 7.723586 5.885005 7.036331 6.440488\n41 7.727903 5.913520 7.039724 6.358536\n42 7.690323 5.894879 7.092623 6.484075\n43 7.741177 5.855015 7.063396 6.408379\n44 7.731829 5.897012 7.115979 6.432608\n45 7.768036 5.873023 7.057458 6.501102\n46 7.708982 5.894930 7.072425 6.425957\n47 7.670717 5.833270 7.072245 6.512370\n48 7.699773 5.870835 7.109132 6.458289\n49 7.705364 5.846849 7.098625 6.321133\n50 7.713340 5.920546 7.072171 6.461629\n\n$out_cf_m\n          1        4        2        3\n1  7.708350 7.722585 7.723794 7.716957\n2  7.712913 7.733073 7.729318 7.694003\n3  7.736111 7.764328 7.740986 7.718688\n4  7.705646 7.742209 7.734476 7.723840\n5  7.704552 7.699852 7.709041 7.675510\n6  7.713983 7.716303 7.718639 7.661669\n7  7.708561 7.726610 7.715453 7.687802\n8  7.730732 7.716861 7.732809 7.719617\n9  7.735597 7.731789 7.732477 7.743175\n10 7.693129 7.696694 7.722009 7.660469\n11 7.690469 7.701887 7.684750 7.718891\n12 7.714458 7.726172 7.723104 7.687651\n13 7.688269 7.701443 7.691177 7.630499\n14 7.705480 7.719773 7.720290 7.723078\n15 7.699542 7.722704 7.723965 7.664636\n16 7.705741 7.688666 7.705191 7.710428\n17 7.700182 7.719994 7.706265 7.712396\n18 7.674769 7.712930 7.695907 7.644272\n19 7.707815 7.703147 7.714546 7.665647\n20 7.707407 7.713980 7.734133 7.718156\n21 7.700225 7.730233 7.715030 7.682727\n22 7.716103 7.679059 7.675806 7.662687\n23 7.691072 7.687812 7.695899 7.685424\n24 7.714525 7.725910 7.743707 7.703314\n25 7.717127 7.700888 7.714776 7.708556\n26 7.717766 7.703144 7.709922 7.698403\n27 7.743411 7.724071 7.737973 7.729505\n28 7.718924 7.725136 7.724812 7.690157\n29 7.707100 7.718769 7.714549 7.726620\n30 7.728967 7.774284 7.761742 7.713455\n31 7.655832 7.693236 7.700636 7.621821\n32 7.721800 7.704404 7.711805 7.684146\n33 7.717686 7.692474 7.710944 7.682718\n34 7.711620 7.710729 7.743366 7.702984\n35 7.711407 7.714970 7.718632 7.729888\n36 7.675748 7.676697 7.689787 7.653327\n37 7.704454 7.705490 7.717777 7.718488\n38 7.726633 7.749533 7.732910 7.719652\n39 7.696247 7.665819 7.710711 7.638711\n40 7.733168 7.751703 7.742452 7.714189\n41 7.712413 7.752997 7.748047 7.714767\n42 7.699511 7.717476 7.703775 7.733894\n43 7.736720 7.730948 7.751500 7.730856\n44 7.724982 7.747891 7.743453 7.699396\n45 7.769107 7.786505 7.785645 7.768933\n46 7.710610 7.704217 7.720534 7.695336\n47 7.663390 7.685722 7.697455 7.672185\n48 7.688782 7.720037 7.701725 7.703989\n49 7.708263 7.716284 7.701585 7.706781\n50 7.717211 7.720879 7.739882 7.706129\n\n$out_nc_quantile_m\n             1        4        2        3\n2.5%  7.661751 5.821100 7.039377 6.317957\n50%   7.711905 5.875855 7.076671 6.418272\n97.5% 7.743000 5.928322 7.115478 6.504699\n\n$out_cf_quantile_m\n             1        4        2        3\n2.5%  7.665950 7.677228 7.685884 7.632347\n50%   7.709585 7.717169 7.718636 7.703652\n97.5% 7.741905 7.772044 7.759438 7.741087\n\n$out_nc_y\n          1        4        2        3\n1  7.621034 6.643970 7.322055 6.924275\n2  7.633738 6.670871 7.317397 6.966114\n3  7.613516 6.654114 7.339716 6.962072\n4  7.566001 6.714272 7.344477 6.923949\n5  7.582663 6.608517 7.342108 6.982281\n6  7.624485 6.721935 7.344109 7.015607\n7  7.615700 6.669074 7.317056 6.937811\n8  7.590878 6.688900 7.297130 6.921428\n9  7.616753 6.662662 7.347560 6.953588\n10 7.598659 6.681245 7.315698 6.910735\n11 7.641283 6.713114 7.232797 7.026384\n12 7.632483 6.713234 7.337456 6.938076\n13 7.597162 6.705088 7.369370 6.854373\n14 7.545819 6.799541 7.324269 7.202672\n15 7.604351 6.638954 7.359117 6.883424\n16 7.637843 6.681505 7.313626 6.977353\n17 7.573141 6.662691 7.302981 7.036249\n18 7.627397 6.671481 7.302699 6.827321\n19 7.632182 6.669407 7.339087 6.933783\n20 7.667363 6.623564 7.285607 7.011452\n21 7.546940 6.761133 7.320757 7.029462\n22 7.619675 6.777865 7.317513 6.913706\n23 7.597489 6.665322 7.329419 7.145886\n24 7.589674 6.693107 7.305496 6.855245\n25 7.628732 6.758465 7.335364 6.944407\n26 7.695394 6.726443 7.322023 6.900229\n27 7.617854 6.704219 7.353574 6.912435\n28 7.584768 6.686726 7.324453 6.965376\n29 7.585898 6.705098 7.328178 6.890520\n30 7.595551 6.717583 7.330339 6.921962\n31 7.556648 6.740562 7.341279 6.845864\n32 7.651729 6.675705 7.288505 6.985546\n33 7.598460 6.721346 7.300277 6.891533\n34 7.611980 6.679736 7.314816 6.954367\n35 7.569213 6.764731 7.269572 7.001851\n36 7.569618 6.663174 7.341241 6.981874\n37 7.526921 6.713062 7.341001 6.950535\n38 7.585837 6.663266 7.304704 7.068719\n39 7.601394 6.760833 7.289152 6.978806\n40 7.596236 6.706722 7.316682 7.026793\n41 7.576958 6.713345 7.280860 6.940524\n42 7.554825 6.716216 7.326328 7.071289\n43 7.588654 6.663031 7.300477 6.945659\n44 7.588610 6.642729 7.319296 6.908200\n45 7.592491 6.720409 7.354450 7.113536\n46 7.584048 6.741128 7.314114 7.078543\n47 7.579594 6.666064 7.341296 6.976665\n48 7.611844 6.690621 7.320031 6.974167\n49 7.563035 6.696348 7.331681 6.869391\n50 7.617165 6.672579 7.364220 6.931810\n\n$out_cf_y\n          1        4        2        3\n1  7.615449 7.066474 7.491511 7.384101\n2  7.634262 6.992428 7.451083 7.015862\n3  7.615087 7.156897 7.481442 7.287263\n4  7.568536 7.268018 7.497931 7.372646\n5  7.581442 7.060231 7.499538 7.052493\n6  7.624983 7.440954 7.514685 7.139768\n7  7.612750 7.261943 7.487122 7.231810\n8  7.589686 7.195857 7.439595 7.129562\n9  7.614400 7.243737 7.490440 7.276700\n10 7.598802 7.288846 7.498990 7.385749\n11 7.638632 7.301550 7.391038 7.298745\n12 7.633616 7.129459 7.484796 7.067990\n13 7.603697 7.190594 7.493655 6.832317\n14 7.544576 7.368068 7.511505 7.318609\n15 7.601031 7.131709 7.509931 7.076647\n16 7.639145 7.357993 7.478816 7.170166\n17 7.571903 7.397812 7.487491 7.107398\n18 7.628797 7.192846 7.462779 7.117366\n19 7.634705 7.111518 7.434190 7.399737\n20 7.665212 7.268880 7.436029 7.225628\n21 7.543768 7.306616 7.491977 7.237363\n22 7.619980 7.368201 7.448897 7.113867\n23 7.598233 7.351131 7.486429 7.424914\n24 7.589759 7.274429 7.483346 7.033521\n25 7.628955 7.269230 7.478446 7.145831\n26 7.695517 7.276190 7.449751 7.042623\n27 7.621503 7.297217 7.503546 7.152087\n28 7.584135 7.221680 7.471701 7.426957\n29 7.586723 7.283837 7.482231 7.208557\n30 7.594070 7.094408 7.471540 7.275556\n31 7.556706 7.495113 7.467534 7.165933\n32 7.654033 7.215387 7.401768 7.310744\n33 7.605717 7.257055 7.481343 7.428953\n34 7.609447 7.211330 7.470230 7.316104\n35 7.569079 7.475382 7.437128 7.377035\n36 7.570174 7.191352 7.458905 7.313817\n37 7.527441 7.289334 7.488176 7.142439\n38 7.586023 7.232231 7.453361 7.230344\n39 7.600187 7.270059 7.426482 7.222822\n40 7.598377 7.151726 7.466375 7.256411\n41 7.573373 7.307811 7.415343 7.270441\n42 7.557685 7.300850 7.479898 7.305634\n43 7.587329 7.072684 7.488139 7.161201\n44 7.586934 7.324096 7.484599 7.048006\n45 7.592793 7.302028 7.525389 7.534480\n46 7.584543 7.191902 7.473777 7.384518\n47 7.577544 7.254755 7.484254 7.325500\n48 7.609535 7.086800 7.457066 7.277445\n49 7.563548 7.304756 7.486797 7.128809\n50 7.618269 7.276524 7.488949 7.242437\n\n$out_nc_quantile_y\n             1        4        2        3\n2.5%  7.546071 6.627026 7.272112 6.847778\n50%   7.597325 6.691864 7.322039 6.952062\n97.5% 7.663845 6.774910 7.363072 7.138607\n\n$out_cf_quantile_y\n             1        4        2        3\n2.5%  7.543950 7.061636 7.404822 7.019835\n50%   7.598589 7.268449 7.481392 7.234587\n97.5% 7.662697 7.467635 7.513970 7.428504\n\n$mediation\n        4         2         3 \n0.6137552 0.5564163 0.4214775 \n\n$mediation_quantile\n              4         2          3\n2.5%  0.4293937 0.3284408 0.08406186\n97.5% 0.8662784 0.7838182 0.75684386\n\n$mc_conv_info_m\n          [,1]     [,2]     [,3]     [,4]\n [1,] 7.714687 5.882658 7.101293 6.434694\n [2,] 7.735273 5.860228 7.109142 6.425760\n [3,] 7.729892 5.869614 7.102033 6.427961\n [4,] 7.728124 5.867163 7.105497 6.422210\n [5,] 7.725496 5.868032 7.099355 6.435313\n [6,] 7.731321 5.872739 7.096583 6.433744\n [7,] 7.721929 5.866966 7.094652 6.436417\n [8,] 7.724117 5.860777 7.098441 6.437772\n [9,] 7.727046 5.865477 7.096471 6.429285\n[10,] 7.731107 5.862364 7.096035 6.420827\n\n$mc_conv_info_y\n          [,1]     [,2]     [,3]     [,4]\n [1,] 7.617004 6.648579 7.323474 6.929194\n [2,] 7.622056 6.643485 7.325593 6.926025\n [3,] 7.620736 6.645616 7.323674 6.926806\n [4,] 7.620302 6.645060 7.324609 6.924765\n [5,] 7.619657 6.645257 7.322951 6.929414\n [6,] 7.621086 6.646326 7.322203 6.928857\n [7,] 7.618781 6.645015 7.321681 6.929806\n [8,] 7.619318 6.643609 7.322704 6.930286\n [9,] 7.620037 6.644677 7.322172 6.927276\n[10,] 7.621034 6.643970 7.322055 6.924275\n\nmean(fit_cfdecomp$out_nc_y[,2] - fit_cfdecomp$out_nc_y[,1])\n\n[1] -0.9041596\n\nmean(fit_cfdecomp$out_cf_y[,2] - fit_cfdecomp$out_nc_y[,1])\n\n[1] -0.3525951\n\nmean(fit_cfdecomp$out_nc_y[,2] - fit_cfdecomp$out_cf_y[,2])\n\n[1] -0.5515645\n\nfit_cfdecomp$mediation\n\n        4         2         3 \n0.6137552 0.5564163 0.4214775 \n\nmean(fit_cfdecomp$out_nc_y[,3] - fit_cfdecomp$out_nc_y[,1])\n\n[1] -0.2785655\n\nmean(fit_cfdecomp$out_cf_y[,3] - fit_cfdecomp$out_nc_y[,1])\n\n[1] -0.1272748\n\nmean(fit_cfdecomp$out_nc_y[,3] - fit_cfdecomp$out_cf_y[,3])\n\n[1] -0.1512907\n\nmean(fit_cfdecomp$out_nc_y[,4] - fit_cfdecomp$out_nc_y[,1])\n\n[1] -0.6369167\n\nmean(fit_cfdecomp$out_cf_y[,4] - fit_cfdecomp$out_nc_y[,1])\n\n[1] -0.3722955\n\nmean(fit_cfdecomp$out_nc_y[,4] - fit_cfdecomp$out_cf_y[,4])\n\n[1] -0.2646212"
  },
  {
    "objectID": "01_Causal_Inference/Causal_Decomposition.html#causal.decomp",
    "href": "01_Causal_Inference/Causal_Decomposition.html#causal.decomp",
    "title": "Causal Decomposition Analysis",
    "section": "causal.decomp",
    "text": "causal.decomp\n\nPark, Kang, and Lee (2023) の方法。\n\n\n# smi \nfit.y &lt;- lm(Y ~ X + T + X:T + L1 + L2 + C1 + C2 + C3 + C4, data = d)\nfit.m &lt;- lm(T ~ X + C1 + C2 + C3 + C4, data = d)\n\nfit_smi &lt;- smi(fit.y = fit.y,\n    fit.m = fit.m,\n    treat = \"X\", \n    sims = 100, \n    conf.level = .95,\n    conditional = TRUE,\n    covariates = 1,\n    # baseline covariatesを調整できる\n    #covariates = c(\"C1\", \"C2\", \"C3\", \"C4\"),\n    seed = 227,\n    )\n\nfit_smi\n\n\nResults:\n\n                               estimate 95% CI Lower 95% CI Upper\nInitial Disparity   (1 vs 4) -0.8993401   -0.9925190  -0.79813505\nDisparity Remaining (1 vs 4) -0.3384430   -0.4880863  -0.14795873\nDisparity Reduction (1 vs 4) -0.5608971   -0.7346168  -0.42650061\nInitial Disparity   (1 vs 2) -0.2749659   -0.3378874  -0.19366549\nDisparity Remaining (1 vs 2) -0.1213246   -0.2203441  -0.05458727\nDisparity Reduction (1 vs 2) -0.1536412   -0.1896213  -0.10521328\nInitial Disparity   (1 vs 3) -0.6137425   -0.7326095  -0.47793913\nDisparity Remaining (1 vs 3) -0.3500123   -0.5151994  -0.08733348\nDisparity Reduction (1 vs 3) -0.2637302   -0.4880614  -0.11574038\n\n\n\nsensitivity analysis(Park et al. 2023)\n\n\nsensRes &lt;- sensitivity(boot.res = fit_smi, fit.m = fit.m, fit.y = fit.y, \n                       mediator = \"T\",\n                       covariates = c(\"C1\", \"C2\", \"C3\", \"C4\"), \n                       treat = \"X\",\n                       sel.lev.treat = \"4\", \n                       max.rsq = 0.3)\nplot(sensRes)"
  },
  {
    "objectID": "01_Causal_Inference/Causal_Decomposition.html#cfdecomp-1",
    "href": "01_Causal_Inference/Causal_Decomposition.html#cfdecomp-1",
    "title": "Causal Decomposition Analysis",
    "section": "cfdecomp",
    "text": "cfdecomp\n\n# cfd.mean\nset.seed(123456)\nfit_cfdecomp_b &lt;-\n  cfd.mean(\n    formula.y = 'Y ~ X + T2 + X:T2 + L1 + L2 + C1 + C2 + C3 + C4',\n    formula.m = 'T2 ~ X + C1 + C2 + C3 + C4',\n    mediator = 'T2',\n    group = 'X',\n    data = d |&gt; mutate(T2 = as.numeric(T2) - 1) |&gt; data.frame(),\n    family.y = 'gaussian',\n    family.m = 'binomial',\n    bs.size = 50,\n    mc.size = 10,\n    alpha = 0.05\n  )\nmean(fit_cfdecomp_b$out_nc_y[,\"4\"] - fit_cfdecomp_b$out_nc_y[,\"1\"])\n\n[1] -0.8981701\n\nmean(fit_cfdecomp_b$out_cf_y[,\"4\"] - fit_cfdecomp_b$out_nc_y[,\"1\"])\n\n[1] -0.5546351\n\nmean(fit_cfdecomp_b$out_nc_y[,\"4\"] - fit_cfdecomp_b$out_cf_y[,\"4\"])\n\n[1] -0.343535\n\nfit_cfdecomp_b$mediation\n\n        4         2         3 \n0.3828971 0.1298177 0.2061071 \n\nmean(fit_cfdecomp_b$out_nc_y[,\"2\"] - fit_cfdecomp_b$out_nc_y[,\"1\"])\n\n[1] -0.2774845\n\nmean(fit_cfdecomp_b$out_cf_y[,\"2\"] - fit_cfdecomp_b$out_nc_y[,\"1\"])\n\n[1] -0.2419289\n\nmean(fit_cfdecomp_b$out_nc_y[,\"2\"] - fit_cfdecomp_b$out_cf_y[,\"2\"])\n\n[1] -0.03555558\n\nmean(fit_cfdecomp_b$out_nc_y[,\"3\"] - fit_cfdecomp_b$out_nc_y[,\"1\"])\n\n[1] -0.5849521\n\nmean(fit_cfdecomp_b$out_cf_y[,\"3\"] - fit_cfdecomp_b$out_nc_y[,\"1\"])\n\n[1] -0.4655888\n\nmean(fit_cfdecomp_b$out_nc_y[,\"3\"] - fit_cfdecomp_b$out_cf_y[,\"3\"])\n\n[1] -0.1193632"
  },
  {
    "objectID": "01_Causal_Inference/Causal_Decomposition.html#causal.decomp-1",
    "href": "01_Causal_Inference/Causal_Decomposition.html#causal.decomp-1",
    "title": "Causal Decomposition Analysis",
    "section": "causal.decomp",
    "text": "causal.decomp\n\n# smi\nfit.y &lt;- lm(Y ~ X + T2 + X:T2 + L1 + L2 + C1 + C2 + C3 + C4, data = d)\nfit.m &lt;- glm(T2 ~ X + C1 + C2 + C3 + C4, data = d, family = binomial(link = \"logit\"))\n\nfit_smi_b &lt;- smi(fit.y = fit.y,\n               fit.m = fit.m,\n               treat = \"X\", \n               sims = 100, \n               conf.level = .95,\n               conditional = TRUE,\n               # covariates = 1,\n               covariates = c(\"C1\", \"C2\", \"C3\", \"C4\"),\n               seed = 123456)\nfit_smi_b\n\n\nResults:\n\n                                estimate 95% CI Lower 95% CI Upper\nInitial Disparity   (1 vs 4) -0.95667843  -1.02938841  -0.88175003\nDisparity Remaining (1 vs 4) -0.61262729  -0.77966809  -0.46844380\nDisparity Reduction (1 vs 4) -0.34405113  -0.49650368  -0.21581205\nInitial Disparity   (1 vs 2) -0.31394841  -0.38058533  -0.26000373\nDisparity Remaining (1 vs 2) -0.27995004  -0.34624791  -0.22226331\nDisparity Reduction (1 vs 2) -0.03399837  -0.05003528  -0.02022012\nInitial Disparity   (1 vs 3) -0.59968604  -0.69525994  -0.49987857\nDisparity Remaining (1 vs 3) -0.48148718  -0.61387973  -0.32105036\nDisparity Reduction (1 vs 3) -0.11819886  -0.24676548  -0.03361808\n\nsensRes &lt;- sensitivity(boot.res = fit_smi_b, \n                       fit.m = fit.m, \n                       fit.y = fit.y, \n                       mediator = \"T2\",\n                       covariates = c(\"C1\", \"C2\", \"C3\", \"C4\"), \n                       treat = \"X\",\n                       sel.lev.treat = \"4\", \n                       max.rsq = 0.3)\nplot(sensRes)"
  },
  {
    "objectID": "01_Causal_Inference/Causal_Decomposition.html#gapclosing",
    "href": "01_Causal_Inference/Causal_Decomposition.html#gapclosing",
    "title": "Causal Decomposition Analysis",
    "section": "gapclosing",
    "text": "gapclosing\n\nLundberg (2022)\n\n\n# gapclosing - regression\n# stochastic intervention\n# treatmentの割り当て確率の予測値を算出\nfit_glm &lt;- glm(T2 ~ X + C1 + C2 + C3, data = d, family = binomial(link = \"logit\"))\n\n# 全員のtreatmentが1だった時の予測値\nassing_prob &lt;- predict(fit_glm, newdata = d |&gt; mutate(X = \"1\"), type = \"response\")\n\n# 予測値をもとにrandom draw\ndraw &lt;- rbinom(n = nrow(d), size = 1, prob = assing_prob)\n\nfit_gapclosing &lt;- \n  gapclosing(\n    data = d |&gt; mutate(T2 = as.numeric(T2) - 1),\n    outcome_formula = Y ~ T2 * X + C1 + C2 + C3 + C4 + L1 + L2,\n    treatment_name = \"T2\",\n    category_name = \"X\",\n    counterfactual_assignments = draw # random draw\n  )\n\nfit_gapclosing\n\n\nFactual mean outcomes:\n# A tibble: 4 × 2\n  X     estimate\n  &lt;fct&gt;    &lt;dbl&gt;\n1 1         7.60\n2 4         6.70\n3 2         7.32\n4 3         6.98\n\nCounterfactual mean outcomes (post-intervention means):\n# A tibble: 4 × 2\n  X     estimate\n  &lt;fct&gt;    &lt;dbl&gt;\n1 1         7.60\n2 4         7.03\n3 2         7.36\n4 3         7.11\n\nFactual disparities:\n# A tibble: 12 × 2\n   X     estimate\n   &lt;chr&gt;    &lt;dbl&gt;\n 1 1 - 4    0.899\n 2 1 - 2    0.275\n 3 1 - 3    0.614\n 4 4 - 1   -0.899\n 5 4 - 2   -0.624\n 6 4 - 3   -0.286\n 7 2 - 1   -0.275\n 8 2 - 4    0.624\n 9 2 - 3    0.339\n10 3 - 1   -0.614\n11 3 - 4    0.286\n12 3 - 2   -0.339\n\nCounterfactual disparities (gap-closing estimands):\n# A tibble: 12 × 2\n   X     estimate\n   &lt;chr&gt;    &lt;dbl&gt;\n 1 1 - 4   0.566 \n 2 1 - 2   0.241 \n 3 1 - 3   0.483 \n 4 4 - 1  -0.566 \n 5 4 - 2  -0.325 \n 6 4 - 3  -0.0830\n 7 2 - 1  -0.241 \n 8 2 - 4   0.325 \n 9 2 - 3   0.242 \n10 3 - 1  -0.483 \n11 3 - 4   0.0830\n12 3 - 2  -0.242 \n\nAdditive gap closed: Counterfactual - Factual\n# A tibble: 12 × 2\n   X     estimate\n   &lt;chr&gt;    &lt;dbl&gt;\n 1 1 - 4   0.334 \n 2 1 - 2   0.0339\n 3 1 - 3   0.131 \n 4 4 - 1  -0.334 \n 5 4 - 2  -0.300 \n 6 4 - 3  -0.203 \n 7 2 - 1  -0.0339\n 8 2 - 4   0.300 \n 9 2 - 3   0.0972\n10 3 - 1  -0.131 \n11 3 - 4   0.203 \n12 3 - 2  -0.0972\n\nProportional gap closed: (Counterfactual - Factual) / Factual\n# A tibble: 12 × 2\n   X     estimate\n   &lt;chr&gt;    &lt;dbl&gt;\n 1 1 - 4    0.371\n 2 1 - 2    0.123\n 3 1 - 3    0.214\n 4 4 - 1    0.371\n 5 4 - 2    0.480\n 6 4 - 3    0.710\n 7 2 - 1    0.123\n 8 2 - 4    0.480\n 9 2 - 3    0.287\n10 3 - 1    0.214\n11 3 - 4    0.710\n12 3 - 2    0.287\n\ndisparityplot(fit_gapclosing, category_A = \"1\", category_B = \"4\")\n\n\n\n\n\n\n\ndisparityplot(fit_gapclosing, category_A = \"1\", category_B = \"2\")\n\n\n\n\n\n\n\ndisparityplot(fit_gapclosing, category_A = \"1\", category_B = \"3\")\n\n\n\n\n\n\n\n\n\n機械学習をつかったdoubly robustな方法も使える\n\n\n# gapclosing - ranger, doubly robust\nfit_gapclosing_ranger &lt;- \n  gapclosing(\n  data = d |&gt; mutate(T2 = as.numeric(T2) - 1),\n  outcome_formula = Y ~ T2 + X + C1 + C2 + C3 + C4 + L1 + L2,\n  treatment_formula = T2 ~ X + C1 + C2 + C3 + C4 + L1 + L2, \n  treatment_name = \"T2\",\n  treatment_algorithm = \"ranger\",\n  outcome_algorithm = \"ranger\",\n  category_name = \"X\",\n  counterfactual_assignments = rbinom(n = nrow(d), size = 1, prob = assing_prob) \n)\n\nfit_gapclosing\n\n\nFactual mean outcomes:\n# A tibble: 4 × 2\n  X     estimate\n  &lt;fct&gt;    &lt;dbl&gt;\n1 1         7.60\n2 4         6.70\n3 2         7.32\n4 3         6.98\n\nCounterfactual mean outcomes (post-intervention means):\n# A tibble: 4 × 2\n  X     estimate\n  &lt;fct&gt;    &lt;dbl&gt;\n1 1         7.60\n2 4         7.03\n3 2         7.36\n4 3         7.11\n\nFactual disparities:\n# A tibble: 12 × 2\n   X     estimate\n   &lt;chr&gt;    &lt;dbl&gt;\n 1 1 - 4    0.899\n 2 1 - 2    0.275\n 3 1 - 3    0.614\n 4 4 - 1   -0.899\n 5 4 - 2   -0.624\n 6 4 - 3   -0.286\n 7 2 - 1   -0.275\n 8 2 - 4    0.624\n 9 2 - 3    0.339\n10 3 - 1   -0.614\n11 3 - 4    0.286\n12 3 - 2   -0.339\n\nCounterfactual disparities (gap-closing estimands):\n# A tibble: 12 × 2\n   X     estimate\n   &lt;chr&gt;    &lt;dbl&gt;\n 1 1 - 4   0.566 \n 2 1 - 2   0.241 \n 3 1 - 3   0.483 \n 4 4 - 1  -0.566 \n 5 4 - 2  -0.325 \n 6 4 - 3  -0.0830\n 7 2 - 1  -0.241 \n 8 2 - 4   0.325 \n 9 2 - 3   0.242 \n10 3 - 1  -0.483 \n11 3 - 4   0.0830\n12 3 - 2  -0.242 \n\nAdditive gap closed: Counterfactual - Factual\n# A tibble: 12 × 2\n   X     estimate\n   &lt;chr&gt;    &lt;dbl&gt;\n 1 1 - 4   0.334 \n 2 1 - 2   0.0339\n 3 1 - 3   0.131 \n 4 4 - 1  -0.334 \n 5 4 - 2  -0.300 \n 6 4 - 3  -0.203 \n 7 2 - 1  -0.0339\n 8 2 - 4   0.300 \n 9 2 - 3   0.0972\n10 3 - 1  -0.131 \n11 3 - 4   0.203 \n12 3 - 2  -0.0972\n\nProportional gap closed: (Counterfactual - Factual) / Factual\n# A tibble: 12 × 2\n   X     estimate\n   &lt;chr&gt;    &lt;dbl&gt;\n 1 1 - 4    0.371\n 2 1 - 2    0.123\n 3 1 - 3    0.214\n 4 4 - 1    0.371\n 5 4 - 2    0.480\n 6 4 - 3    0.710\n 7 2 - 1    0.123\n 8 2 - 4    0.480\n 9 2 - 3    0.287\n10 3 - 1    0.214\n11 3 - 4    0.710\n12 3 - 2    0.287\n\ndisparityplot(fit_gapclosing_ranger, category_A = \"1\", category_B = \"4\")\n\n\n\n\n\n\n\ndisparityplot(fit_gapclosing_ranger, category_A = \"1\", category_B = \"2\")\n\n\n\n\n\n\n\ndisparityplot(fit_gapclosing_ranger, category_A = \"1\", category_B = \"3\")"
  },
  {
    "objectID": "01_R_Tips/how_to_change_BLAS.html",
    "href": "01_R_Tips/how_to_change_BLAS.html",
    "title": "BLASの変更によるRの高速化",
    "section": "",
    "text": "Do GPU-based Basic Linear Algebra Subprograms (BLAS) improve the performance of standard modeling techniques in R?\nEven faster matrix math in R on macOS with M1\nNVBLASを使って「R」の並列演算処理を高速化"
  },
  {
    "objectID": "01_R_Tips/how_to_change_BLAS.html#参考",
    "href": "01_R_Tips/how_to_change_BLAS.html#参考",
    "title": "BLASの変更によるRの高速化",
    "section": "",
    "text": "Do GPU-based Basic Linear Algebra Subprograms (BLAS) improve the performance of standard modeling techniques in R?\nEven faster matrix math in R on macOS with M1\nNVBLASを使って「R」の並列演算処理を高速化"
  },
  {
    "objectID": "01_R_Tips/how_to_change_BLAS.html#概要",
    "href": "01_R_Tips/how_to_change_BLAS.html#概要",
    "title": "BLASの変更によるRの高速化",
    "section": "概要",
    "text": "概要\nR言語の計算速度はそれほど速くはないが、いくつか工夫することで高速化が可能。\n\nベクトル演算の駆使\n並列化（paralell, future, furrrなど）\nBLAS (Basic Linear Algebra Subprograms) の変更\n\n今回はBLASの変更による高速化について。"
  },
  {
    "objectID": "01_R_Tips/how_to_change_BLAS.html#blasとは",
    "href": "01_R_Tips/how_to_change_BLAS.html#blasとは",
    "title": "BLASの変更によるRの高速化",
    "section": "BLASとは",
    "text": "BLASとは\nBLASは、行列演算を高速に行うためのライブラリ。たとえば分散や共分散を計算するときは、行列の掛け算をする必要がある。機械学習やニューラルネットワークでは行列の掛け算が多用されるため、BLASの高速化は全体としての計算速度の向上に大きく貢献する。\nBLASはオープンソースのものもあるが、各CPU、GPUメーカーがそれぞれのチップに適したライブラリを提供している。\n\nRについてるBLAS\n\nRにデフォルトでついてくるBLAS\n\nOpenBLAS\n\nオープンソースのBLASで、homebrewでインストール可能\npythonのnumpyで使われているらしい\n\nATLAS\n\nこちらもオープンソース\n\nMKL\n\nIntelが提供するBLAS\nMacだと使えない\n\nNVBLAS\n\nNVIDIAが提供するBLAS\nGPUを使用するのでめちゃくちゃ速いらしい\nNVIDIAのGPUじゃないと使えない…\n\nAccelerate\n\nAppleが提供\n今回使用するやつ"
  },
  {
    "objectID": "01_R_Tips/how_to_change_BLAS.html#blasの変更",
    "href": "01_R_Tips/how_to_change_BLAS.html#blasの変更",
    "title": "BLASの変更によるRの高速化",
    "section": "BLASの変更",
    "text": "BLASの変更\n以下、homebrewでRをインストールしていることを前提とする。\nterminalで以下のコマンドを実行する。\n\ncd /Library/Frameworks/R.framework/Resources/lib\nln -sf libRblas.veclib.dylib libRblas.dylib\n\n意味としては、RはlibRblas.dylibをBLASとして認識するので、libRblas.veclib.dylibをlibRblas.dylibにリンクすることで、libRblas.veclib.dylibをBLASとして使うようにする。\nもどすときはlibRblas.veclib.dylibの代わりに、libRblas.0.dylib（デフォルトのBLAS）をリンクすればよい。\n\ncd /Library/Frameworks/R.framework/Resources/lib\nln -sf libRblas.0.dylib libRblas.dylib"
  },
  {
    "objectID": "01_R_Tips/r_installation_with_rig.html",
    "href": "01_R_Tips/r_installation_with_rig.html",
    "title": "バージョンを指定したRのインストール",
    "section": "",
    "text": "わたし：Macユーザーで、MacのソフトウェアはすべてHomebrewで管理\nある日：Rのバージョンが4.4.0になっていたのに気づかずにアップデートしてしまった\nやりたいこと：Rのバージョンを4.3.3に戻したい"
  },
  {
    "objectID": "01_R_Tips/r_installation_with_rig.html#背景",
    "href": "01_R_Tips/r_installation_with_rig.html#背景",
    "title": "バージョンを指定したRのインストール",
    "section": "",
    "text": "わたし：Macユーザーで、MacのソフトウェアはすべてHomebrewで管理\nある日：Rのバージョンが4.4.0になっていたのに気づかずにアップデートしてしまった\nやりたいこと：Rのバージョンを4.3.3に戻したい"
  },
  {
    "objectID": "01_R_Tips/r_installation_with_rig.html#こんなのを見つけた",
    "href": "01_R_Tips/r_installation_with_rig.html#こんなのを見つけた",
    "title": "バージョンを指定したRのインストール",
    "section": "こんなのを見つけた",
    "text": "こんなのを見つけた\nイマドキRのインストール事情？ ～rig、pak、p3m～\nどうやらrigというものを使えば、Rのバージョンを指定してインストールできるらしい。"
  },
  {
    "objectID": "01_R_Tips/r_installation_with_rig.html#使ってみる",
    "href": "01_R_Tips/r_installation_with_rig.html#使ってみる",
    "title": "バージョンを指定したRのインストール",
    "section": "使ってみる",
    "text": "使ってみる\ngithubを見ながらインストール。\nまずはrigをhomebrewからインストール\n\nbrew tap r-lib/rig\nbrew install --cask rig \n\n次に、rigを使ってインストールしたいRのバージョンを指定してインストール\n\nrig add 4.3.3\n\nアンインストールしたい時はこう\n\nrig rm 4.3.3\n\nかんたん！"
  },
  {
    "objectID": "01_Others/Multilevel.html",
    "href": "01_Others/Multilevel.html",
    "title": "マルチレベルモデルは点推定値に影響を及ぼすか？",
    "section": "",
    "text": "library(tidyverse)\nlibrary(lmerTest)\nlibrary(broom)\nlibrary(broom.mixed)"
  },
  {
    "objectID": "01_Others/Multilevel.html#マルチレベルモデルにおけるよくある誤解",
    "href": "01_Others/Multilevel.html#マルチレベルモデルにおけるよくある誤解",
    "title": "マルチレベルモデルは点推定値に影響を及ぼすか？",
    "section": "マルチレベルモデルにおけるよくある誤解",
    "text": "マルチレベルモデルにおけるよくある誤解\n「マルチレベルモデルでやんなきゃ係数にバイアスが…」←ホント？"
  },
  {
    "objectID": "01_Others/Multilevel.html#データ生成",
    "href": "01_Others/Multilevel.html#データ生成",
    "title": "マルチレベルモデルは点推定値に影響を及ぼすか？",
    "section": "データ生成",
    "text": "データ生成\nマルチレベルのデータを考える サンプルサイズ1000、グループ数50のデータ\n\\[\\begin{align}\ny_{ig} \\sim 0.5x_{ig} + \\mathrm{Normal}(\\theta_g, 1) \\\\\n\\theta_g \\sim \\mathrm{Normal}(0, 3) \\\\\nx_i \\sim \\mathrm{Normal}(0, 1)\n\\end{align}\\]\n\ndgp &lt;- function(samplesize = 1000) {\n  tibble(\n    id = 1:samplesize,\n    group = rep(1:50, 20),\n    x = rnorm(samplesize, mean = 0, sd = 1),\n  ) |&gt; \n    group_by(group) |&gt; \n    mutate(group_mean = rnorm(1, mean = 0, sd = 3)) |&gt; \n    ungroup() |&gt; \n    mutate(y = 0.5*x + rnorm(samplesize, mean = group_mean, sd = 1))\n}\n\ndata &lt;- dgp()\n\n\nlm(y ~ x, data = data) |&gt; \n  summary()\n\n\nCall:\nlm(formula = y ~ x, data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-9.3427 -2.6225  0.0757  2.6275  7.6910 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   0.1105     0.1040   1.063 0.288194    \nx             0.3755     0.1033   3.635 0.000292 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.29 on 998 degrees of freedom\nMultiple R-squared:  0.01307,   Adjusted R-squared:  0.01208 \nF-statistic: 13.21 on 1 and 998 DF,  p-value: 0.0002921\n\nlmer(y ~ x + (1|group), data = data) |&gt; \n  summary()\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: y ~ x + (1 | group)\n   Data: data\n\nREML criterion at convergence: 3124.4\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.0764 -0.6308 -0.0189  0.6780  3.5932 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n group    (Intercept) 9.991    3.161   \n Residual             1.020    1.010   \nNumber of obs: 1000, groups:  group, 50\n\nFixed effects:\n             Estimate Std. Error        df t value Pr(&gt;|t|)    \n(Intercept)   0.10994    0.44814  48.99953   0.245    0.807    \nx             0.46983    0.03281 949.67653  14.321   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n  (Intr)\nx 0.000"
  },
  {
    "objectID": "01_Others/Multilevel.html#シミュレーション",
    "href": "01_Others/Multilevel.html#シミュレーション",
    "title": "マルチレベルモデルは点推定値に影響を及ぼすか？",
    "section": "シミュレーション",
    "text": "シミュレーション\nデータを1000個作成\n\ndata_list &lt;- \n  map(1:1000, \\(x) dgp(1000)) |&gt; \n  enframe()\n\nOLSと変量効果モデルで推定\n\nresult &lt;- \n  data_list |&gt; \n  mutate(\n    lm = map(value, \\(data) lm(y ~ x, data = data)),\n    lmer = map(value, \\(data) lmer(y ~ x + (1|group), data = data))\n  )\n\nxの係数のみ取り出す\n\nres2 &lt;- \n  result |&gt; \n  mutate(\n    lm_res = map(lm, \\(model) {\n      tidy(model) |&gt; \n        select(term, estimate)\n    }),\n    lmer_res = map(lmer, \\(model) {\n      tidy(model) |&gt; \n        select(term, estimate)\n    })\n  ) |&gt; \n  select(name, lm_res, lmer_res) |&gt; \n  pivot_longer(!name, names_to = 'model', values_to = 'value') |&gt; \n  unnest(value) |&gt; \n  filter(term == 'x')\n\n結果を図示\n\nどちらの点推定値も真の値の0.5を中心に分布＝バイアスはない\nOLSによる点推定値はバリアンスが大きい\n変量効果（マルチレベル）モデルによる点推定値はバリアンスが小さい\n\n\nres2 |&gt; \n  ggplot(aes(estimate, fill = model))+\n  geom_vline(xintercept = 0.5, linetype = 'dashed', alpha = 0.5)+\n  geom_histogram(alpha = 0.3, color = 'black', binwidth = 0.02, position = 'identity')+\n  scale_x_continuous(breaks = seq(0, 1, 0.2))"
  },
  {
    "objectID": "01_Others/複利計算.html",
    "href": "01_Others/複利計算.html",
    "title": "複利計算",
    "section": "",
    "text": "library(tidyverse)"
  },
  {
    "objectID": "01_Others/複利計算.html#複利計算",
    "href": "01_Others/複利計算.html#複利計算",
    "title": "複利計算",
    "section": "複利計算",
    "text": "複利計算\n\n年間\\(a\\)万円ずつ積み立てることを考える\n年利は\\(r\\)とする\n\n\\[\\begin{align*}\n& 1年目:ar^1 \\\\\n& 2年目:ar^1 + ar^2 \\\\\n& 3年目:ar^1 + ar^2 + ar^3 \\\\\n& n年目:\\sum_{i=1}^{n} ar^i = \\frac{ar(1 - r^n)}{1 - r}\n\\end{align*}\\]\n\n等比級数の和の導出\n\n\\[\\begin{align*}\nS =& ar + ar^2 + ar^3 + ... + ar^n \\\\\nrS =& \\qquad\\; ar^2 + ar^3 + ... + ar^n + ar^{n+1} \\\\\nS - rS =& ar - ar^{n+1} \\\\\nS(1 - r) =& ar(1 - r^n) \\\\\nS =& \\frac{ar(1 - r^n)}{1 - r}\n\\end{align*}\\]\n\nシミュレーション\n\n\\(n\\)：30年積み立てる\n\\(a\\)：年間40万円ずつ（月あたり3.3333万円）\n\\(r\\)：年利は5％と仮定\n\n\n\nn &lt;- 30\na &lt;- 40\nr &lt;- 1.05\n\ntibble(\n  year = 1:n,\n  cost = a*year,\n  gain = a*r^year,\n  return = a*r*(1 - r^year)/(1 - r),\n) |&gt; \n  ggplot()+\n  geom_line(aes(x = year, y = return), color = 'red')+\n  geom_col(aes(x = year, y = cost), alpha = .7)+\n  geom_line(aes(x = year, y = cost), color = 'blue')"
  },
  {
    "objectID": "01_Others/複利計算.html#利率が年度ごとに変動する場合",
    "href": "01_Others/複利計算.html#利率が年度ごとに変動する場合",
    "title": "複利計算",
    "section": "利率が年度ごとに変動する場合",
    "text": "利率が年度ごとに変動する場合\n\n年利は確率的に変動する値\n\n定数（\\(r\\)）ではなく、変数（\\(r_i\\)）として考える\n\n\n\\[\\begin{align*}\n& 1年目:ar_1 \\\\\n& 2年目:ar_1 + ar_1r_2\\\\\n& 3年目:ar_1 + ar_1r_2 + ar_1r_2r_3 = a(r_1 + r_1r_2 + r_1r_2r_3)\\\\\n& n年目:ar_1 + ar_1r_2 + ar_1r_2r_3 + ... + ar_1r_2r_3...r_n = a\\sum_{i=1}^{n} \\prod_{i=1}^{n}r_i\n\\end{align*}\\]\nここで、\\(r_i \\sim \\mathrm{Normal}(1.05, 0.1)\\)とする\n\ntibble(\n  year = 1:n,\n  rate = rnorm(n, r, .1),\n  cost = a*year,\n  return = a*(accumulate(rate, \\(x, y) x*y) |&gt; accumulate(\\(x, y) x + y)),\n) |&gt; \n  ggplot()+\n  geom_line(aes(x = year, y = return), color = 'red')+\n  geom_col(aes(x = year, y = cost), alpha = .7)+\n  geom_line(aes(x = year, y = cost), color = 'blue')"
  },
  {
    "objectID": "01_Others/複利計算.html#利率がランダムウォークする場合",
    "href": "01_Others/複利計算.html#利率がランダムウォークする場合",
    "title": "複利計算",
    "section": "利率がランダムウォークする場合",
    "text": "利率がランダムウォークする場合\n\n景気は各時点でランダムに発生するわけではない\nむしろ1時点前の状況を強く反映する\n年利がランダムウォークすると仮定する\n\n\\[\\begin{align*}\n& r_1 \\sim \\mathrm{Normal}(1.05, 0.1) \\\\\n& r_{i + 1} = r_i + e_i, \\; e_i \\sim \\mathrm{Normal}(0, 0.01)\n\\end{align*}\\]\nパラメータの設定が難しい…\n\ntibble(\n  year = 1:n,\n  error = rnorm(n, 0, 0.01),\n  rate = accumulate(error, \\(x, y) x + y, .init = rnorm(1, r, .1))[-n],\n  cost = a*year,\n  return = a*(accumulate(rate, \\(x, y) x*y) |&gt; accumulate(\\(x, y) x + y)),\n) |&gt; \n  ggplot()+\n  geom_line(aes(x = year, y = return), color = 'red')+\n  geom_col(aes(x = year, y = cost), alpha = .7)+\n  geom_line(aes(x = year, y = cost), color = 'blue')"
  },
  {
    "objectID": "01_Others/rlang学習.html",
    "href": "01_Others/rlang学習.html",
    "title": "rlang学習",
    "section": "",
    "text": "Advanced R"
  },
  {
    "objectID": "01_Others/rlang学習.html#参考",
    "href": "01_Others/rlang学習.html#参考",
    "title": "rlang学習",
    "section": "",
    "text": "Advanced R"
  },
  {
    "objectID": "01_Others/rlang学習.html#下準備",
    "href": "01_Others/rlang学習.html#下準備",
    "title": "rlang学習",
    "section": "下準備",
    "text": "下準備\n\nlibrary(rlang)\nlibrary(lobstr)"
  },
  {
    "objectID": "01_Others/rlang学習.html#expression",
    "href": "01_Others/rlang学習.html#expression",
    "title": "rlang学習",
    "section": "expression",
    "text": "expression\n\nexpr：コードをそのまま捕まえる\n\n作成されたオブジェクト：expression\ncall, symbol, constant, pairlistの4つからなる\n\n\n\nexpr(mean(x, na.rm = TRUE))\n\nmean(x, na.rm = TRUE)\n\nexpr(10 + 100 + 1000)\n\n10 + 100 + 1000\n\n\n\nexprは関数の引数なども書いてあるまま保持する\n\n\n# これはxになる\ncapture_it &lt;- function(x) {\n  expr(x)\n}\n\ncapture_it(a + b + c)\n\nx\n\n\n\n引数は評価した上でコードを保持したい場合はenexprを使う\n\n\ncapture_it &lt;- function(x) {\n  enexpr(x)\n}\ncapture_it(a + b + c)\n\na + b + c"
  },
  {
    "objectID": "01_Others/rlang学習.html#abstract-syntax-tree-ast-による表現",
    "href": "01_Others/rlang学習.html#abstract-syntax-tree-ast-による表現",
    "title": "rlang学習",
    "section": "abstract syntax tree (AST) による表現",
    "text": "abstract syntax tree (AST) による表現\n\ncodeはtree形式で表現できる\nfがcall, aがsymbol, “b”がconstantらしい\n\n\nlobstr::ast(f(a, \"b\"))\n\n█─f \n├─a \n└─\"b\" \n\n\n\n+や*も関数であることがわかる\n\n\nlobstr::ast(1 + 2 * 3)\n\n█─`+` \n├─1 \n└─█─`*` \n  ├─2 \n  └─3"
  },
  {
    "objectID": "01_Others/rlang学習.html#codeを生成するcode",
    "href": "01_Others/rlang学習.html#codeを生成するcode",
    "title": "rlang学習",
    "section": "codeを生成するcode",
    "text": "codeを生成するcode\n\ncall2\n\n\ncall2('f', 1, 2)\n\nf(1, 2)\n\ncall2('+', 1, call2('*', 2, 3))\n\n1 + 2 * 3\n\n\n\n!!を使ったcodeの生成\n\n捕まえておいたコードを評価したいところで評価する\n\n\n\nxx &lt;- expr(x + x)\nyy &lt;- expr(y + y)\n\nexpr(!!xx / !!yy)\n\n(x + x)/(y + y)"
  },
  {
    "objectID": "01_Others/rlang学習.html#codeの評価",
    "href": "01_Others/rlang学習.html#codeの評価",
    "title": "rlang学習",
    "section": "codeの評価",
    "text": "codeの評価\n\neval(expr, env)：exprをenvで評価\n\n\neval(expr(x + y), env(x = 1, y = 10))\n\n[1] 11\n\neval(expr(x + y), env(x = 2, y = 100))\n\n[1] 102\n\n\n\nenvを指定しないときは現在のenvironmentで評価\n\n\nx &lt;- 10\ny &lt;- 100\n\neval(expr(x + y))\n\n[1] 110\n\n\n\nデータを環境として扱う\neval_tidy(expr, data)：exprをdata内部で評価\n\ndataでcurrent environmentを上書きするのでdata-mask呼ばれる\n\nevalでもできないことはないが、落とし穴があるらしい\n\n\ndf &lt;- data.frame(x = 1:5, y = sample(5))\neval_tidy(expr(x + y), df)\n\n[1] 5 4 8 7 6\n\n\n\ndplyrっぽい関数\n\n\nwith2 &lt;- function(df, expr) {\n  eval_tidy(enexpr(expr), df)\n}\n\nwith2(df, x + y)\n\n[1] 5 4 8 7 6"
  },
  {
    "objectID": "01_Others/rlang学習.html#quosure",
    "href": "01_Others/rlang学習.html#quosure",
    "title": "rlang学習",
    "section": "Quosure",
    "text": "Quosure\n\nexprだけを使う方法は問題がでてくる\n関数の内部でaを定義してみる\n\n\nwith2 &lt;- function(df, expr) {\n  a &lt;- 1000\n  eval_tidy(enexpr(expr), df)\n}\n\n\nさらに関数の外部（global environment）でもaを定義する\n\n\ndf &lt;- data.frame(x = 1:3)\na &lt;- 10\n\n\nこの状態でwith2を使うと問題が発生\n\nglobal environmentのaではなく、関数内部で定義したaが優先的に使われる\n\n\n\nwith2(df, x + a)\n\n[1] 1001 1002 1003\n\n\n\nquosure：expressionとenvironmentをセットで保持する\n\ndata-maskでは、eval_tidyのdata -&gt; quosureのenv -&gt; global envの順で評価される\n\n\n\nwith2 &lt;- function(df, expr) {\n  a &lt;- 1000\n  eval_tidy(enquo(expr), df)\n}\n\nx &lt;- 10\n\nwith2(df, x + a)\n\n[1] 11 12 13"
  },
  {
    "objectID": "01_Others/度数から対数尤度を計算.html",
    "href": "01_Others/度数から対数尤度を計算.html",
    "title": "観測度数・期待度数から対数尤度を計算する方法",
    "section": "",
    "text": "\\(f_{ij}\\)：観測度数\n\n\n\n\n\nB\nB’\n\n\n\n\nA\n\\(f_{11}\\)\n\\(f_{12}\\)\n\n\nA’\n\\(f_{21}\\)\n\\(f_{22}\\)\n\n\n\n\n\\(p_{ij}\\)：セル確率\n\nモデルから計算されたセルの確率\nAとBに関連がないモデルにおいては、期待度数をサンプルサイズで割ったもの\n\n\n\n\n\n\nB\nB’\n\n\n\n\nA\n\\(p_{11}\\)\n\\(p_{12}\\)\n\n\nA’\n\\(p_{21}\\)\n\\(p_{22}\\)\n\n\n\n尤度関数を\\(L\\)とすると\n\\[\nL = \\prod_i \\prod_j p_{ij}^{f_{ij}} =  p_{11}^{f_{11}} \\times p_{12}^{f_{12}} \\times p_{21}^{f_{21}} \\times p_{22}^{f_{22}}\n\\]\n各セルの確率を(観測度数)乗したものを全部掛け算\n対数尤度はこれの対数を取ったもの\n\\[\n\\log L = \\sum_i \\sum_j f_{ij} \\log p_{ij} = f_{11} \\log p_{11} + f_{12} \\log p_{12} + f_{21} \\log p_{21} + f_{22} \\log p_{22}\n\\]\n掛け算が足し算になるので計算しやすい"
  },
  {
    "objectID": "01_Others/度数から対数尤度を計算.html#観測度数とセル確率から対数尤度を計算",
    "href": "01_Others/度数から対数尤度を計算.html#観測度数とセル確率から対数尤度を計算",
    "title": "観測度数・期待度数から対数尤度を計算する方法",
    "section": "",
    "text": "\\(f_{ij}\\)：観測度数\n\n\n\n\n\nB\nB’\n\n\n\n\nA\n\\(f_{11}\\)\n\\(f_{12}\\)\n\n\nA’\n\\(f_{21}\\)\n\\(f_{22}\\)\n\n\n\n\n\\(p_{ij}\\)：セル確率\n\nモデルから計算されたセルの確率\nAとBに関連がないモデルにおいては、期待度数をサンプルサイズで割ったもの\n\n\n\n\n\n\nB\nB’\n\n\n\n\nA\n\\(p_{11}\\)\n\\(p_{12}\\)\n\n\nA’\n\\(p_{21}\\)\n\\(p_{22}\\)\n\n\n\n尤度関数を\\(L\\)とすると\n\\[\nL = \\prod_i \\prod_j p_{ij}^{f_{ij}} =  p_{11}^{f_{11}} \\times p_{12}^{f_{12}} \\times p_{21}^{f_{21}} \\times p_{22}^{f_{22}}\n\\]\n各セルの確率を(観測度数)乗したものを全部掛け算\n対数尤度はこれの対数を取ったもの\n\\[\n\\log L = \\sum_i \\sum_j f_{ij} \\log p_{ij} = f_{11} \\log p_{11} + f_{12} \\log p_{12} + f_{21} \\log p_{21} + f_{22} \\log p_{22}\n\\]\n掛け算が足し算になるので計算しやすい"
  },
  {
    "objectID": "01_Others/度数から対数尤度を計算.html#尤度比カイ2乗値との関係",
    "href": "01_Others/度数から対数尤度を計算.html#尤度比カイ2乗値との関係",
    "title": "観測度数・期待度数から対数尤度を計算する方法",
    "section": "尤度比カイ2乗値との関係",
    "text": "尤度比カイ2乗値との関係\n尤度比カイ2乗値（\\(G^2\\)）と観測度数（\\(f\\)）・期待度数（\\(F\\)）の関係は以下の式で表される(Hout 1983; McCutcheon 1987)\n\\[\nG^2 = 2 \\sum_i \\sum_j f_{ij} \\log \\frac{f_{ij}}{F_{ij}}\n\\]\nなぜこうなるのか？対数尤度の比から考えてみる。\nまず、AとBに関連がないというモデル（独立モデル）のもとで対数尤度を計算する。この条件ではセル確率\\(p_{ij}\\)は期待度数\\(F_{ij}\\)をサンプルサイズ\\(N\\)で割ったものになる。\n\\[\n\\log L_1 = \\sum_i \\sum_j f_{ij} \\log p_{ij} = \\sum_i \\sum_j f_{ij} \\log \\frac{F_{ij}}{N} = \\sum_i \\sum_j f_{ij} \\log F_{ij} - \\sum_i \\sum_j f_{ij} \\log N\n\\]\n次に、AとBに関連があるというモデル（飽和モデル）のもとで対数尤度を計算する。この条件ではセル確率\\(p_{ij}\\)は観測度数\\(f_{ij}\\)をサンプルサイズ\\(N\\)で割ったものになる。\n\\[\n\\log L_2 = \\sum_i \\sum_j f_{ij} \\log p_{ij} = \\sum_i \\sum_j f_{ij} \\log \\frac{f_{ij}}{N} = \\sum_i \\sum_j f_{ij} \\log f_{ij} - \\sum_i \\sum_j f_{ij} \\log N\n\\]\n対数尤度の差（＝尤度の比）を計算すると\n\\[\\begin{align}\n\\log L_2 - \\log L_1 &= \\sum_i \\sum_j f_{ij} \\log f_{ij} - \\sum_i \\sum_j f_{ij} \\log F_{ij} \\\\\n\\log \\frac{L_2}{L_1} &= \\sum_i \\sum_j (f_{ij} \\log f_{ij} - f_{ij} \\log F_{ij}) \\\\\n\\log \\frac{L_2}{L_1} &= \\sum_i \\sum_j f_{ij} \\log \\frac{f_{ij}}{F_{ij}} \\\\\n\\end{align}\\]\nそれっぽい形になった。\nさらに両辺を2倍すると\n\\[\n2 \\log \\frac{L_2}{L_1} = \\log (\\frac{L_2}{L_1})^2 = 2 \\sum_i \\sum_j f_{ij} \\log \\frac{f_{ij}}{F_{ij}}\n\\]\nしたがって尤度比カイ二乗値は、尤度比の2乗の対数をとったものである。\nあるいは、以下のように変形すればdeviance（-2対数尤度）の差としても表現できる。\n\\[\n2 \\log \\frac{L_2}{L_1} = (-2 \\log L_1) - (- 2 \\log L_2)\n\\]"
  },
  {
    "objectID": "01_Others/multi_answer.html",
    "href": "01_Others/multi_answer.html",
    "title": "マルチアンサーの処理",
    "section": "",
    "text": "library(tidyverse)"
  },
  {
    "objectID": "01_Others/multi_answer.html#パッケージ",
    "href": "01_Others/multi_answer.html#パッケージ",
    "title": "マルチアンサーの処理",
    "section": "",
    "text": "library(tidyverse)"
  },
  {
    "objectID": "01_Others/multi_answer.html#googleフォームなどのマルチアンサーの処理",
    "href": "01_Others/multi_answer.html#googleフォームなどのマルチアンサーの処理",
    "title": "マルチアンサーの処理",
    "section": "Googleフォームなどのマルチアンサーの処理",
    "text": "Googleフォームなどのマルチアンサーの処理\ngoogleフォームでマルチアンサー形式の設問を作ると、以下のようにカンマで区切られた形式でデータが入力される。\n\ndata &lt;- \n  tibble(\n    q1 = c('A,B,C', 'A,B', 'A,C', 'B,C', 'A', 'B', 'C'),\n  ) |&gt; \n  mutate(id = row_number()) |&gt; \n  relocate(id, .before = q1) \n\ndata\n\n# A tibble: 7 × 2\n     id q1   \n  &lt;int&gt; &lt;chr&gt;\n1     1 A,B,C\n2     2 A,B  \n3     3 A,C  \n4     4 B,C  \n5     5 A    \n6     6 B    \n7     7 C    \n\n\nまずはカンマで区切られているやつを分割する。\n\ndata |&gt; \n  separate_longer_delim(cols = q1, delim = ',') \n\n# A tibble: 12 × 2\n      id q1   \n   &lt;int&gt; &lt;chr&gt;\n 1     1 A    \n 2     1 B    \n 3     1 C    \n 4     2 A    \n 5     2 B    \n 6     3 A    \n 7     3 C    \n 8     4 B    \n 9     4 C    \n10     5 A    \n11     6 B    \n12     7 C    \n\n\nこのデータの意味自体は、id:1はA,B,Cを選択した、というものである。\nなのでフラグとして1を立てておく。\n\ndata |&gt; \n  separate_longer_delim(cols = q1, delim = ',') |&gt;\n  mutate(flag = 1)\n\n# A tibble: 12 × 3\n      id q1     flag\n   &lt;int&gt; &lt;chr&gt; &lt;dbl&gt;\n 1     1 A         1\n 2     1 B         1\n 3     1 C         1\n 4     2 A         1\n 5     2 B         1\n 6     3 A         1\n 7     3 C         1\n 8     4 B         1\n 9     4 C         1\n10     5 A         1\n11     6 B         1\n12     7 C         1\n\n\nつぎにこれを横持ちに展開する。\n\ndata |&gt; \n  separate_longer_delim(cols = q1, delim = ',') |&gt;\n  mutate(flag = 1) |&gt; \n  pivot_wider(names_from = q1, values_from = flag)\n\n# A tibble: 7 × 4\n     id     A     B     C\n  &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     1     1     1     1\n2     2     1     1    NA\n3     3     1    NA     1\n4     4    NA     1     1\n5     5     1    NA    NA\n6     6    NA     1    NA\n7     7    NA    NA     1\n\n\nこれで、A,B,Cの選択状況がわかるようになった。\n非選択がNAになっているのは少々アレなので、0で埋めるようにする。\n\ndata |&gt; \n  separate_longer_delim(cols = q1, delim = ',') |&gt;\n  mutate(flag = 1) |&gt; \n  pivot_wider(names_from = q1, values_from = flag, values_fill = 0)\n\n# A tibble: 7 × 4\n     id     A     B     C\n  &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     1     1     1     1\n2     2     1     1     0\n3     3     1     0     1\n4     4     0     1     1\n5     5     1     0     0\n6     6     0     1     0\n7     7     0     0     1\n\n\nこれでダミー変数に変換できた。\nもともとの変数名であるq1が消えてしまっているので、それを残すにはnames_prefixを指定する。\n\ndata |&gt; \n  separate_longer_delim(cols = q1, delim = ',') |&gt;\n  mutate(flag = 1) |&gt; \n  pivot_wider(\n    names_from = q1, \n    names_prefix = 'q1_', \n    values_from = flag, \n    values_fill = 0\n  ) \n\n# A tibble: 7 × 4\n     id  q1_A  q1_B  q1_C\n  &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     1     1     1     1\n2     2     1     1     0\n3     3     1     0     1\n4     4     0     1     1\n5     5     1     0     0\n6     6     0     1     0\n7     7     0     0     1"
  },
  {
    "objectID": "01_R_Tips/tensorflow_with_Rstudio.html",
    "href": "01_R_Tips/tensorflow_with_Rstudio.html",
    "title": "tensorflowをRstudioで",
    "section": "",
    "text": "pythonをRStudioで使うのはreticulateパッケージによりだいぶ楽になった。\nただpythonにおけるdeep learningの代表的パッケージであるtensorflowをRStudioで使う際に少しつまづいたのでメモです。"
  },
  {
    "objectID": "01_R_Tips/tensorflow_with_Rstudio.html#pythonの環境をどうやって構築するか",
    "href": "01_R_Tips/tensorflow_with_Rstudio.html#pythonの環境をどうやって構築するか",
    "title": "tensorflowをRstudioで",
    "section": "pythonの環境をどうやって構築するか？",
    "text": "pythonの環境をどうやって構築するか？\nRではrenvパッケージを使っている。ではpythonでは？\npythonはRよりも環境が複雑で変化も早そうなので、環境構築は重要。\n今回はpython公式が推奨しているっぽいvenvを使う。\n\n\n\n\n\n\nNote\n\n\n\n最初global環境でそのままやろうとしたが上手くいかず挫折。Dockerとかも有力だと思う。"
  },
  {
    "objectID": "01_R_Tips/tensorflow_with_Rstudio.html#venvを使って仮想環境を用意",
    "href": "01_R_Tips/tensorflow_with_Rstudio.html#venvを使って仮想環境を用意",
    "title": "tensorflowをRstudioで",
    "section": "venvを使って仮想環境を用意",
    "text": "venvを使って仮想環境を用意\n\npythonがインストールされているか確認\n\nhomebrewなりでインストールしておく\nPATHを通しておく\n\n作業ディレクトリにて、terminalで以下のコマンドを実行し.venvフォルダを作成\n\n\n\n\nterminal\n\npython3 -m venv .venv"
  },
  {
    "objectID": "01_R_Tips/tensorflow_with_Rstudio.html#pythonライブラリをインストール",
    "href": "01_R_Tips/tensorflow_with_Rstudio.html#pythonライブラリをインストール",
    "title": "tensorflowをRstudioで",
    "section": "pythonライブラリをインストール",
    "text": "pythonライブラリをインストール\n\n以下のコマンドを実行し、仮想環境を起動\n\npythonの場所が.venv内のフォルダになる\n\n\n\n\n\nterminal\n\nsource .venv/bin/activate\n\n\n\n\n\n\n\n\nNote\n\n\n\nsource .venv/bin/activateは、ライブラリをインストールするときは毎回実行する必要があります！\nまちがってglobal環境を汚さないように！\n\n\n\nライブラリをインストール\n\ntensorflowのほか、numpyも必須（tensorflowをインストールしたときに入る）\nM1 macでGPUを使いたい場合はtensorflow-metalもインストールする\n\n\n\n\n\nterminal\n\npip install tensorflow-macos\npip install tensorflow-metal\n\n\n\n仮想環境を抜けるときは以下のコマンドを実行\n\n\n\n\nterminal\n\ndeactivate"
  },
  {
    "objectID": "01_R_Tips/tensorflow_with_Rstudio.html#rstudioでpythonを使う",
    "href": "01_R_Tips/tensorflow_with_Rstudio.html#rstudioでpythonを使う",
    "title": "tensorflowをRstudioで",
    "section": "RStudioでpythonを使う",
    "text": "RStudioでpythonを使う\n\npythonの場所を指定\n\nRStudioでpythonを使うには、RStudioにpythonの場所を教えてやる必要がある\n\n今回は.venv内のpythonを使いたいので、作業ディレクトリに.Rprofileを作成し、以下のコードを書く\nrenvを使っている場合は、.Rprofileが既に作成されていると思うので、その中に追記する\n\n\n\n\n\n.Rprofile\n\n# renv起動\nsource(\"renv/activate.R\")\n# pythonの場所を指定\nSys.setenv(RETICULATE_PYTHON = \".venv/bin/python3\")\n\n\n\n\n\n\n\n\nNote\n\n\n\npythonの場所の指定はrenvの起動より後に行う必要がある。もし先にpythonの場所を指定してしまうと、renvを起動したときにpythonの場所が上書きされてしまい、上手くいかなくなる。\n\n\n\n\npythonを起動\n\nRStudioでpythonのコードを実行すると、reticulate::repl_python()が走ってコンソールがpythonになる\n\n\n\n\npython\n\nimport sys\nprint(sys.version)\n\n\n3.11.9 (main, Apr  2 2024, 08:25:04) [Clang 15.0.0 (clang-1500.3.9.4)]\n\n\n\ntensorflowを起動して、GPUが使えるかを確認\nphysical_deviceでCPUとGPUの両方が表示されてればOK\n\n\n\n\npython\n\nimport tensorflow as tf\ntf.config.list_physical_devices()\n\n\n[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
  },
  {
    "objectID": "01_R_Tips/tensorflow_with_Rstudio.html#ニューラルネットをやってみる",
    "href": "01_R_Tips/tensorflow_with_Rstudio.html#ニューラルネットをやってみる",
    "title": "tensorflowをRstudioで",
    "section": "ニューラルネットをやってみる",
    "text": "ニューラルネットをやってみる\n\nチュートリアルとして、手書きのアルファベットを分類するモデルを作成してみる\nRでデータを読み込んで、pythonに渡してtensorflowで分析\n参考\n\n\n\n\nR\n\nlibrary(tidyverse)\nlibrary(rsample)\nlibrary(reticulate)\n\n\n\nデータのダウンロード\n\n\n\nR\n\n# データのダウンロード\nif (!dir.exists('data')) dir.create('data')\ndownload.file('https://ai.stanford.edu/~btaskar/ocr/letter.data.gz', destfile = 'data/letter.data')\n\n\n\n\nデータの読み込み\n\n列名はhttps://ai.stanford.edu/~btaskar/ocr/letter.namesより取得できる\n詳細は以下の通り\n\nid: each letter is assigned a unique integer id\nletter: a-z\nnext_id: id for next letter in the word, -1 if last letter\nword_id: each word is assigned a unique integer id (not used)\nposition: position of letter in the word (not used)\nfold: 0-9 – cross-validation fold\np_i_j: 0/1 – value of pixel in row i, column j\n\nletterとp_i_j（pixel）のみ使うのでそれ以外は削除\n\n\n\n\nR\n\ncolname &lt;- read_lines('https://ai.stanford.edu/~btaskar/ocr/letter.names')\ndata &lt;- \n  read_tsv('data/letter.data', col_names = colname) |&gt; \n  # letterとpixelだけ残す\n  select(id, letter, matches('p_\\\\d+_\\\\d+'))\n\n\n\n\nデータの前処理\n\nletterを数値に変換\n\n\n\n\nR\n\ndf &lt;- \n  data |&gt; \n  mutate(\n    letter = factor(letter),\n    # pythonは0からはじまるので-1する（Rは1から）\n    letter_num = as.numeric(letter) - 1\n  )\n\n\n\n訓練データとテストデータに分割\n\n\n\n\nR\n\nsplit &lt;- rsample::initial_split(df, prop = 0.9)\ntrain &lt;- rsample::training(split)\ntest &lt;- rsample::testing(split)\n\n\n\npythonに渡せる形にデータを変換\n\nreticulate::np_array()であらかじめndarrayにしておくと便利\n\n\n\n\n\nR\n\ntrain_x &lt;- \n  train |&gt; \n  select(!c(id, letter, letter_num)) |&gt;\n  as.matrix() |&gt; \n  np_array()\n\ntrain_y &lt;-\n  train |&gt;\n  pull(letter_num) |&gt;\n  np_array()\n\ntest_x &lt;-\n  test |&gt; \n  select(!c(id, letter, letter_num)) |&gt;\n  as.matrix() |&gt; \n  np_array()\n\ntest_y &lt;-\n  test |&gt;\n  pull(letter_num) |&gt;\n  np_array()\n\n\n\n\nニューラルネットで学習\n\n中間層が1つのニューラルネットを作成\n\n入力層は16x8=128（ピクセル）\n中間層は64次元\n出力層はa-zの26分類\n\n\n\n\n\npython\n\nimport tensorflow as tf\nimport numpy as np\n\n# モデルの作成\nmodel = tf.keras.Sequential([\n  tf.keras.layers.Dense(64, activation='relu'),\n  tf.keras.layers.Dense(26, activation='softmax')\n])\n\nmodel.compile(\n  optimizer='adam',\n  loss='sparse_categorical_crossentropy',\n  metrics=['accuracy']\n)\n\n\n\nrで作成したデータをpythonに渡して学習\nr.train_xでRのtrain_xを参照できる\npythonのオブジェクトを渡すときはpy$train_x\n\n\n\n\npython\n\n# モデルの学習\nmodel.fit(r.train_x, r.train_y, epochs=10, verbose=2)\n\n\nEpoch 1/10\n1467/1467 - 8s - loss: 1.3368 - accuracy: 0.6375 - 8s/epoch - 6ms/step\nEpoch 2/10\n1467/1467 - 6s - loss: 0.9414 - accuracy: 0.7472 - 6s/epoch - 4ms/step\nEpoch 3/10\n1467/1467 - 6s - loss: 0.8763 - accuracy: 0.7654 - 6s/epoch - 4ms/step\nEpoch 4/10\n1467/1467 - 5s - loss: 0.8460 - accuracy: 0.7722 - 5s/epoch - 4ms/step\nEpoch 5/10\n1467/1467 - 5s - loss: 0.8300 - accuracy: 0.7778 - 5s/epoch - 4ms/step\nEpoch 6/10\n1467/1467 - 5s - loss: 0.8217 - accuracy: 0.7804 - 5s/epoch - 4ms/step\nEpoch 7/10\n1467/1467 - 5s - loss: 0.8176 - accuracy: 0.7794 - 5s/epoch - 4ms/step\nEpoch 8/10\n1467/1467 - 5s - loss: 0.8149 - accuracy: 0.7816 - 5s/epoch - 4ms/step\nEpoch 9/10\n1467/1467 - 5s - loss: 0.8158 - accuracy: 0.7801 - 5s/epoch - 4ms/step\nEpoch 10/10\n1467/1467 - 5s - loss: 0.8173 - accuracy: 0.7795 - 5s/epoch - 4ms/step\n&lt;keras.src.callbacks.History object at 0x3032ac210&gt;\n\n\n\n\nテストデータで評価\n\nテストデータでの正答率は77%くらい\n\n\n\n\npython\n\n# モデルの評価\nmodel.evaluate(r.test_x, r.test_y, verbose=2)\n\n\n163/163 - 1s - loss: 0.8477 - accuracy: 0.7665 - 543ms/epoch - 3ms/step\n[0.8476578593254089, 0.766487717628479]"
  },
  {
    "objectID": "01_Causal_Inference/Sudharsanan_Bijlsma_2021.html",
    "href": "01_Causal_Inference/Sudharsanan_Bijlsma_2021.html",
    "title": "Sudharsanan & Bijlsma (2021)",
    "section": "",
    "text": "Sudharsanan, Nikkil & Maarten J. Bijlsma, 2021, “Educational Note: Causal Decomposition of Population Health Differences Using Monte Carlo Integration and the G-Formula,” International Journal of Epidemiology, 50(6): 2098–2107 (10.1093/ije/dyab090)."
  },
  {
    "objectID": "01_Causal_Inference/Sudharsanan_Bijlsma_2021.html#文献",
    "href": "01_Causal_Inference/Sudharsanan_Bijlsma_2021.html#文献",
    "title": "Sudharsanan & Bijlsma (2021)",
    "section": "",
    "text": "Sudharsanan, Nikkil & Maarten J. Bijlsma, 2021, “Educational Note: Causal Decomposition of Population Health Differences Using Monte Carlo Integration and the G-Formula,” International Journal of Epidemiology, 50(6): 2098–2107 (10.1093/ije/dyab090)."
  },
  {
    "objectID": "01_Causal_Inference/Sudharsanan_Bijlsma_2021.html#下準備",
    "href": "01_Causal_Inference/Sudharsanan_Bijlsma_2021.html#下準備",
    "title": "Sudharsanan & Bijlsma (2021)",
    "section": "下準備",
    "text": "下準備\n\nパッケージ\n\n\nlibrary(tidyverse)\nlibrary(cfdecomp)\n\nkable &lt;- partial(\n  knitr::kable,\n  digits = 3\n)\n\n\nデータ\n\n\n# the decomposition functions in our package are computationally intensive\n# to make the example run quick, I perform it on a subsample (n=125) of the data:\nset.seed(100)\ndata &lt;- \n  cfd.example.data[sample(1000),] |&gt; \n  select(SES, age, med.gauss, out.gauss, id) |&gt; \n  as_tibble()"
  },
  {
    "objectID": "01_Causal_Inference/Sudharsanan_Bijlsma_2021.html#パッケージによる推定",
    "href": "01_Causal_Inference/Sudharsanan_Bijlsma_2021.html#パッケージによる推定",
    "title": "Sudharsanan & Bijlsma (2021)",
    "section": "パッケージによる推定",
    "text": "パッケージによる推定\n\ncfdecompパッケージで推定\n\n\n# cfd.mean \nmean.results.1 &lt;- \n  cfd.mean(\n    formula.y = out.gauss ~ SES * med.gauss * age,\n    formula.m = med.gauss ~ SES * age,\n    mediator = 'med.gauss',\n    group = 'SES',\n    data = as.data.frame(data),\n    family.y = 'gaussian',\n    family.m = 'gaussian',\n    bs.size=250,\n    mc.size=10,\n    alpha=0.05,\n    # cluster.sample=FALSE,\n    # cluster.name='id'\n  )\n\n\nSES2とSES3のmediatorの分布をSES1のmediatorの分布に揃える\n\n\ntibble(\n  category = c(\"SES1\", \"SES2\", \"SES3\"),\n  factual_mean = c(\n    mean(mean.results.1$out_nc_y[, 1]),\n    mean(mean.results.1$out_nc_y[, 2]),\n    mean(mean.results.1$out_nc_y[, 3])\n  ),\n  # and after giving the gaussian mediator of SES group 2 the distribution of the one in group 1\n  # the difference becomes:\n  counterfactual_mean = c(\n    mean(mean.results.1$out_cf_y[, 1]),\n    mean(mean.results.1$out_cf_y[, 2]),\n    mean(mean.results.1$out_cf_y[, 3])\n  )) |&gt; \n  kable()\n\n\n\n\ncategory\nfactual_mean\ncounterfactual_mean\n\n\n\n\nSES1\n4.306\n4.306\n\n\nSES2\n3.239\n3.511\n\n\nSES3\n2.212\n2.946"
  },
  {
    "objectID": "01_Causal_Inference/Sudharsanan_Bijlsma_2021.html#自分でコードを書いてみる",
    "href": "01_Causal_Inference/Sudharsanan_Bijlsma_2021.html#自分でコードを書いてみる",
    "title": "Sudharsanan & Bijlsma (2021)",
    "section": "自分でコードを書いてみる",
    "text": "自分でコードを書いてみる"
  },
  {
    "objectID": "01_Causal_Inference/Sudharsanan_Bijlsma_2021.html#step-1-regression-estimates",
    "href": "01_Causal_Inference/Sudharsanan_Bijlsma_2021.html#step-1-regression-estimates",
    "title": "Sudharsanan & Bijlsma (2021)",
    "section": "step 1: regression estimates",
    "text": "step 1: regression estimates\n\nmediatorとoutcomeのモデルをデータから推定\n\n\nmediator_model &lt;- lm(med.gauss ~ SES * age, data = data)\noutcome_model &lt;- lm(out.gauss ~ SES * age * med.gauss, data = data)"
  },
  {
    "objectID": "01_Causal_Inference/Sudharsanan_Bijlsma_2021.html#step-2-simulate-the-natural-course-pseudo-population",
    "href": "01_Causal_Inference/Sudharsanan_Bijlsma_2021.html#step-2-simulate-the-natural-course-pseudo-population",
    "title": "Sudharsanan & Bijlsma (2021)",
    "section": "step 2: simulate the natural-course pseudo-population",
    "text": "step 2: simulate the natural-course pseudo-population\n\n推定したmediatorのモデルから、mediatorの「分布」を再現\n\n\\[\\begin{align*}\nMed_i = \\mathrm{E}[Med | X] + e_i \\\\\ne_i \\sim \\mathrm{N}(0, \\sigma)\n\\end{align*}\\]\n\nmediatorの分布のパラメータを取得\n\n\n# predict mediator\n# mediatorの「分布」のパラメータを取得\npred_mean_m &lt;- predict(mediator_model, newdata = data, type = \"response\")\nresidual_ref_m &lt;- mediator_model$residuals\nsd_ref_m &lt;- sd(residual_ref_m)\n\n\n推定したパラメータをもとに、mediatorの値をシミュレート\n\n\ndf_nc_med &lt;- \n  data |&gt; \n  mutate(\n    # ランダム性なし\n    pred_med = pred_mean_m,\n    # ランダム性をもたせる1（推定した標準偏差のパラメータを使用）\n    pred_med_draw_1 = rnorm(n(), mean = pred_mean_m, sd = sd_ref_m),\n    # ランダム性をもたせる2（残差からランダムにサンプリング）\n    pred_med_draw_2 = pred_mean_m + sample(residual_ref_m, n(), replace = TRUE)\n  )\n\ndf_nc_med |&gt; \n  summarise(\n    across(c(med.gauss, pred_med:pred_med_draw_2), mean),\n    .by = SES\n  ) |&gt; \n  arrange(SES) |&gt; \n  kable()\n\n\n\n\nSES\nmed.gauss\npred_med\npred_med_draw_1\npred_med_draw_2\n\n\n\n\n1\n8.428\n8.428\n8.418\n8.286\n\n\n2\n7.223\n7.223\n7.276\n7.454\n\n\n3\n5.410\n5.410\n5.579\n5.374\n\n\n\n\n\n\nシミュレートした値をoutcomeモデルに代入して予測値を計算・集計\n\n\ndf_nc_med |&gt; \n  mutate(\n    # ランダム性なし\n    pred_out = predict(\n      outcome_model, newdata = df_nc_med |&gt; mutate(med.gauss = pred_med)\n    ),\n    # ランダム性をもたせる1（推定した標準偏差のパラメータを使用）\n    pred_out_draw_1 = predict(\n      outcome_model, newdata = df_nc_med |&gt; mutate(med.gauss = pred_med_draw_1)\n    ),\n    # ランダム性をもたせる2（残差からランダムにサンプリング）\n    pred_out_draw_2 = predict(\n      outcome_model, newdata = df_nc_med |&gt; mutate(med.gauss = pred_med_draw_2)\n    )\n  ) |&gt; \n  summarise(\n    across(c(out.gauss, pred_out:pred_out_draw_2), mean),\n    .by = SES\n  ) |&gt; \n  arrange(SES) |&gt; \n  kable()\n\n\n\n\nSES\nout.gauss\npred_out\npred_out_draw_1\npred_out_draw_2\n\n\n\n\n1\n4.309\n4.309\n4.312\n4.286\n\n\n2\n3.237\n3.237\n3.249\n3.292\n\n\n3\n2.217\n2.217\n2.254\n2.214"
  },
  {
    "objectID": "01_Causal_Inference/Sudharsanan_Bijlsma_2021.html#step-3-simulate-the-counterfactual-pseudo-population",
    "href": "01_Causal_Inference/Sudharsanan_Bijlsma_2021.html#step-3-simulate-the-counterfactual-pseudo-population",
    "title": "Sudharsanan & Bijlsma (2021)",
    "section": "step 3: simulate the counterfactual pseudo-population",
    "text": "step 3: simulate the counterfactual pseudo-population\n\n推定したmediatorのモデルにおいて、全員のSESが1だった場合のmediatorの分布を再現\n全員のSESを1にしてmediatorのパラメータを取得\n回帰モデルでは残差の部分は共変量に依存しない（SES間で分布が同じ、平均0・共通の標準偏差の正規分布）\n\nならばSESが1のグループの標準偏差を使わなくても良いのでは？（全体の標準偏差でもよい）\n標準偏差もグループによって異なる、といったモデルの場合にはどうなるか？\n\n\n\n# 平均\npred_mean_m_SES1 &lt;- predict(mediator_model, newdata = data |&gt; mutate(SES = '1'))\n# SES = 1のグループの残差\nresidual_ref_m_SES1 &lt;-\n  broom::augment(mediator_model)  |&gt;\n  filter(SES == '1') |&gt; \n  pull(.resid)\n# 標準偏差\nsd_ref_m_SES1 &lt;- sd(residual_ref_m_SES1)\n\n\n推定したパラメータをもとに、mediatorの値をシミュレート\n\n\ndf_cf_med &lt;- \n  data |&gt; \n  mutate(\n    # ランダム性なし\n    pred_med_SES1 = pred_mean_m_SES1,\n    # ランダム性をもたせる1（推定した標準偏差のパラメータを使用）\n    pred_med_draw_1_SES1 = rnorm(n(), mean = pred_mean_m_SES1, sd = sd_ref_m_SES1),\n    # ランダム性をもたせる2（残差からランダムにサンプリング）\n    pred_med_draw_2_SES1 = pred_mean_m_SES1 + sample(residual_ref_m_SES1, n(), replace = TRUE)\n  )\n\ndf_cf_med |&gt; \n  summarise(\n    across(c(med.gauss, pred_med_SES1:pred_med_draw_2_SES1), mean),\n    .by = SES\n  ) |&gt; \n  arrange(SES) |&gt; \n  kable()\n\n\n\n\n\n\n\n\n\n\n\nSES\nmed.gauss\npred_med_SES1\npred_med_draw_1_SES1\npred_med_draw_2_SES1\n\n\n\n\n1\n8.428\n8.428\n8.386\n8.632\n\n\n2\n7.223\n8.337\n8.258\n8.292\n\n\n3\n5.410\n8.442\n8.292\n8.593\n\n\n\n\n\n\nシミュレートした値をoutcomeモデルに代入して予測値を計算・集計\n\n\ndf_cf_med |&gt; \n  mutate(\n    # ランダム性なし\n    pred_out_SES1 = predict(\n      outcome_model, newdata = df_cf_med |&gt; mutate(med.gauss = pred_med_SES1)\n    ),\n    # ランダム性をもたせる1（推定した標準偏差のパラメータを使用）\n    pred_out_draw_1_SES1 = predict(\n      outcome_model, newdata = df_cf_med |&gt; mutate(med.gauss = pred_med_draw_1_SES1)\n    ),\n    # ランダム性をもたせる2（残差からランダムにサンプリング）\n    pred_out_draw_2_SES1 = predict(\n      outcome_model, newdata = df_cf_med |&gt; mutate(med.gauss = pred_med_draw_2_SES1)\n    )\n  ) |&gt; \n  summarise(\n    across(c(out.gauss, pred_out_SES1:pred_out_draw_2_SES1), mean),\n    .by = SES\n  ) |&gt; \n  arrange(SES) |&gt; \n  kable()\n\n\n\n\n\n\n\n\n\n\n\nSES\nout.gauss\npred_out_SES1\npred_out_draw_1_SES1\npred_out_draw_2_SES1\n\n\n\n\n1\n4.309\n4.309\n4.303\n4.347\n\n\n2\n3.237\n3.507\n3.485\n3.498\n\n\n3\n2.217\n2.949\n2.920\n2.979"
  },
  {
    "objectID": "01_Causal_Inference/Sudharsanan_Bijlsma_2021.html#monte-carloとbootstrapの実装",
    "href": "01_Causal_Inference/Sudharsanan_Bijlsma_2021.html#monte-carloとbootstrapの実装",
    "title": "Sudharsanan & Bijlsma (2021)",
    "section": "monte carloとbootstrapの実装",
    "text": "monte carloとbootstrapの実装\n\n実際にはrandom drawは一回ではなく何回か行うことで不確実性を表現する\n標準誤差の推定のためにbootstrap法も必要\nまずはtreatmentとoutcomeのモデルを推定し、パラメータを取得\n\n\n# パラメータ推定\nestimate_model &lt;- function(data) {\n  \n  mediator_model &lt;- lm(med.gauss ~ SES * age, data = data)\n  outcome_model &lt;- lm(out.gauss ~ SES * age * med.gauss, data = data)\n  \n  pred_mean_m &lt;- predict(mediator_model, newdata = data, type = \"response\")\n  residual_ref_m &lt;- mediator_model$residuals\n  sd_ref_m &lt;- sd(residual_ref_m)\n\n  pred_mean_m_SES1 &lt;- predict(mediator_model, newdata = data |&gt; mutate(SES = '1'))\n  residual_ref_m_SES1 &lt;-\n    broom::augment(mediator_model)  |&gt;\n    filter(SES == '1') |&gt; \n    pull(.resid)\n  sd_ref_m_SES1 &lt;- sd(residual_ref_m_SES1)\n\n}\n\n\nパラメータをもとにmediatorをシミュレートするのを何回か繰り返す\n\n\nmontecarlo_sampling &lt;- function(data, mc = 10) {\n  # パラメータ推定\n  estimate_model(data)\n  # モンテカルロシミュレーション\n  map(1:mc, \\(mc) {\n    # mediatorサンプリング\n    boot_sample &lt;- \n      data |&gt; \n      mutate(\n        pred_med_draw_1 = rnorm(n(), mean = pred_mean_m, sd = sd_ref_m),\n        pred_med_draw_2 = pred_mean_m + sample(residual_ref_m, n(), replace = TRUE),\n        pred_med_draw_1_SES1 = rnorm(n(), mean = pred_mean_m_SES1, sd = sd_ref_m_SES1),\n        pred_med_draw_2_SES1 = pred_mean_m_SES1 + sample(residual_ref_m_SES1, n(), replace = TRUE),\n      )\n    # サンプリングしたものからoutcome予測\n    boot_sample |&gt; \n      mutate(\n        pred_out_draw_1 = predict(\n          outcome_model, newdata = boot_sample |&gt; mutate(med.gauss = pred_med_draw_1)\n        ),\n        pred_out_draw_2 = predict(\n          outcome_model, newdata = boot_sample |&gt; mutate(med.gauss = pred_med_draw_2)\n        ),\n        pred_out_draw_1_SES1 = predict(\n          outcome_model, newdata = boot_sample |&gt; mutate(med.gauss = pred_med_draw_1_SES1)\n        ),\n        pred_out_draw_2_SES1 = predict(\n          outcome_model, newdata = boot_sample |&gt; mutate(med.gauss = pred_med_draw_2_SES1)\n        )\n      ) |&gt; \n      group_by(SES) |&gt; \n      summarise(across(c(pred_out_draw_1:pred_out_draw_2_SES1), mean))\n    \n  }) |&gt; \n    list_rbind(names_to = 'mc') |&gt; \n    # シミュレーション結果を集計\n    group_by(SES) |&gt; \n    summarise(across(c(pred_out_draw_1:pred_out_draw_2_SES1), mean))\n  \n}\n\n\nこれをbootstrapで繰り返す\n\n\nresult &lt;- \n  map(1:250, \\(index) {\n    # bootstrapサンプル発生\n    bootsample &lt;- slice_sample(data, prop = 1, replace = TRUE)\n    montecarlo_sampling(bootsample, mc = 10) \n  }) |&gt; \n  list_rbind(names_to = 'index')\n\n\n結果を集計\n\n\nresult |&gt; \n  pivot_longer(\n    cols = c(pred_out_draw_1:pred_out_draw_2_SES1), \n    names_to = 'type', \n    values_to = 'value'\n  ) |&gt; \n  summarise(\n    mean = mean(value), \n    conf.low = quantile(value, 0.025),\n    conf.high = quantile(value, 0.975),\n    .by = c(SES, type)\n  ) |&gt; \n  arrange(type) |&gt; \n  kable()\n\n\n\n\nSES\ntype\nmean\nconf.low\nconf.high\n\n\n\n\n1\npred_out_draw_1\n4.015\n3.937\n4.091\n\n\n2\npred_out_draw_1\n3.183\n3.124\n3.251\n\n\n3\npred_out_draw_1\n2.645\n2.599\n2.688\n\n\n1\npred_out_draw_1_SES1\n4.276\n4.193\n4.351\n\n\n2\npred_out_draw_1_SES1\n3.513\n3.446\n3.582\n\n\n3\npred_out_draw_1_SES1\n2.970\n2.934\n3.000\n\n\n1\npred_out_draw_2\n4.015\n3.938\n4.094\n\n\n2\npred_out_draw_2\n3.182\n3.123\n3.253\n\n\n3\npred_out_draw_2\n2.646\n2.600\n2.687\n\n\n1\npred_out_draw_2_SES1\n4.276\n4.183\n4.358\n\n\n2\npred_out_draw_2_SES1\n3.515\n3.457\n3.584\n\n\n3\npred_out_draw_2_SES1\n2.970\n2.941\n2.998"
  },
  {
    "objectID": "01_Causal_Inference/L-TMLE.html",
    "href": "01_Causal_Inference/L-TMLE.html",
    "title": "Longitudinal TMLE",
    "section": "",
    "text": "Hernán and Robins (2020) では、TMLEは時間依存交絡などに対処するG-Methodの一般化として提示されている。ここではltmleパッケージを用いた時間依存交絡への対処を中心に記述する。\n\n\n\n\n\nlibrary(tidyverse)\nlibrary(broom)\nlibrary(SuperLearner)\nlibrary(ltmle)\nlibrary(randomForest)\nlibrary(ggdag)\nlibrary(future)\n\nkable &lt;- partial(\n  knitr::kable,\n  digits = 3\n)\n\noptions(mc.cores = 8)\nplan(strategy = 'multisession')\n\nset.seed(95)\n\n\n\n\n以下のDAGを考える\n\ndagify(\n  L1 ~ A0 + U1,\n  A1 ~ A0 + L1,\n  Y ~ U1,\n  exposure = c('A0', 'A1'), \n  outcome = 'Y', \n  coords = list(x = c(A0 = 0, L1 = 1, A1 = 2, U1 = 1, Y = 3),\n                y = c(A0 = 0, L1 = 0, A1 = 0, U1 = -1, Y = 0))\n) |&gt; \n  ggdag()+\n  theme_dag()\n\n\n\n\n\n\n\n\nTreatmentはA0とA1であるが、ここからYへのpathはないので、因果効果は全ての組み合わせについて0になる\n具体的には、potential outcomeのすべての組み合わせについて、差分を取った値が0になる\n\\[\\begin{align}\n\\mathrm{E}[Y^{0, 0} - Y^{1, 0}] &= 0\\\\\n\\mathrm{E}[Y^{0, 0} - Y^{0, 1}] &= 0\\\\\n\\mathrm{E}[Y^{0, 0} - Y^{1, 1}] &= 0\\\\\n\\mathrm{E}[Y^{1, 0} - Y^{0, 1}] &= 0\\\\\n\\mathrm{E}[Y^{1, 0} - Y^{1, 1}] &= 0\\\\\n\\mathrm{E}[Y^{0, 1} - Y^{1, 1}] &= 0\n\\end{align}\\]\n\ngenerate_data &lt;- function(n){ \n    A0 &lt;- rbinom(n, size=1, prob=0.3) # binary treatment\n    U1 &lt;- rnorm(n, mean = 0, sd = 1) # latent continuous confounder\n    L1 &lt;- 0.1 + 0.3*A0 + 0.3*U1 + rnorm(n, sd = 0.1)\n    A1 &lt;- rbinom(n, size=1, prob = plogis(-0.2 + 0.3*L1 + 0.02*L1^2)) # binary treatment\n    Y &lt;- -0.2 + 0.5*U1 + rnorm(n, sd = 0.1) # continuous outcome depends on confounders\n    return(tibble(Y, A0, A1, L1, U1))\n}\n\ndata_obs &lt;- generate_data(1000)\n\n\nA0の因果効果\n\n\ndata_obs |&gt; \n  lm(Y ~ A0, data = _) |&gt; \n  tidy() |&gt; \n  kable()\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-0.207\n0.019\n-10.925\n0.000\n\n\nA0\n0.029\n0.036\n0.804\n0.422\n\n\n\n\n\n\nA1の因果効果\n\n\ndata_obs |&gt; \n  lm(Y ~ A1 + L1, data = _) |&gt; \n  tidy() |&gt; \n  kable()\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-0.427\n0.012\n-34.532\n0.000\n\n\nA1\n-0.017\n0.017\n-1.020\n0.308\n\n\nL1\n1.275\n0.025\n51.719\n0.000\n\n\n\n\n\n\ncollider bias\n\n\\(L_1\\)を条件づけると、\\(A_0 \\to L_1 \\gets U_1 \\to Y\\)というパスが開いて、バイアスをもたらす\n\ndata_obs |&gt; \n  lm(Y ~ A0 + L1, data = _) |&gt; \n  tidy() |&gt; \n  kable()\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-0.352\n0.007\n-49.626\n0\n\n\nA0\n-0.469\n0.014\n-32.796\n0\n\n\nL1\n1.540\n0.019\n81.391\n0\n\n\n\n\n\n\ndata_obs |&gt; \n  lm(Y ~ A0*A1, data = _) |&gt; \n  tidy() |&gt; \n  kable()\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-0.217\n0.026\n-8.364\n0.000\n\n\nA0\n0.043\n0.051\n0.859\n0.390\n\n\nA1\n0.021\n0.038\n0.559\n0.576\n\n\nA0:A1\n-0.031\n0.072\n-0.428\n0.669\n\n\n\n\n\n\n\n\n\nHernán and Robins (2020) にて紹介されている方法\n以下のウェイトを作成する\n\\[\nW^{A_0, A_1} = \\frac{1}{f(A_0 | L_0)} \\times \\frac{1}{f(A_1 | A_0, L_0, L_1)}\n\\]\n今回はベースライン共変量がない（\\(L_0 = \\varnothing\\)）ので\n\\[\nW^{A_0, A_1} = \\frac{1}{f(A_0)} \\times \\frac{1}{f(A_1 | A_0, L_1)}\n\\]\n\nres_ipw &lt;-\n  broom::augment_columns(\n    glm(A0 ~ 1, data = data_obs, family = 'binomial'), \n    data = data_obs, \n    type.predict = 'response'\n  ) |&gt; \n  # A0に対するウェイト\n  mutate(\n    ipw_A0 = case_when(\n      A0 == 1 ~ 1 / .fitted,\n      A0 == 0 ~ 1 / (1 - .fitted)\n    )\n  ) |&gt; \n  select(Y:U1, ipw_A0) |&gt; \n  broom::augment_columns(\n    glm(A1 ~ A0 + L1 + I(L1^2), data = data_obs, family = 'binomial'),\n    data = _,\n    type.predict = 'response'\n  ) |&gt; \n  mutate(\n    # A1に対するウェイト\n    ipw_A1 = case_when(\n      A1 == 1 ~ 1 / .fitted,\n      A1 == 0 ~ 1 / (1 - .fitted)\n    )\n  ) |&gt; \n  select(Y:U1, ipw_A0, ipw_A1) |&gt; \n  # A0に対するウェイトとA1に対するウェイトをかけ算\n  mutate(ipw = ipw_A0*ipw_A1) |&gt; \n  # 重み付け推定\n  summarise(CFmean = weighted.mean(Y, ipw), .by = c(A0, A1)) |&gt; \n  arrange(A0, A1)\n\nipwによる各treatment strategyにおけるcounterfactual mean\n\nres_ipw\n\n# A tibble: 4 × 3\n     A0    A1 CFmean\n  &lt;int&gt; &lt;int&gt;  &lt;dbl&gt;\n1     0     0 -0.208\n2     0     1 -0.207\n3     1     0 -0.166\n4     1     1 -0.189\n\n\ncounterfactual meanの差分\n\ncross_join(\n  res_ipw |&gt; \n    mutate(A0_A1 = str_c(A0, ',', A1)) |&gt; \n    select(A0_A1, CFmean),\n  res_ipw |&gt; \n    mutate(A0_A1 = str_c(A0, ',', A1)) |&gt; \n    select(A0_A1, CFmean)\n  ) |&gt; \n  filter(A0_A1.x != A0_A1.y) |&gt; \n  mutate(ATE = CFmean.y - CFmean.x)\n\n# A tibble: 12 × 5\n   A0_A1.x CFmean.x A0_A1.y CFmean.y       ATE\n   &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;      &lt;dbl&gt;     &lt;dbl&gt;\n 1 0,0       -0.208 0,1       -0.207  0.000654\n 2 0,0       -0.208 1,0       -0.166  0.0419  \n 3 0,0       -0.208 1,1       -0.189  0.0190  \n 4 0,1       -0.207 0,0       -0.208 -0.000654\n 5 0,1       -0.207 1,0       -0.166  0.0412  \n 6 0,1       -0.207 1,1       -0.189  0.0184  \n 7 1,0       -0.166 0,0       -0.208 -0.0419  \n 8 1,0       -0.166 0,1       -0.207 -0.0412  \n 9 1,0       -0.166 1,1       -0.189 -0.0229  \n10 1,1       -0.189 0,0       -0.208 -0.0190  \n11 1,1       -0.189 0,1       -0.207 -0.0184  \n12 1,1       -0.189 1,0       -0.166  0.0229  \n\n\n\n\n\n\nltmleによる実装\n\n\nres_tmle &lt;- \n  ltmle::ltmle(\n    # データセットの列の順番が大切\n    data = data_obs |&gt; select(A0, L1, A1, Y), \n    Anodes = c('A0', 'A1'), \n    Lnodes = 'L1',\n    Ynodes = 'Y',\n    abar = list(treatment = c(1, 1), control = c(0, 0)), \n    SL.library = c('SL.glm', 'SL.gam', 'SL.randomForest')\n  )\n\n\nsummary(res_tmle)\n\nEstimator:  tmle \nCall:\nltmle::ltmle(data = select(data_obs, A0, L1, A1, Y), Anodes = c(\"A0\", \n    \"A1\"), Lnodes = \"L1\", Ynodes = \"Y\", abar = list(treatment = c(1, \n    1), control = c(0, 0)), SL.library = c(\"SL.glm\", \"SL.gam\", \n    \"SL.randomForest\"))\n\nTreatment Estimate:\n   Parameter Estimate:  -0.17554 \n    Estimated Std Err:  0.033541 \n              p-value:  &lt;2e-16 \n    95% Conf Interval: (-0.24128, -0.1098) \n\nControl Estimate:\n   Parameter Estimate:  -0.20361 \n    Estimated Std Err:  0.019777 \n              p-value:  &lt;2e-16 \n    95% Conf Interval: (-0.24237, -0.16485) \n\nAdditive Treatment Effect:\n   Parameter Estimate:  0.028069 \n    Estimated Std Err:  0.038937 \n              p-value:  0.47099 \n    95% Conf Interval: (-0.048247, 0.10438) \n\n\n\n\n\nltmleはメジャー（？）だが、あんまり汎用性が高いと言えず、開発も盛んなようには見えない。ほかにlongitudinal TMLEを実装しているパッケージとしてlmtpやstremrがある。lmtpはCRANにも登録してあり、試してみる価値あり。stremrはさまざまなアプローチの統合を目指す非常に意欲的なパッケージだが、開発は止まっている？っぽい。"
  },
  {
    "objectID": "01_Causal_Inference/L-TMLE.html#下準備",
    "href": "01_Causal_Inference/L-TMLE.html#下準備",
    "title": "Longitudinal TMLE",
    "section": "",
    "text": "library(tidyverse)\nlibrary(broom)\nlibrary(SuperLearner)\nlibrary(ltmle)\nlibrary(randomForest)\nlibrary(ggdag)\nlibrary(future)\n\nkable &lt;- partial(\n  knitr::kable,\n  digits = 3\n)\n\noptions(mc.cores = 8)\nplan(strategy = 'multisession')\n\nset.seed(95)\n\n\n\n\n以下のDAGを考える\n\ndagify(\n  L1 ~ A0 + U1,\n  A1 ~ A0 + L1,\n  Y ~ U1,\n  exposure = c('A0', 'A1'), \n  outcome = 'Y', \n  coords = list(x = c(A0 = 0, L1 = 1, A1 = 2, U1 = 1, Y = 3),\n                y = c(A0 = 0, L1 = 0, A1 = 0, U1 = -1, Y = 0))\n) |&gt; \n  ggdag()+\n  theme_dag()\n\n\n\n\n\n\n\n\nTreatmentはA0とA1であるが、ここからYへのpathはないので、因果効果は全ての組み合わせについて0になる\n具体的には、potential outcomeのすべての組み合わせについて、差分を取った値が0になる\n\\[\\begin{align}\n\\mathrm{E}[Y^{0, 0} - Y^{1, 0}] &= 0\\\\\n\\mathrm{E}[Y^{0, 0} - Y^{0, 1}] &= 0\\\\\n\\mathrm{E}[Y^{0, 0} - Y^{1, 1}] &= 0\\\\\n\\mathrm{E}[Y^{1, 0} - Y^{0, 1}] &= 0\\\\\n\\mathrm{E}[Y^{1, 0} - Y^{1, 1}] &= 0\\\\\n\\mathrm{E}[Y^{0, 1} - Y^{1, 1}] &= 0\n\\end{align}\\]\n\ngenerate_data &lt;- function(n){ \n    A0 &lt;- rbinom(n, size=1, prob=0.3) # binary treatment\n    U1 &lt;- rnorm(n, mean = 0, sd = 1) # latent continuous confounder\n    L1 &lt;- 0.1 + 0.3*A0 + 0.3*U1 + rnorm(n, sd = 0.1)\n    A1 &lt;- rbinom(n, size=1, prob = plogis(-0.2 + 0.3*L1 + 0.02*L1^2)) # binary treatment\n    Y &lt;- -0.2 + 0.5*U1 + rnorm(n, sd = 0.1) # continuous outcome depends on confounders\n    return(tibble(Y, A0, A1, L1, U1))\n}\n\ndata_obs &lt;- generate_data(1000)\n\n\nA0の因果効果\n\n\ndata_obs |&gt; \n  lm(Y ~ A0, data = _) |&gt; \n  tidy() |&gt; \n  kable()\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-0.207\n0.019\n-10.925\n0.000\n\n\nA0\n0.029\n0.036\n0.804\n0.422\n\n\n\n\n\n\nA1の因果効果\n\n\ndata_obs |&gt; \n  lm(Y ~ A1 + L1, data = _) |&gt; \n  tidy() |&gt; \n  kable()\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-0.427\n0.012\n-34.532\n0.000\n\n\nA1\n-0.017\n0.017\n-1.020\n0.308\n\n\nL1\n1.275\n0.025\n51.719\n0.000\n\n\n\n\n\n\ncollider bias\n\n\\(L_1\\)を条件づけると、\\(A_0 \\to L_1 \\gets U_1 \\to Y\\)というパスが開いて、バイアスをもたらす\n\ndata_obs |&gt; \n  lm(Y ~ A0 + L1, data = _) |&gt; \n  tidy() |&gt; \n  kable()\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-0.352\n0.007\n-49.626\n0\n\n\nA0\n-0.469\n0.014\n-32.796\n0\n\n\nL1\n1.540\n0.019\n81.391\n0\n\n\n\n\n\n\ndata_obs |&gt; \n  lm(Y ~ A0*A1, data = _) |&gt; \n  tidy() |&gt; \n  kable()\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-0.217\n0.026\n-8.364\n0.000\n\n\nA0\n0.043\n0.051\n0.859\n0.390\n\n\nA1\n0.021\n0.038\n0.559\n0.576\n\n\nA0:A1\n-0.031\n0.072\n-0.428\n0.669"
  },
  {
    "objectID": "01_Causal_Inference/L-TMLE.html#ipw",
    "href": "01_Causal_Inference/L-TMLE.html#ipw",
    "title": "Longitudinal TMLE",
    "section": "",
    "text": "Hernán and Robins (2020) にて紹介されている方法\n以下のウェイトを作成する\n\\[\nW^{A_0, A_1} = \\frac{1}{f(A_0 | L_0)} \\times \\frac{1}{f(A_1 | A_0, L_0, L_1)}\n\\]\n今回はベースライン共変量がない（\\(L_0 = \\varnothing\\)）ので\n\\[\nW^{A_0, A_1} = \\frac{1}{f(A_0)} \\times \\frac{1}{f(A_1 | A_0, L_1)}\n\\]\n\nres_ipw &lt;-\n  broom::augment_columns(\n    glm(A0 ~ 1, data = data_obs, family = 'binomial'), \n    data = data_obs, \n    type.predict = 'response'\n  ) |&gt; \n  # A0に対するウェイト\n  mutate(\n    ipw_A0 = case_when(\n      A0 == 1 ~ 1 / .fitted,\n      A0 == 0 ~ 1 / (1 - .fitted)\n    )\n  ) |&gt; \n  select(Y:U1, ipw_A0) |&gt; \n  broom::augment_columns(\n    glm(A1 ~ A0 + L1 + I(L1^2), data = data_obs, family = 'binomial'),\n    data = _,\n    type.predict = 'response'\n  ) |&gt; \n  mutate(\n    # A1に対するウェイト\n    ipw_A1 = case_when(\n      A1 == 1 ~ 1 / .fitted,\n      A1 == 0 ~ 1 / (1 - .fitted)\n    )\n  ) |&gt; \n  select(Y:U1, ipw_A0, ipw_A1) |&gt; \n  # A0に対するウェイトとA1に対するウェイトをかけ算\n  mutate(ipw = ipw_A0*ipw_A1) |&gt; \n  # 重み付け推定\n  summarise(CFmean = weighted.mean(Y, ipw), .by = c(A0, A1)) |&gt; \n  arrange(A0, A1)\n\nipwによる各treatment strategyにおけるcounterfactual mean\n\nres_ipw\n\n# A tibble: 4 × 3\n     A0    A1 CFmean\n  &lt;int&gt; &lt;int&gt;  &lt;dbl&gt;\n1     0     0 -0.208\n2     0     1 -0.207\n3     1     0 -0.166\n4     1     1 -0.189\n\n\ncounterfactual meanの差分\n\ncross_join(\n  res_ipw |&gt; \n    mutate(A0_A1 = str_c(A0, ',', A1)) |&gt; \n    select(A0_A1, CFmean),\n  res_ipw |&gt; \n    mutate(A0_A1 = str_c(A0, ',', A1)) |&gt; \n    select(A0_A1, CFmean)\n  ) |&gt; \n  filter(A0_A1.x != A0_A1.y) |&gt; \n  mutate(ATE = CFmean.y - CFmean.x)\n\n# A tibble: 12 × 5\n   A0_A1.x CFmean.x A0_A1.y CFmean.y       ATE\n   &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;      &lt;dbl&gt;     &lt;dbl&gt;\n 1 0,0       -0.208 0,1       -0.207  0.000654\n 2 0,0       -0.208 1,0       -0.166  0.0419  \n 3 0,0       -0.208 1,1       -0.189  0.0190  \n 4 0,1       -0.207 0,0       -0.208 -0.000654\n 5 0,1       -0.207 1,0       -0.166  0.0412  \n 6 0,1       -0.207 1,1       -0.189  0.0184  \n 7 1,0       -0.166 0,0       -0.208 -0.0419  \n 8 1,0       -0.166 0,1       -0.207 -0.0412  \n 9 1,0       -0.166 1,1       -0.189 -0.0229  \n10 1,1       -0.189 0,0       -0.208 -0.0190  \n11 1,1       -0.189 0,1       -0.207 -0.0184  \n12 1,1       -0.189 1,0       -0.166  0.0229"
  },
  {
    "objectID": "01_Causal_Inference/L-TMLE.html#longitudinal-tmle",
    "href": "01_Causal_Inference/L-TMLE.html#longitudinal-tmle",
    "title": "Longitudinal TMLE",
    "section": "",
    "text": "ltmleによる実装\n\n\nres_tmle &lt;- \n  ltmle::ltmle(\n    # データセットの列の順番が大切\n    data = data_obs |&gt; select(A0, L1, A1, Y), \n    Anodes = c('A0', 'A1'), \n    Lnodes = 'L1',\n    Ynodes = 'Y',\n    abar = list(treatment = c(1, 1), control = c(0, 0)), \n    SL.library = c('SL.glm', 'SL.gam', 'SL.randomForest')\n  )\n\n\nsummary(res_tmle)\n\nEstimator:  tmle \nCall:\nltmle::ltmle(data = select(data_obs, A0, L1, A1, Y), Anodes = c(\"A0\", \n    \"A1\"), Lnodes = \"L1\", Ynodes = \"Y\", abar = list(treatment = c(1, \n    1), control = c(0, 0)), SL.library = c(\"SL.glm\", \"SL.gam\", \n    \"SL.randomForest\"))\n\nTreatment Estimate:\n   Parameter Estimate:  -0.17554 \n    Estimated Std Err:  0.033541 \n              p-value:  &lt;2e-16 \n    95% Conf Interval: (-0.24128, -0.1098) \n\nControl Estimate:\n   Parameter Estimate:  -0.20361 \n    Estimated Std Err:  0.019777 \n              p-value:  &lt;2e-16 \n    95% Conf Interval: (-0.24237, -0.16485) \n\nAdditive Treatment Effect:\n   Parameter Estimate:  0.028069 \n    Estimated Std Err:  0.038937 \n              p-value:  0.47099 \n    95% Conf Interval: (-0.048247, 0.10438)"
  },
  {
    "objectID": "01_Causal_Inference/L-TMLE.html#他のパッケージ",
    "href": "01_Causal_Inference/L-TMLE.html#他のパッケージ",
    "title": "Longitudinal TMLE",
    "section": "",
    "text": "ltmleはメジャー（？）だが、あんまり汎用性が高いと言えず、開発も盛んなようには見えない。ほかにlongitudinal TMLEを実装しているパッケージとしてlmtpやstremrがある。lmtpはCRANにも登録してあり、試してみる価値あり。stremrはさまざまなアプローチの統合を目指す非常に意欲的なパッケージだが、開発は止まっている？っぽい。"
  },
  {
    "objectID": "01_Causal_Inference/TMLE.html",
    "href": "01_Causal_Inference/TMLE.html",
    "title": "Targeted Maximum Likelihood Estimation / Targeted Minimum Loss-based Estimation (TMLE)",
    "section": "",
    "text": "参考ページ：https://www.khstats.com/blog/tmle/tutorial-pt2\n\n\n\n\n\nlibrary(tidyverse)\nlibrary(sl3)\nlibrary(tmle3)\nlibrary(future)\n\nkable &lt;- partial(\n  knitr::kable,\n  digits = 3\n)\n\nplan(multisession, workers = 8)\n\nset.seed(7)\n\n\n\n\n\n元のサイトでは\\(Y\\)が2値変数になっていて、推定が真値と一致しているかどうかの評価が難しいので、ここでは\\(Y\\)を連続変数としている。\nATEの真値は0.3に設定\n\n\ngenerate_data &lt;- function(n){ \n    W1 &lt;- rbinom(n, size=1, prob=0.2) # binary confounder\n    W2 &lt;- rbinom(n, size=1, prob=0.5) # binary confounder\n    W3 &lt;- round(runif(n, min=2, max=7)) # continuous confounder\n    W4 &lt;- round(runif(n, min=0, max=4)) # continuous confounder\n    A  &lt;- rbinom(n, size=1, prob= plogis(-0.2 + 0.2*W2 + log(0.1*W3) + 0.3*W4 + 0.2*W1*W4)) # binary treatment depends on confounders\n    Y &lt;- -1 + 0.3*A - 0.1*W1 + 0.2*W2 + 0.3*W3 - 0.1*W4 + sin(0.1*W2*W4) # continuous outcome depends on confounders\n    return(tibble(Y, W1, W2, W3, W4, A))\n}\n\nn &lt;- 1000\ndat_obs &lt;- generate_data(n) # generate a data set with n observations\n\n\ndat_obs |&gt; \n  summarise(samplemean = mean(Y), .by = A)\n\n# A tibble: 2 × 2\n      A samplemean\n  &lt;int&gt;      &lt;dbl&gt;\n1     1      0.686\n2     0      0.237\n\n\n\n\n\n\nglm、Lasso、Random forest、Multivariate adaptive regression splineをスタッキング\n\n\nsl_libs &lt;-\n  Lrnr_sl$new(\n    learners = Stack$new(\n      Lrnr_glm$new(), \n      Lrnr_glmnet$new(alpha = 1), \n      Lrnr_ranger$new(num.trees = 2000, max.depth = 3), \n      Lrnr_earth$new()\n    )\n  )\n\n\n\n\n\n\n\n\\[\n{\\mathrm E}[Y | A, W]\n\\]\n\ntask &lt;- \n  sl3_Task$new(\n    dat_obs, \n    covariates = select(dat_obs, !Y) |&gt; names(), \n    outcome = 'Y', \n    outcome_type = 'continuous', \n    folds = 8L\n  )\n\ntask_A1 &lt;- \n  sl3_Task$new(\n    dat_obs |&gt; mutate(A = 1), \n    covariates = select(dat_obs, !Y) |&gt; names(), \n    outcome = 'Y', \n    outcome_type = 'continuous', \n    folds = 8L\n  )\n\ntask_A0 &lt;- \n  sl3_Task$new(\n    dat_obs |&gt; mutate(A = 0), \n    covariates = select(dat_obs, !Y) |&gt; names(), \n    outcome = 'Y', \n    outcome_type = 'continuous', \n    folds = 8L\n  )\n\n# 全サンプルで学習\nsl_fit &lt;- sl_libs$train(task)\n\n\n以下の3つの予測値を算出\n\\(A := 1\\)は全サンプルでAを1にする（\\(A = 1\\)はサンプルのうちのAが1となる部分集団）\n\n\\[\\begin{align}\n&{\\mathrm E}[Y | A, W] \\\\\n&{\\mathrm E}[Y | A := 1, W] \\\\\n&{\\mathrm E}[Y | A := 0, W]\n\\end{align}\\]\n\ndat_tmle1 &lt;- \n  dat_obs |&gt; \n  mutate(\n    # 観測サンプルについての予測値\n    Q_A = sl_fit$predict(task),\n    # 全てのサンプルでA = 1に固定したときの予測値\n    Q_A1 = sl_fit$predict(task_A1),\n    # 全てのサンプルでA = 0に固定した時の予測値\n    Q_A0 = sl_fit$predict(task_A0)\n  )\n\n\nstandardization（g-computation）によるATE\n\n\\[\nATE_{g \\mathrm{-}comp} = {\\mathrm E}[ {\\mathrm E}[Y | A := 1, W] - {\\mathrm E}[Y | A := 0, W]]\n\\]\n\ndat_tmle1 |&gt; \n  summarise(ATE = mean(Q_A1 - Q_A0))\n\n# A tibble: 1 × 1\n    ATE\n  &lt;dbl&gt;\n1 0.300\n\n\n\n\n\n\n\n傾向スコアを機械学習モデルにより予測\n\n\\[\n\\mathrm{Pr}(A = 1 | W)\n\\]\n\ntask_g &lt;- \n  sl3_Task$new(\n    data = dat_obs, \n    covariates = select(dat_obs, !c(Y, A)) |&gt; names(), \n    outcome = 'A', \n    outcome_type = 'binomial', \n    folds = 8\n  )\n\nsl_fit_g &lt;- sl_libs$train(task_g)\n\n\n\n\n傾向スコアからClever Covariateと呼ばれる情報を作成（IPWに似ている）\n\n\\[\\begin{align}\n&H(A,W) &= \\frac{A}{\\mathrm{Pr}(A = 1 | W)} - \\frac{1 - A}{1 - \\mathrm{Pr}(A = 1 | W)} \\\\\n&H(1,W) &= \\frac{A}{\\mathrm{Pr}(A = 1 | W)} \\\\\n&H(0,W) &= - \\frac{1 - A}{1 - \\mathrm{Pr}(A = 1 | W)}\n\\end{align}\\]\n\ndat_tmle2 &lt;- \n  dat_tmle1 |&gt; \n  mutate(\n    # Propensity Scoreの予測\n    ps = sl_fit_g$predict(task_g),\n    # ipw (Inverse Probability Weight)\n    ipw = case_when(\n      A == 1 ~ 1 / ps, \n      A == 0 ~ 1 / (1 - ps)\n    ),\n    # Clever Covariates\n    H_A = case_when(\n      A == 1 ~ 1 / ps,\n      A == 0 ~ -1 / (1 - ps)\n    ),\n    H_A1 = case_when(\n      A == 1 ~ H_A,\n      A == 0 ~ 0\n    ),\n    H_A0 = case_when(\n      A == 1 ~ 0,\n      A == 0 ~ H_A\n    )\n  )\n\n\nIPWによるATE\n\n\\[\nATE_{ipw} = {\\mathrm E}[\\frac{A}{\\mathrm{Pr}(A = 1 | W)}Y - \\frac{1 - A}{1 - \\mathrm{Pr}(A = 1 | W)}Y]\n\\]\n\ndat_tmle2 |&gt; \n  summarise(CFmean = sum(Y*ipw) / sum(ipw), .by = A) |&gt; \n  arrange(A) |&gt; \n  summarise(ATE = diff(CFmean))\n\n# A tibble: 1 × 1\n    ATE\n  &lt;dbl&gt;\n1 0.306\n\n\n\nAugumented IPWによるATE\n\n（ほんとは関数推定時にcross-fitをする）\n\n\n\\[\nATE_{aipw} = \\mathrm E[\\mathrm E[Y | A := 1, W] - \\mathrm E[Y | A := 0, W] + \\frac{A}{\\mathrm Pr(A = 1 | W)}(Y - {\\mathrm E}[Y | A := 1, W]) - \\frac{1 - A}{1 - \\mathrm Pr(A = 1 | W)}(Y - \\mathrm E[Y | A := 0, W])]\n\\]\n\ndat_tmle2 |&gt; \n  summarise(ATE = mean(Q_A1 - Q_A0 + ipw*A*(Y - Q_A1) - ipw*(1 - A)*(Y - Q_A0)))\n\n# A tibble: 1 × 1\n    ATE\n  &lt;dbl&gt;\n1 0.300\n\n\n\n\n\n\n\nAIPWの問題点：統計的最適化がターゲットのパラメータ（ATE）に対してではなく、母平均関数\\({\\mathrm E}[Y | A,W]\\)および傾向スコア関数\\(\\mathrm{Pr}(A = 1 | W)\\)のパラメータについて最適化されている点\n推定したいパラメータ（ATE）のEIF（Efficient Influence Function）を解くことがこのステップのポイントらしい\n具体的には、Step1で推定した\\(\\mathrm{E}[Y | A, W]\\)と、Step2で推定したClever Covariate\\(H(A, W)\\)を用いて、以下の回帰式の\\(\\epsilon\\)（変動パラメータ）を推定する\n\n\\[\nY = \\mathrm{E}[Y | A, W] + \\epsilon H(A,W)\n\\]\n\n切片が0で、Step1の推定値の係数を1に固定するために、-1とoffsetを利用する\n\n\nfit &lt;- glm(Y ~ -1 + offset(Q_A) + H_A, data = dat_tmle2, family = gaussian())\n\n\n変動パラメータの推定値\n\n\nepsilon &lt;- coef(fit)\n\nepsilon\n\n         H_A \n1.386454e-06 \n\n\n\n\n\n\n推定したepsilonと\\(Y\\)の予測値をもとに、\\(Y\\)の予測値を更新\n\n\ndat_tmle3 &lt;- \n  dat_tmle2 |&gt; \n  mutate(\n    Q_A_update = Q_A + epsilon*H_A,\n    Q_A1_update = Q_A1 + epsilon*H_A1,\n    Q_A0_update = Q_A0 + epsilon*H_A0,\n  )\n\n\n\n\n\n更新されたアウトカムの予測値を用いて、Standardizationの要領でATEを推定\n\n\ndat_tmle3 |&gt; \n  summarise(ATE = mean(Q_A1_update - Q_A0_update))\n\n# A tibble: 1 × 1\n    ATE\n  &lt;dbl&gt;\n1 0.300\n\nATE &lt;- mean(dat_tmle3$Q_A1_update - dat_tmle3$Q_A0_update)\n\n\n\n\n\nTMLEではbootstrapによらずとも標準誤差を算出できる（！）\nまずは、Influence Functionを推定する\nInfluence Function：各サンプルがどれだけATEに影響をあたえるか？\n\n\ndat_tmle4 &lt;- \n  dat_tmle3 |&gt; \n  mutate(\n    IF = (Y - Q_A_update)*H_A + Q_A1_update - Q_A0_update - ATE\n  )\n\nATEの標準誤差はIFを用いて\n\\[\nSE = \\sqrt{\\frac{\\mathrm{var}(IF)}{N}}\n\\]\n\ndat_tmle4 |&gt; \n  summarise(SE = sqrt(var(IF) / 1000))\n\n# A tibble: 1 × 1\n         SE\n      &lt;dbl&gt;\n1 0.0000215"
  },
  {
    "objectID": "01_Causal_Inference/TMLE.html#下準備",
    "href": "01_Causal_Inference/TMLE.html#下準備",
    "title": "Targeted Maximum Likelihood Estimation / Targeted Minimum Loss-based Estimation (TMLE)",
    "section": "",
    "text": "library(tidyverse)\nlibrary(sl3)\nlibrary(tmle3)\nlibrary(future)\n\nkable &lt;- partial(\n  knitr::kable,\n  digits = 3\n)\n\nplan(multisession, workers = 8)\n\nset.seed(7)\n\n\n\n\n\n元のサイトでは\\(Y\\)が2値変数になっていて、推定が真値と一致しているかどうかの評価が難しいので、ここでは\\(Y\\)を連続変数としている。\nATEの真値は0.3に設定\n\n\ngenerate_data &lt;- function(n){ \n    W1 &lt;- rbinom(n, size=1, prob=0.2) # binary confounder\n    W2 &lt;- rbinom(n, size=1, prob=0.5) # binary confounder\n    W3 &lt;- round(runif(n, min=2, max=7)) # continuous confounder\n    W4 &lt;- round(runif(n, min=0, max=4)) # continuous confounder\n    A  &lt;- rbinom(n, size=1, prob= plogis(-0.2 + 0.2*W2 + log(0.1*W3) + 0.3*W4 + 0.2*W1*W4)) # binary treatment depends on confounders\n    Y &lt;- -1 + 0.3*A - 0.1*W1 + 0.2*W2 + 0.3*W3 - 0.1*W4 + sin(0.1*W2*W4) # continuous outcome depends on confounders\n    return(tibble(Y, W1, W2, W3, W4, A))\n}\n\nn &lt;- 1000\ndat_obs &lt;- generate_data(n) # generate a data set with n observations\n\n\ndat_obs |&gt; \n  summarise(samplemean = mean(Y), .by = A)\n\n# A tibble: 2 × 2\n      A samplemean\n  &lt;int&gt;      &lt;dbl&gt;\n1     1      0.686\n2     0      0.237\n\n\n\n\n\n\nglm、Lasso、Random forest、Multivariate adaptive regression splineをスタッキング\n\n\nsl_libs &lt;-\n  Lrnr_sl$new(\n    learners = Stack$new(\n      Lrnr_glm$new(), \n      Lrnr_glmnet$new(alpha = 1), \n      Lrnr_ranger$new(num.trees = 2000, max.depth = 3), \n      Lrnr_earth$new()\n    )\n  )"
  },
  {
    "objectID": "01_Causal_Inference/TMLE.html#step1アウトカムの予測",
    "href": "01_Causal_Inference/TMLE.html#step1アウトカムの予測",
    "title": "Targeted Maximum Likelihood Estimation / Targeted Minimum Loss-based Estimation (TMLE)",
    "section": "",
    "text": "\\[\n{\\mathrm E}[Y | A, W]\n\\]\n\ntask &lt;- \n  sl3_Task$new(\n    dat_obs, \n    covariates = select(dat_obs, !Y) |&gt; names(), \n    outcome = 'Y', \n    outcome_type = 'continuous', \n    folds = 8L\n  )\n\ntask_A1 &lt;- \n  sl3_Task$new(\n    dat_obs |&gt; mutate(A = 1), \n    covariates = select(dat_obs, !Y) |&gt; names(), \n    outcome = 'Y', \n    outcome_type = 'continuous', \n    folds = 8L\n  )\n\ntask_A0 &lt;- \n  sl3_Task$new(\n    dat_obs |&gt; mutate(A = 0), \n    covariates = select(dat_obs, !Y) |&gt; names(), \n    outcome = 'Y', \n    outcome_type = 'continuous', \n    folds = 8L\n  )\n\n# 全サンプルで学習\nsl_fit &lt;- sl_libs$train(task)\n\n\n以下の3つの予測値を算出\n\\(A := 1\\)は全サンプルでAを1にする（\\(A = 1\\)はサンプルのうちのAが1となる部分集団）\n\n\\[\\begin{align}\n&{\\mathrm E}[Y | A, W] \\\\\n&{\\mathrm E}[Y | A := 1, W] \\\\\n&{\\mathrm E}[Y | A := 0, W]\n\\end{align}\\]\n\ndat_tmle1 &lt;- \n  dat_obs |&gt; \n  mutate(\n    # 観測サンプルについての予測値\n    Q_A = sl_fit$predict(task),\n    # 全てのサンプルでA = 1に固定したときの予測値\n    Q_A1 = sl_fit$predict(task_A1),\n    # 全てのサンプルでA = 0に固定した時の予測値\n    Q_A0 = sl_fit$predict(task_A0)\n  )\n\n\nstandardization（g-computation）によるATE\n\n\\[\nATE_{g \\mathrm{-}comp} = {\\mathrm E}[ {\\mathrm E}[Y | A := 1, W] - {\\mathrm E}[Y | A := 0, W]]\n\\]\n\ndat_tmle1 |&gt; \n  summarise(ATE = mean(Q_A1 - Q_A0))\n\n# A tibble: 1 × 1\n    ATE\n  &lt;dbl&gt;\n1 0.300"
  },
  {
    "objectID": "01_Causal_Inference/TMLE.html#step2処置確率傾向スコアの予測",
    "href": "01_Causal_Inference/TMLE.html#step2処置確率傾向スコアの予測",
    "title": "Targeted Maximum Likelihood Estimation / Targeted Minimum Loss-based Estimation (TMLE)",
    "section": "",
    "text": "傾向スコアを機械学習モデルにより予測\n\n\\[\n\\mathrm{Pr}(A = 1 | W)\n\\]\n\ntask_g &lt;- \n  sl3_Task$new(\n    data = dat_obs, \n    covariates = select(dat_obs, !c(Y, A)) |&gt; names(), \n    outcome = 'A', \n    outcome_type = 'binomial', \n    folds = 8\n  )\n\nsl_fit_g &lt;- sl_libs$train(task_g)\n\n\n\n\n傾向スコアからClever Covariateと呼ばれる情報を作成（IPWに似ている）\n\n\\[\\begin{align}\n&H(A,W) &= \\frac{A}{\\mathrm{Pr}(A = 1 | W)} - \\frac{1 - A}{1 - \\mathrm{Pr}(A = 1 | W)} \\\\\n&H(1,W) &= \\frac{A}{\\mathrm{Pr}(A = 1 | W)} \\\\\n&H(0,W) &= - \\frac{1 - A}{1 - \\mathrm{Pr}(A = 1 | W)}\n\\end{align}\\]\n\ndat_tmle2 &lt;- \n  dat_tmle1 |&gt; \n  mutate(\n    # Propensity Scoreの予測\n    ps = sl_fit_g$predict(task_g),\n    # ipw (Inverse Probability Weight)\n    ipw = case_when(\n      A == 1 ~ 1 / ps, \n      A == 0 ~ 1 / (1 - ps)\n    ),\n    # Clever Covariates\n    H_A = case_when(\n      A == 1 ~ 1 / ps,\n      A == 0 ~ -1 / (1 - ps)\n    ),\n    H_A1 = case_when(\n      A == 1 ~ H_A,\n      A == 0 ~ 0\n    ),\n    H_A0 = case_when(\n      A == 1 ~ 0,\n      A == 0 ~ H_A\n    )\n  )\n\n\nIPWによるATE\n\n\\[\nATE_{ipw} = {\\mathrm E}[\\frac{A}{\\mathrm{Pr}(A = 1 | W)}Y - \\frac{1 - A}{1 - \\mathrm{Pr}(A = 1 | W)}Y]\n\\]\n\ndat_tmle2 |&gt; \n  summarise(CFmean = sum(Y*ipw) / sum(ipw), .by = A) |&gt; \n  arrange(A) |&gt; \n  summarise(ATE = diff(CFmean))\n\n# A tibble: 1 × 1\n    ATE\n  &lt;dbl&gt;\n1 0.306\n\n\n\nAugumented IPWによるATE\n\n（ほんとは関数推定時にcross-fitをする）\n\n\n\\[\nATE_{aipw} = \\mathrm E[\\mathrm E[Y | A := 1, W] - \\mathrm E[Y | A := 0, W] + \\frac{A}{\\mathrm Pr(A = 1 | W)}(Y - {\\mathrm E}[Y | A := 1, W]) - \\frac{1 - A}{1 - \\mathrm Pr(A = 1 | W)}(Y - \\mathrm E[Y | A := 0, W])]\n\\]\n\ndat_tmle2 |&gt; \n  summarise(ATE = mean(Q_A1 - Q_A0 + ipw*A*(Y - Q_A1) - ipw*(1 - A)*(Y - Q_A0)))\n\n# A tibble: 1 × 1\n    ATE\n  &lt;dbl&gt;\n1 0.300"
  },
  {
    "objectID": "01_Causal_Inference/TMLE.html#step3変動パラメータの推定",
    "href": "01_Causal_Inference/TMLE.html#step3変動パラメータの推定",
    "title": "Targeted Maximum Likelihood Estimation / Targeted Minimum Loss-based Estimation (TMLE)",
    "section": "",
    "text": "AIPWの問題点：統計的最適化がターゲットのパラメータ（ATE）に対してではなく、母平均関数\\({\\mathrm E}[Y | A,W]\\)および傾向スコア関数\\(\\mathrm{Pr}(A = 1 | W)\\)のパラメータについて最適化されている点\n推定したいパラメータ（ATE）のEIF（Efficient Influence Function）を解くことがこのステップのポイントらしい\n具体的には、Step1で推定した\\(\\mathrm{E}[Y | A, W]\\)と、Step2で推定したClever Covariate\\(H(A, W)\\)を用いて、以下の回帰式の\\(\\epsilon\\)（変動パラメータ）を推定する\n\n\\[\nY = \\mathrm{E}[Y | A, W] + \\epsilon H(A,W)\n\\]\n\n切片が0で、Step1の推定値の係数を1に固定するために、-1とoffsetを利用する\n\n\nfit &lt;- glm(Y ~ -1 + offset(Q_A) + H_A, data = dat_tmle2, family = gaussian())\n\n\n変動パラメータの推定値\n\n\nepsilon &lt;- coef(fit)\n\nepsilon\n\n         H_A \n1.386454e-06"
  },
  {
    "objectID": "01_Causal_Inference/TMLE.html#step4アウトカムの予測値を更新",
    "href": "01_Causal_Inference/TMLE.html#step4アウトカムの予測値を更新",
    "title": "Targeted Maximum Likelihood Estimation / Targeted Minimum Loss-based Estimation (TMLE)",
    "section": "",
    "text": "推定したepsilonと\\(Y\\)の予測値をもとに、\\(Y\\)の予測値を更新\n\n\ndat_tmle3 &lt;- \n  dat_tmle2 |&gt; \n  mutate(\n    Q_A_update = Q_A + epsilon*H_A,\n    Q_A1_update = Q_A1 + epsilon*H_A1,\n    Q_A0_update = Q_A0 + epsilon*H_A0,\n  )"
  },
  {
    "objectID": "01_Causal_Inference/TMLE.html#step5推定したい統計量を推定",
    "href": "01_Causal_Inference/TMLE.html#step5推定したい統計量を推定",
    "title": "Targeted Maximum Likelihood Estimation / Targeted Minimum Loss-based Estimation (TMLE)",
    "section": "",
    "text": "更新されたアウトカムの予測値を用いて、Standardizationの要領でATEを推定\n\n\ndat_tmle3 |&gt; \n  summarise(ATE = mean(Q_A1_update - Q_A0_update))\n\n# A tibble: 1 × 1\n    ATE\n  &lt;dbl&gt;\n1 0.300\n\nATE &lt;- mean(dat_tmle3$Q_A1_update - dat_tmle3$Q_A0_update)"
  },
  {
    "objectID": "01_Causal_Inference/TMLE.html#step6標準誤差の推定",
    "href": "01_Causal_Inference/TMLE.html#step6標準誤差の推定",
    "title": "Targeted Maximum Likelihood Estimation / Targeted Minimum Loss-based Estimation (TMLE)",
    "section": "",
    "text": "TMLEではbootstrapによらずとも標準誤差を算出できる（！）\nまずは、Influence Functionを推定する\nInfluence Function：各サンプルがどれだけATEに影響をあたえるか？\n\n\ndat_tmle4 &lt;- \n  dat_tmle3 |&gt; \n  mutate(\n    IF = (Y - Q_A_update)*H_A + Q_A1_update - Q_A0_update - ATE\n  )\n\nATEの標準誤差はIFを用いて\n\\[\nSE = \\sqrt{\\frac{\\mathrm{var}(IF)}{N}}\n\\]\n\ndat_tmle4 |&gt; \n  summarise(SE = sqrt(var(IF) / 1000))\n\n# A tibble: 1 × 1\n         SE\n      &lt;dbl&gt;\n1 0.0000215"
  }
]