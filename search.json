[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "鎌田 健太郎（Kentaro Kamada）",
    "section": "",
    "text": "東京大学大学院教育学研究科比較教育社会学コース 博士課程・日本学術振興会 特別研究員（DC1）"
  },
  {
    "objectID": "blog/posts/度数から対数尤度を計算.html",
    "href": "blog/posts/度数から対数尤度を計算.html",
    "title": "観測度数・期待度数から対数尤度を計算する方法",
    "section": "",
    "text": "\\(f_{ij}\\)：観測度数\n\n\n\n\n\nB\nB’\n\n\n\n\nA\n\\(f_{11}\\)\n\\(f_{12}\\)\n\n\nA’\n\\(f_{21}\\)\n\\(f_{22}\\)\n\n\n\n\n\\(p_{ij}\\)：セル確率\n\nモデルから計算されたセルの確率\nAとBに関連がないモデルにおいては、期待度数をサンプルサイズで割ったもの\n\n\n\n\n\n\nB\nB’\n\n\n\n\nA\n\\(p_{11}\\)\n\\(p_{12}\\)\n\n\nA’\n\\(p_{21}\\)\n\\(p_{22}\\)\n\n\n\n尤度関数を\\(L\\)とすると\n\\[\nL = \\prod_i \\prod_j p_{ij}^{f_{ij}} =  p_{11}^{f_{11}} \\times p_{12}^{f_{12}} \\times p_{21}^{f_{21}} \\times p_{22}^{f_{22}}\n\\]\n各セルの確率を(観測度数)乗したものを全部掛け算\n対数尤度はこれの対数を取ったもの\n\\[\n\\log L = \\sum_i \\sum_j f_{ij} \\log p_{ij} = f_{11} \\log p_{11} + f_{12} \\log p_{12} + f_{21} \\log p_{21} + f_{22} \\log p_{22}\n\\]\n掛け算が足し算になるので計算しやすい"
  },
  {
    "objectID": "blog/posts/度数から対数尤度を計算.html#観測度数とセル確率から対数尤度を計算",
    "href": "blog/posts/度数から対数尤度を計算.html#観測度数とセル確率から対数尤度を計算",
    "title": "観測度数・期待度数から対数尤度を計算する方法",
    "section": "",
    "text": "\\(f_{ij}\\)：観測度数\n\n\n\n\n\nB\nB’\n\n\n\n\nA\n\\(f_{11}\\)\n\\(f_{12}\\)\n\n\nA’\n\\(f_{21}\\)\n\\(f_{22}\\)\n\n\n\n\n\\(p_{ij}\\)：セル確率\n\nモデルから計算されたセルの確率\nAとBに関連がないモデルにおいては、期待度数をサンプルサイズで割ったもの\n\n\n\n\n\n\nB\nB’\n\n\n\n\nA\n\\(p_{11}\\)\n\\(p_{12}\\)\n\n\nA’\n\\(p_{21}\\)\n\\(p_{22}\\)\n\n\n\n尤度関数を\\(L\\)とすると\n\\[\nL = \\prod_i \\prod_j p_{ij}^{f_{ij}} =  p_{11}^{f_{11}} \\times p_{12}^{f_{12}} \\times p_{21}^{f_{21}} \\times p_{22}^{f_{22}}\n\\]\n各セルの確率を(観測度数)乗したものを全部掛け算\n対数尤度はこれの対数を取ったもの\n\\[\n\\log L = \\sum_i \\sum_j f_{ij} \\log p_{ij} = f_{11} \\log p_{11} + f_{12} \\log p_{12} + f_{21} \\log p_{21} + f_{22} \\log p_{22}\n\\]\n掛け算が足し算になるので計算しやすい"
  },
  {
    "objectID": "blog/posts/度数から対数尤度を計算.html#尤度比カイ2乗値との関係",
    "href": "blog/posts/度数から対数尤度を計算.html#尤度比カイ2乗値との関係",
    "title": "観測度数・期待度数から対数尤度を計算する方法",
    "section": "尤度比カイ2乗値との関係",
    "text": "尤度比カイ2乗値との関係\n尤度比カイ2乗値（\\(G^2\\)）と観測度数（\\(f\\)）・期待度数（\\(F\\)）の関係は以下の式で表される(Hout 1983; McCutcheon 1987)\n\\[\nG^2 = 2 \\sum_i \\sum_j f_{ij} \\log \\frac{f_{ij}}{F_{ij}}\n\\]\nなぜこうなるのか？対数尤度の比から考えてみる。\nまず、AとBに関連がないというモデル（独立モデル）のもとで対数尤度を計算する。この条件ではセル確率\\(p_{ij}\\)は期待度数\\(F_{ij}\\)をサンプルサイズ\\(N\\)で割ったものになる。\n\\[\n\\log L_1 = \\sum_i \\sum_j f_{ij} \\log p_{ij} = \\sum_i \\sum_j f_{ij} \\log \\frac{F_{ij}}{N} = \\sum_i \\sum_j f_{ij} \\log F_{ij} - \\sum_i \\sum_j f_{ij} \\log N\n\\]\n次に、AとBに関連があるというモデル（飽和モデル）のもとで対数尤度を計算する。この条件ではセル確率\\(p_{ij}\\)は観測度数\\(f_{ij}\\)をサンプルサイズ\\(N\\)で割ったものになる。\n\\[\n\\log L_2 = \\sum_i \\sum_j f_{ij} \\log p_{ij} = \\sum_i \\sum_j f_{ij} \\log \\frac{f_{ij}}{N} = \\sum_i \\sum_j f_{ij} \\log f_{ij} - \\sum_i \\sum_j f_{ij} \\log N\n\\]\n対数尤度の差（＝尤度の比）を計算すると\n\\[\\begin{align}\n\\log L_2 - \\log L_1 &= \\sum_i \\sum_j f_{ij} \\log f_{ij} - \\sum_i \\sum_j f_{ij} \\log F_{ij} \\\\\n\\log \\frac{L_2}{L_1} &= \\sum_i \\sum_j (f_{ij} \\log f_{ij} - f_{ij} \\log F_{ij}) \\\\\n\\log \\frac{L_2}{L_1} &= \\sum_i \\sum_j f_{ij} \\log \\frac{f_{ij}}{F_{ij}} \\\\\n\\end{align}\\]\nそれっぽい形になった。\nさらに両辺を2倍すると\n\\[\n2 \\log \\frac{L_2}{L_1} = \\log (\\frac{L_2}{L_1})^2 = 2 \\sum_i \\sum_j f_{ij} \\log \\frac{f_{ij}}{F_{ij}}\n\\]\nしたがって尤度比カイ二乗値は、尤度比の2乗の対数をとったものである。\nあるいは、以下のように変形すればdeviance（-2対数尤度）の差としても表現できる。\n\\[\n2 \\log \\frac{L_2}{L_1} = (-2 \\log L_1) - (- 2 \\log L_2)\n\\]"
  },
  {
    "objectID": "blog/posts/Zoteroの設定備忘録.html",
    "href": "blog/posts/Zoteroの設定備忘録.html",
    "title": "Zoteroの設定備忘録",
    "section": "",
    "text": "先日ようやく文献管理をはじめた．\n社会学周りだと，ここらへんのメソッドが全く確立されていない&共有されないので，試行錯誤が大変だった．\n以下では自分なりに見つけた方法をメモっていく．（メモらないと絶対忘れる）"
  },
  {
    "objectID": "blog/posts/Zoteroの設定備忘録.html#ようやく始めた文献管理",
    "href": "blog/posts/Zoteroの設定備忘録.html#ようやく始めた文献管理",
    "title": "Zoteroの設定備忘録",
    "section": "",
    "text": "先日ようやく文献管理をはじめた．\n社会学周りだと，ここらへんのメソッドが全く確立されていない&共有されないので，試行錯誤が大変だった．\n以下では自分なりに見つけた方法をメモっていく．（メモらないと絶対忘れる）"
  },
  {
    "objectID": "blog/posts/Zoteroの設定備忘録.html#zoteroのインストール",
    "href": "blog/posts/Zoteroの設定備忘録.html#zoteroのインストール",
    "title": "Zoteroの設定備忘録",
    "section": "Zoteroのインストール",
    "text": "Zoteroのインストール\nなにはともあれZotero本体のインストール．\n「zotero install」みたいな感じで適当にググったら出てくる． https://www.zotero.org/download/\nインストール時の注意とかも特になし．サクサク進める．"
  },
  {
    "objectID": "blog/posts/Zoteroの設定備忘録.html#ブラウザコネクタのインストール",
    "href": "blog/posts/Zoteroの設定備忘録.html#ブラウザコネクタのインストール",
    "title": "Zoteroの設定備忘録",
    "section": "ブラウザコネクタのインストール",
    "text": "ブラウザコネクタのインストール\nZoteroを起動して「ツール」&gt;「ブラウザ・コネクタのインストール」をクリック．\nするとウェブブラウザが立ち上がるので，自分が使うブラウザのコネクタをインストールする．\nこれも作りが丁寧でわかりやすいのでサクサク進める．"
  },
  {
    "objectID": "blog/posts/Zoteroの設定備忘録.html#はじめにやっとく設定",
    "href": "blog/posts/Zoteroの設定備忘録.html#はじめにやっとく設定",
    "title": "Zoteroの設定備忘録",
    "section": "はじめにやっとく設定",
    "text": "はじめにやっとく設定\n「編集」&gt;「設定」からやっておいた方がいい設定をいじる（Macだと「Zotero」&gt;「環境設定」）\n\n一般\n\n「ウェブページからアイテムを作成するときに自動的にスナップショットを作成する」のチェックを外す\nゴミ箱の中のアイテムを7日（お好きなように）経過で削除\n\n同期\n\nデータの同期，作ったアカウントでログインしておく\nファイルの同期，すべてチェックを外す（pdfはOnedriveに保存するので使わない）\n\n詳細\n\nファイルとフォルダ，リンク付き添付ファイルの基本ディレクトリを，pdfを保存したいディレクトリに設定する（自分の場合はOnedrive&gt;Zotero）"
  },
  {
    "objectID": "blog/posts/Zoteroの設定備忘録.html#アドオンのインストール",
    "href": "blog/posts/Zoteroの設定備忘録.html#アドオンのインストール",
    "title": "Zoteroの設定備忘録",
    "section": "アドオンのインストール",
    "text": "アドオンのインストール\nZoteroはオープンソースなので，便利なアドオンをいろんな人が作ってくれている．\n以下ではZotFileとBetter-BibTexの二つを導入する．\n\nZotFileの導入（Zotero 7では使えないので注意！）\n\nまずはアドオンをダウンロードしてくる．http://zotfile.com/index.html#changelog\nxpi形式のファイルがダウンロードされるので，適当なフォルダに保存する（自分はProgram FilesのZoteroのフォルダにAddonというフォルダを作って入れている）．\n\nZoteroを起動し，「ツール」&gt;「アドオン」をクリックすると現在インストールされているアドオンのリストが表示されるので，右上の歯車アイコンで「Install Add-on from File…」をクリック\n\n先ほど保存したxpi形式のファイルを選択．Zoteroを再起動しろと言われるので，Restart nowをクリック\n\nこれでインストールは完了．\n\npdfの保存先を設定する\ngoogle scholarやjstageなどから，ブラウザコネクタを使って文献を落としてきたときに，pdfファイルを保存するフォルダを任意に設定できる．自分はOnedrive内のフォルダを指定することでiPadなどから文献にアクセスできるようにしている．\n\n「ツール」&gt;「ZotFile Preferences…」&gt;「General Settings」&gt;「Location Files」と進む\n\nCustom Locationの横のChoose…を選択し，pdfを保存したいディレクトリを選択する\n\nUse subfolder defined byにチェックを入れ，横に「/%c」と入力する\n\n最後の行はフォルダ内にサブフォルダを作るかどうかで，「/%c」はZoteroのコレクションに対応する形でサブフォルダを作る設定．もちろん著者（「/%a」）や年度（「/%y」）でサブフォルダを作ることもできる．\n\n\npdfファイルの名前を設定する\n同じくブラウザコネクタ経由で文献を落としてきたときに，pdfファイルの命名規則を定めることができる．\n\n「ツール」&gt;「ZotFile Preferences…」&gt;「Renaming Rules」&gt;「Renaming Format」と進む\n\nUse Zotero to Renameのチェックを外す\n\nFormat for all Item Types except Patentのところに「{%a_}{%y_}{%t}」と入力する\n\n最後のところが命名規則で，「著者_年度_タイトル」という名前がpdfファイルにつくことになる．ここも先ほどと同じで任意に規則を作ることができる．\n命名規則を設定してから，文献を右クリックして「Manage Attachments」&gt;「Rename Attachments」とすると，pdfファイルがリネームされる．\n\n\n\nBetter BibTeXの導入\n導入の流れは先ほどと同じ． ダウンロード元はGithubのreleaseから． https://github.com/retorquere/zotero-better-bibtex/releases\n\nCite Keyの自動生成\nBibtexで引用する際に使用するCitekeyをいい感じで自動生成してくれる．\n\n「編集」&gt; 「設定」&gt;「Better BibTeX」&gt;「Citation keys」とすすむ\n「Citation key format」のところに「[Auth]-[year]」と入力\nとなりの「Export」に移って，「Fields to omit from export」のところに「abstract, doi, file」と入力\n「Advanced」に移って，「Ideographs in citekeys」の「Apply kuroshiro romajization in Japanese names/titles」のところにチェックをいれる\n\n2のところがCitation keyの命名規則で，「著者-年」という形で生成される．もちろんTitleなどを組み合わせてカスタマイズできる． 3はBibtexファイルの出力時にabstractやdoiを出力しないという設定（ファイルが長くなって邪魔くさいので…）． 4は著者名に日本語が含まれている場合に読みを抽出してローマ字化し，Citation keyに利用するという設定．完全に正確ではないが，今のところは不自由なく使えていると思う．"
  },
  {
    "objectID": "blog/posts/Zoteroの設定備忘録.html#j-stageのtranslatorの修正",
    "href": "blog/posts/Zoteroの設定備忘録.html#j-stageのtranslatorの修正",
    "title": "Zoteroの設定備忘録",
    "section": "J-StageのTranslatorの修正",
    "text": "J-StageのTranslatorの修正\n社会学だと主要な邦ジャーナルはjstageからダウンロードすることになるが，そのままだと著者名とタイトルがうまく取れないという問題がある．\n\n著者名：姓と名が反対でとれてしまう．これが連鎖的にpdfのファイル名にまで影響してくる\nタイトル：サブタイトルがとれない．このままだと手動入力することになる．\n\nこれを，JstageのTranslatorを修正することで対応する．\nZoteroのデータフォルダ（Windowsの場合Users）内のTranslatorsの中にある「J-Stage.js」を開く．\n87行目あたりから以下のように修正する．\n// get RIS Link\n    //var bibtexurl = ZU.xpathText(doc, '//a[contains(text(), \"BIB TEX\")]/@href'); // コメントアウト\n    var risurl = ZU.xpathText(doc, '//a[contains(text(), \"RIS\")]/@href'); // 追加\n    // ZU.doGet(bibtexurl, function (text) { // コメントアウト\n    ZU.doGet(risurl, function (text) { //追加\n        // var bibtex = text; // コメントアウト\n        if (text.match(/TI  - .+?\\nTI  -  \\n/)) { // 追加\n            var bibtex = text.replace(\"TI  - \",\"TIT  - \").replace(\"\\nTI  -  \\n\",\"\\n\").replace(\"TIT  - \",\"TI  - \"); // 追加\n        } else { // 追加\n            var bibtex = text.replace(\"TI  - \",\"TIT  - \").replace(\"\\nTI  - \",\"――\").replace(\"TIT  - \",\"TI  - \"); // 追加\n        } // 追加\n        // Zotero.debug(bibtex)\n        var translator = Zotero.loadTranslator(\"import\");\n        // translator.setTranslator(\"9cb70025-a888-4a29-a210-93ec52da40d4\"); // コメントアウト\n        translator.setTranslator(\"32d59d2d-b65a-4da4-b0a3-bdd3cfb979e7\");  // 追加\n何をしているかを簡単に解説すると，まずjstageから情報を取ってくるときのソースをBibtexからRISに変更している．こうすることでBibtexに適用される日本人著者のオリジナルルールを回避する．\nまたjstageが提供するRISファイルの中身を見るとタイトルを保存するフィールド（TI）が二つ用意されていることがわかるので，二つとも埋まっている（サブタイトルがある）ときはそれを「――」でつないで結合している（7～11行目）"
  },
  {
    "objectID": "blog/posts/Zoteroの設定備忘録.html#社会学評論のcslファイル",
    "href": "blog/posts/Zoteroの設定備忘録.html#社会学評論のcslファイル",
    "title": "Zoteroの設定備忘録",
    "section": "社会学評論のcslファイル",
    "text": "社会学評論のcslファイル\n社会学でよく使われる社会学評論スタイル準拠のCSLファイルを作ってみた． 現状，英語と日本語は別ファイルで，学術論文と本の章は対応している．（2021-08-29）\nhttps://github.com/Kentaro-Kamada/hyoron-style\n\njstage周りの設定について\n\nhttps://home.hirosaki-u.ac.jp/yaneura/353/\nhttps://forums.zotero.org/discussion/5376/j-stage-translator-updated"
  },
  {
    "objectID": "blog/posts/Standard_Errors_in_PIAAC.html",
    "href": "blog/posts/Standard_Errors_in_PIAAC.html",
    "title": "PIAAC、PISAの標準誤差",
    "section": "",
    "text": "library(tidyverse)\nlibrary(haven)\nlibrary(survey)\nlibrary(srvyr)\nlibrary(kamaken)\n\nkable &lt;- partial(knitr::kable, digits = 3)\n\n\n\n\n\nPIAAC 1st cycleの日本データ（SPSS形式）\n\nhttps://webfs.oecd.org/piaac/puf-data/SPSS/prgjpnp1.sav\n\n変数の名前を適宜変更しておく\n\n\ndata &lt;- read_sav('data/prgjpnp1.sav')\n# 直接読み込むこともできるが多少時間がかかる\n# data &lt;- read_sav('https://webfs.oecd.org/piaac/puf-data/SPSS/prgjpnp1.sav')\n\ndf &lt;- \n  data |&gt; \n  # 変数を絞る\n  select(\n    country = CNTRYID, \n    id = SEQID,\n    age = AGE_R, \n    gender = GENDER_R, \n    region = REG_TL2, \n    edu = B_Q01a, \n    medu = J_Q06b, \n    fedu = J_Q07b, \n    numbooks = J_Q08,\n    sampling_weight = SPFWT0, \n    # 読解力、数的思考力、ITスキルのスコア\n    matches('^PV'), \n    # Replicate weights\n    matches(str_c('SPFWT', 1:80)),\n    VEMETHOD\n  ) |&gt; \n  # 変数名を小文字に変換\n  rename_with(str_to_lower) |&gt; \n  as_factor() |&gt; \n  mutate(\n    age = as.character(age) |&gt; parse_double(),\n    # 年齢をカテゴリ化\n    agegroup = cut(\n      age,\n      breaks = c(15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65),\n      labels = c('16-20', '21-25', '26-30', '31-35', '36-40', '41-45', '46-50', '51-55', '56-60', '61-65')\n    ),\n    # 大学卒業ダミー\n    univ = case_match(\n      edu,\n      'No formal qualification or below ISCED 1' ~ 0,\n      'ISCED 2' ~ 0,\n      'ISCED 3C shorter than 2 years' ~ 0,\n      'ISCED 3C 2 years or more' ~ 0,\n      'ISCED 3A-B' ~ 0,\n      'ISCED 3 (without distinction A-B-C, 2y+)' ~ 0,\n      'ISCED 4 (without distinction A-B-C)' ~ 0,\n      'ISCED 5B' ~ 0,\n      'ISCED 5A, bachelor degree' ~ 1,\n      'ISCED 5A, master degree' ~ 1,\n      'ISCED 6' ~ 1,\n      'Foreign qualification' ~ NA,\n      NA ~ NA\n    )\n  )"
  },
  {
    "objectID": "blog/posts/Standard_Errors_in_PIAAC.html#前準備",
    "href": "blog/posts/Standard_Errors_in_PIAAC.html#前準備",
    "title": "PIAAC、PISAの標準誤差",
    "section": "",
    "text": "library(tidyverse)\nlibrary(haven)\nlibrary(survey)\nlibrary(srvyr)\nlibrary(kamaken)\n\nkable &lt;- partial(knitr::kable, digits = 3)\n\n\n\n\n\nPIAAC 1st cycleの日本データ（SPSS形式）\n\nhttps://webfs.oecd.org/piaac/puf-data/SPSS/prgjpnp1.sav\n\n変数の名前を適宜変更しておく\n\n\ndata &lt;- read_sav('data/prgjpnp1.sav')\n# 直接読み込むこともできるが多少時間がかかる\n# data &lt;- read_sav('https://webfs.oecd.org/piaac/puf-data/SPSS/prgjpnp1.sav')\n\ndf &lt;- \n  data |&gt; \n  # 変数を絞る\n  select(\n    country = CNTRYID, \n    id = SEQID,\n    age = AGE_R, \n    gender = GENDER_R, \n    region = REG_TL2, \n    edu = B_Q01a, \n    medu = J_Q06b, \n    fedu = J_Q07b, \n    numbooks = J_Q08,\n    sampling_weight = SPFWT0, \n    # 読解力、数的思考力、ITスキルのスコア\n    matches('^PV'), \n    # Replicate weights\n    matches(str_c('SPFWT', 1:80)),\n    VEMETHOD\n  ) |&gt; \n  # 変数名を小文字に変換\n  rename_with(str_to_lower) |&gt; \n  as_factor() |&gt; \n  mutate(\n    age = as.character(age) |&gt; parse_double(),\n    # 年齢をカテゴリ化\n    agegroup = cut(\n      age,\n      breaks = c(15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65),\n      labels = c('16-20', '21-25', '26-30', '31-35', '36-40', '41-45', '46-50', '51-55', '56-60', '61-65')\n    ),\n    # 大学卒業ダミー\n    univ = case_match(\n      edu,\n      'No formal qualification or below ISCED 1' ~ 0,\n      'ISCED 2' ~ 0,\n      'ISCED 3C shorter than 2 years' ~ 0,\n      'ISCED 3C 2 years or more' ~ 0,\n      'ISCED 3A-B' ~ 0,\n      'ISCED 3 (without distinction A-B-C, 2y+)' ~ 0,\n      'ISCED 4 (without distinction A-B-C)' ~ 0,\n      'ISCED 5B' ~ 0,\n      'ISCED 5A, bachelor degree' ~ 1,\n      'ISCED 5A, master degree' ~ 1,\n      'ISCED 6' ~ 1,\n      'Foreign qualification' ~ NA,\n      NA ~ NA\n    )\n  )"
  },
  {
    "objectID": "blog/posts/Standard_Errors_in_PIAAC.html#標準誤差の推定式",
    "href": "blog/posts/Standard_Errors_in_PIAAC.html#標準誤差の推定式",
    "title": "PIAAC、PISAの標準誤差",
    "section": "標準誤差の推定式",
    "text": "標準誤差の推定式\n\njackknife法による標準誤差\n\n\\[\n\\mathrm{SE}_\\theta = \\sqrt{h\\sum_{r=1}^{R} (\\hat{\\theta}_{(r)} - \\hat{\\theta})^2}\n\\]\n\n\\(R\\)：反復回数、replicate weightの数（PIAACは80）\n\\(\\hat{\\theta}_{(r)}\\)：\\(r\\)番目のreplicate weightを用いた推定値\n\\(\\hat{\\theta}\\)：全体の推定値（サンプリングウェイトを用いた推定値）\n\\(h\\)：乗数、 Jackknife法のバリエーションによって異なる\n\nJK1の場合は\\(h = \\frac{R-1}{R}\\)\nJK2の場合は\\(h = 1\\)"
  },
  {
    "objectID": "blog/posts/Standard_Errors_in_PIAAC.html#分析",
    "href": "blog/posts/Standard_Errors_in_PIAAC.html#分析",
    "title": "PIAAC、PISAの標準誤差",
    "section": "分析",
    "text": "分析\n\n点推定値\njackknife法による標準誤差\n\nの二つを推定する\n\npackageによる推定\n\nsurveyパッケージ（のラッパーのsrvyrパッケージ）を用いる\ntype: 標準誤差の推定方法\n\nPIAACではJK1の国とJK2の国が混在（変数VEMETHODにどちらを使用すれば良いかが書かれている）\n日本はJK2で推定する\n\n\n\ndf_design &lt;- \n  df |&gt; \n  # 調査デザインの設定\n  as_survey_rep(\n    weights = sampling_weight, \n    repweights = matches('spfwt'), \n    type = 'JK2',\n    mse = TRUE\n  )\n\nresult_literacy &lt;- \n  df_design |&gt; \n  # 読解力の各PVごとに平均値と標準誤差を計算\n  summarise(\n    across(matches('pvlit'), \\(x) survey_mean(x, na.rm = TRUE))\n  ) \n  \n# 結果の整理\npackage &lt;- \n  result_literacy |&gt;\n  rename_with(\\(x) str_replace(x, '(\\\\d)$', '\\\\1_estimate')) |&gt;\n  # 縦持ちに変換\n  pivot_longer(\n    cols = matches('pvlit'),\n    names_to = c('literacy', '.value'),\n    names_pattern = '(pvlit\\\\d{1,2})_(.+)',\n  ) \n\nkable(package)\n\n\n\n\nliteracy\nestimate\nse\n\n\n\n\npvlit1\n296.468\n0.549\n\n\npvlit2\n296.143\n0.529\n\n\npvlit3\n296.184\n0.543\n\n\npvlit4\n296.111\n0.541\n\n\npvlit5\n296.172\n0.522\n\n\npvlit6\n296.936\n0.543\n\n\npvlit7\n295.561\n0.481\n\n\npvlit8\n296.827\n0.518\n\n\npvlit9\n296.188\n0.521\n\n\npvlit10\n295.832\n0.542\n\n\n\n\n\n\n\n手作業による推定\n\npoint_estimate &lt;-\n  df |&gt; \n  select(sampling_weight, matches('(pvlit|spfwt)')) |&gt; \n  summarise(\n    across(\n      matches('pvlit'), \n      \\(x) weighted.mean(x, w = sampling_weight, na.rm = TRUE), \n      .names = '{.col}_estimate'\n    )\n  ) |&gt; \n  pivot_longer(\n    cols = matches('pvlit'),\n    names_to = c('literacy', '.value'),\n    names_pattern = '(pvlit\\\\d{1,2})_(.+)',\n  )\n  \n# jackknife法に用いるウェイト（80個）\njackweight &lt;- select(df, matches('spfwt'))\n\njack_estimate &lt;-\n  # 各ウェイトを用いて、それぞれのPVの平均値を計算\n  map(\n    jackweight, \\(weight) \n    df |&gt; \n      summarise(\n        across(\n          matches('pvlit'),\n          \\(x) weighted.mean(x, w = weight, na.rm = TRUE),\n          .names = '{.col}_jack'\n        )\n      )\n  ) |&gt; \n  bind_rows(.id = 'replicate') |&gt; \n  pivot_longer(\n    cols = matches('pvlit'),\n    names_to = c('literacy', '.value'),\n    names_pattern = '(pvlit\\\\d{1,2})_(.+)',\n  )\n\nhandmade &lt;- \n  # 点推定値にジャックナイフウェイトを用いた推定値を結合\n  left_join(\n    point_estimate, \n    jack_estimate, \n    by = join_by(literacy)\n  ) |&gt; \n  # 標準誤差の計算\n  summarise(\n    estimate = mean(estimate),\n    se = sqrt(sum((estimate - jack)^2)),\n    .by = literacy\n  )\n\nkable(handmade)\n\n\n\n\nliteracy\nestimate\nse\n\n\n\n\npvlit1\n296.468\n0.549\n\n\npvlit2\n296.143\n0.529\n\n\npvlit3\n296.184\n0.543\n\n\npvlit4\n296.111\n0.541\n\n\npvlit5\n296.172\n0.522\n\n\npvlit6\n296.936\n0.543\n\n\npvlit7\n295.561\n0.481\n\n\npvlit8\n296.827\n0.518\n\n\npvlit9\n296.188\n0.521\n\n\npvlit10\n295.832\n0.542\n\n\n\n\n\n\n\n結果の比較\n\n結果は一致する\n\n\nleft_join(\n  package, \n  handmade, \n  by = join_by(literacy),\n  suffix = c('_package', '_handmade')\n) |&gt; \n  select(literacy, estimate_package, estimate_handmade, se_package, se_handmade) |&gt;\n  mutate(\n    diff_estimate = estimate_package - estimate_handmade,\n    diff_se = se_package - se_handmade\n  ) |&gt; \n  kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\nliteracy\nestimate_package\nestimate_handmade\nse_package\nse_handmade\ndiff_estimate\ndiff_se\n\n\n\n\npvlit1\n296.468\n296.468\n0.549\n0.549\n0\n0\n\n\npvlit2\n296.143\n296.143\n0.529\n0.529\n0\n0\n\n\npvlit3\n296.184\n296.184\n0.543\n0.543\n0\n0\n\n\npvlit4\n296.111\n296.111\n0.541\n0.541\n0\n0\n\n\npvlit5\n296.172\n296.172\n0.522\n0.522\n0\n0\n\n\npvlit6\n296.936\n296.936\n0.543\n0.543\n0\n0\n\n\npvlit7\n295.561\n295.561\n0.481\n0.481\n0\n0\n\n\npvlit8\n296.827\n296.827\n0.518\n0.518\n0\n0\n\n\npvlit9\n296.188\n296.188\n0.521\n0.521\n0\n0\n\n\npvlit10\n295.832\n295.832\n0.542\n0.542\n0\n0\n\n\n\n\n\n\n\nPVの統合\n\n読解力、数的思考力などは複数のPV（Plausible Values）という形で測定\nPVを用いた分析の結果は、多重代入法の要領で統合が可能\n詳しくは van Buuren (2018) などを参照\n平均値\n\n\\[\n\\hat{\\theta} = \\frac{1}{M} \\sum_{m=1}^{M} \\hat{\\theta}_{(m)}\n\\]\n\n各PVにおいて生じる分散の平均（分散の平均値）\n\n\\[\n\\bar{U} = \\frac{1}{M} \\sum_{m=1}^{M} \\hat{U}_{(m)}\n\\]\n\nPV間で生じる分散（平均値の分散）\n\n\\[\nB = \\frac{1}{M-1} \\sum_{m=1}^{M} (\\hat{\\theta}_{(m)} - \\hat{\\theta})^2\n\\]\n\n統合された推定値の標準誤差\n\n\\[\n\\hat{SE} = \\sqrt{\\bar{U} + \\left(1 + \\frac{1}{M}\\right)B}\n\\]\n\npool_rubin &lt;- function(.tibble, term = NULL) {\n  .tibble |&gt; \n    mutate(variance = std.error^2) |&gt;\n    summarise(\n      M = n(),\n      estimate.combined = mean(estimate),\n      Vw = sum(variance) / M,\n      Vb = sum((estimate - mean(estimate)) ^ 2) / (M - 1),\n      Vt = Vw + (1 + 1 / M) * Vb,\n      SE.combined = sqrt(Vt), \n      .by = {{term}}\n    ) |&gt; \n    # 信頼区間計算\n    mutate(\n      conf.low = estimate.combined - qnorm(1 - .025)*SE.combined,\n      conf.high = estimate.combined + qnorm(1 - .025)*SE.combined\n    ) |&gt; \n    select({{term}}, M, estimate = estimate.combined, std.error = SE.combined, conf.low, conf.high)\n} \n\n\n結果の統合\n\n\nhandmade |&gt; \n  rename(std.error = se) |&gt; \n  pool_rubin() |&gt; \n  kable()\n\n\n\n\nM\nestimate\nstd.error\nconf.low\nconf.high\n\n\n\n\n10\n296.242\n0.685\n294.9\n297.585"
  },
  {
    "objectID": "blog/posts/Standard_Errors_in_PIAAC.html#より複雑なケース",
    "href": "blog/posts/Standard_Errors_in_PIAAC.html#より複雑なケース",
    "title": "PIAAC、PISAの標準誤差",
    "section": "より複雑なケース",
    "text": "より複雑なケース\n\n因果効果の推定\n\n高等教育進学が読解力に与える因果効果の推定\nG-computation(Hernán and Robins 2020; Robins 1986)によるATEの推定\n関数定義\n\n.outcome：アウトカム変数\n.treatment：0-1の二値変数\n.formula_rhs：回帰式の右辺を指定\n\n~ treatment + covariate1 + covariate2 + ...のような形\n固定効果も指定可能：~ 1 | treatment + fixed_effect1 + fixed_effect2 + ...\nfixestパッケージでのformulaに準拠\n\n.estimand：推定対象\n\ncfmean：\\(\\mathrm{E}[Y^1]\\)と\\(\\mathrm{E}[Y^0]\\)\nATE：平均処置効果\n\n.weights：サンプリングウェイト\n.repweights：ジャックナイフ法に用いるreplicate weights\n.type：ジャックナイフ法のタイプ\n.by：効果の異質性を見たい変数を指定（NULLなら集団全体）\n\n\n\n# ATEをjackknife推定する関数\npiaac_ATE_jack &lt;- \n  function(.data, .outcome, .treatment, .formula_rhs, .estimand = c('cfmean', 'ATE'), .weights, .repweights, .type = c('JK1', 'JK2'), .by = NULL) {\n  .treatment &lt;- enquo(.treatment)\n  .weights &lt;- enquo(.weights)\n  .repweights &lt;- select(.data, {{.repweights}}) |&gt; names()\n  .by &lt;- enquo(.by)\n  # scale parameterの設定\n  scale &lt;- case_when(\n    .type == 'JK1' ~ (length(.repweights) - 1) / length(.repweights), \n    .type == 'JK2' ~ 1\n  )\n  # formulaの左辺に.outcomeを追加\n  .formula &lt;- rlang::`f_lhs&lt;-`(.formula_rhs, ensym(.outcome))\n  \n  # OLS推定\n  fit &lt;-\n    fixest::feols(\n      .formula,\n      data = .data,\n      combine.quick = FALSE,\n      weights = str_c('~', rlang::as_name(.weights)) |&gt; as.formula()\n    )\n  # 点推定値\n  res_point_estimate &lt;-\n    bind_rows(\n      broom::augment(fit, newdata = .data |&gt; mutate(!!.treatment := 0)),\n      broom::augment(fit, newdata = .data |&gt; mutate(!!.treatment := 1)),\n    ) |&gt;\n    group_by(pick(!!.treatment, !!.by)) |&gt;\n    summarise(\n      estimate = weighted.mean(.fitted, !!.weights),\n      .groups = 'drop'\n    ) |&gt; \n    mutate(estimand = 'cfmean')\n  if (.estimand == 'ATE') {\n    res_point_estimate &lt;-\n      res_point_estimate |&gt;\n      mutate(estimand = 'ATE') |&gt; \n      pivot_wider(names_from = !!.treatment, values_from = estimate) |&gt;\n      mutate(estimate = `1` - `0`) |&gt;\n      select(!c(`0`, `1`))\n  }\n\n  # 各replicate weightを使ってOLS推定し、jackknifeサンプルにおける点推定値を計算する\n  fit_jack &lt;-\n    furrr::future_map(\n      .repweights, .progress = TRUE, .options = furrr::furrr_options(seed = TRUE), \\(w) {\n        gc()\n        # weightが0のデータを削除\n        df_jack &lt;- .data |&gt; filter(!!ensym(w) != 0)\n        # OLSで推定\n        fit &lt;-\n          fixest::feols(\n            .formula,\n            data = df_jack,\n            weights = str_c('~', w) |&gt; as.formula(),\n            combine.quick = FALSE\n          )\n        # Y^1とY^0を計算\n        res_point_estimate_jack &lt;-\n          bind_rows(\n            broom::augment(fit, newdata = df_jack |&gt; mutate(!!.treatment := 0)),\n            broom::augment(fit, newdata = df_jack |&gt; mutate(!!.treatment := 1)),\n          ) |&gt;\n          group_by(pick(!!.treatment, !!.by)) |&gt;\n          summarise(\n            estimate = weighted.mean(.fitted, !!ensym(w)),\n            .groups = 'drop'\n          ) |&gt; \n          mutate(estimand = 'cfmean')\n        if (.estimand == 'ATE') {\n          res_point_estimate_jack &lt;-\n            res_point_estimate_jack |&gt;\n            mutate(estimand = 'ATE') |&gt;\n            pivot_wider(names_from = !!.treatment, values_from = estimate) |&gt;\n            mutate(estimate = `1` - `0`) |&gt;\n            select(!c(`0`, `1`))\n        }\n        res_point_estimate_jack\n      }\n    )\n  # 結果の統合\n  results &lt;-\n    fit_jack |&gt;\n    bind_rows(.id = 'no') |&gt;\n    rename(estimate_jack = estimate) |&gt;\n    # 点推定値をくっつける\n    left_join(res_point_estimate |&gt; rename(estimate_all = estimate))\n\n  # jackknife標準誤差の計算\n  if (.estimand == 'cfmean') {\n    results &lt;-\n      results |&gt;\n      group_by(pick(estimand, !!.treatment, !!.by)) |&gt;\n      summarise(\n        estimate = mean(estimate_all),\n        std.error = sqrt(scale * sum((estimate_all - estimate_jack)^2, na.rm = TRUE)),\n        .groups = 'drop'\n      )\n  }\n  else if (.estimand == 'ATE') {\n    results &lt;-\n      results |&gt;\n      group_by(pick(estimand, !!.by)) |&gt;\n      summarise(\n        estimate = mean(estimate_all),\n        std.error = sqrt(scale * sum((estimate_all - estimate_jack)^2, na.rm = TRUE)),\n        .groups = 'drop'\n      )\n  }\n  results\n}\n\n\n\n推定結果\n\n# 並列化\nfuture::plan('multisession')\n\ndf |&gt; \n  filter(age &gt;= 26) |&gt; \n  piaac_ATE_jack(\n    .outcome = pvlit1,\n    .treatment = univ,\n    .formula_rhs = ~ 1 | univ^gender^agegroup,\n    .estimand = 'ATE',\n    .weights = sampling_weight,\n    .repweights = matches('spfwt'),\n    .type = 'JK2',\n    .by = c(gender, agegroup)\n  ) |&gt; \n  kable()\n\n\n\n\nestimand\ngender\nagegroup\nestimate\nstd.error\n\n\n\n\nATE\nMale\n26-30\n26.590\n4.692\n\n\nATE\nMale\n31-35\n24.986\n4.766\n\n\nATE\nMale\n36-40\n28.109\n3.835\n\n\nATE\nMale\n41-45\n28.916\n3.917\n\n\nATE\nMale\n46-50\n30.594\n4.789\n\n\nATE\nMale\n51-55\n35.483\n5.616\n\n\nATE\nMale\n56-60\n38.366\n5.703\n\n\nATE\nMale\n61-65\n24.382\n4.711\n\n\nATE\nFemale\n26-30\n22.546\n3.546\n\n\nATE\nFemale\n31-35\n24.534\n3.581\n\n\nATE\nFemale\n36-40\n27.753\n4.466\n\n\nATE\nFemale\n41-45\n34.266\n4.167\n\n\nATE\nFemale\n46-50\n23.310\n4.708\n\n\nATE\nFemale\n51-55\n32.056\n5.081\n\n\nATE\nFemale\n56-60\n26.026\n4.619\n\n\nATE\nFemale\n61-65\n40.169\n10.041\n\n\n\n\n\n\n\n10個のPVに対して一括で推定\n\n# 読解力PVの変数名を取得\npvs &lt;- df |&gt; select(matches('pvlit')) |&gt; names()\n\nresult &lt;- \n  map(\n    pvs,\n    \\(x) piaac_ATE_jack(\n      .data = df |&gt; filter(age &gt;= 26),\n      # mapで回すときは`!!`を使う\n      .outcome = !!x,\n      .treatment = univ,\n      .formula_rhs = ~ 1 | univ^gender^agegroup,\n      .estimand = 'ATE',\n      .weights = sampling_weight,\n      .repweights = matches('spfwt'),\n      .type = 'JK2',\n      .by = gender\n    )) |&gt; \n  bind_rows(.id = 'pv')\n\nresult\n\n# A tibble: 20 × 5\n   pv    estimand gender estimate std.error\n   &lt;chr&gt; &lt;chr&gt;    &lt;fct&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n 1 1     ATE      Male       29.4      1.61\n 2 1     ATE      Female     29.2      2.01\n 3 2     ATE      Male       29.8      1.78\n 4 2     ATE      Female     29.7      1.98\n 5 3     ATE      Male       29.4      1.76\n 6 3     ATE      Female     29.6      2.13\n 7 4     ATE      Male       32.2      1.73\n 8 4     ATE      Female     32.3      2.16\n 9 5     ATE      Male       30.6      1.71\n10 5     ATE      Female     29.8      1.82\n11 6     ATE      Male       30.5      1.67\n12 6     ATE      Female     29.7      2.06\n13 7     ATE      Male       30.4      1.78\n14 7     ATE      Female     31.0      1.80\n15 8     ATE      Male       30.0      1.84\n16 8     ATE      Female     31.2      1.91\n17 9     ATE      Male       32.1      1.66\n18 9     ATE      Female     31.3      2.17\n19 10    ATE      Male       30.8      1.60\n20 10    ATE      Female     32.7      1.81\n\n\n\n\n結果の統合\n\nresult |&gt; \n  pool_rubin(term = c(estimand, gender)) |&gt; \n  kable()\n\n\n\n\nestimand\ngender\nM\nestimate\nstd.error\nconf.low\nconf.high\n\n\n\n\nATE\nMale\n10\n30.526\n1.996\n26.613\n34.439\n\n\nATE\nFemale\n10\n30.643\n2.364\n26.011\n35.276"
  },
  {
    "objectID": "blog/posts/Standard_Errors_in_PIAAC.html#参考文献",
    "href": "blog/posts/Standard_Errors_in_PIAAC.html#参考文献",
    "title": "PIAAC、PISAの標準誤差",
    "section": "参考文献",
    "text": "参考文献\nJakubowski, Maciej & Artur Pokropek, 2019, “piaactools: A program for data analysis with PIAAC data,” The Stata Journal, 19(1): 112-128. https://doi.org/10.1177/1536867X19830909"
  },
  {
    "objectID": "blog/posts/RとPython書き比べ（ユークリッドの互除法）.html",
    "href": "blog/posts/RとPython書き比べ（ユークリッドの互除法）.html",
    "title": "RとPython書き比べ（ユークリッドの互除法）",
    "section": "",
    "text": "ユークリッドの互除法は\n\n2つの自然数の最大公約数（GCD: Greatest Common Diviser）を求めるアルゴリズム\nそのために以下の性質を利用\n\n\\(a\\), \\(b\\)は自然数で\\(a \\neq 0\\)のとき\n等式：\\(a = bq + r\\)において，\\(\\mathrm{GCD}(a, b) = \\mathrm{GCD}(b, r)\\)が成り立つ\nこの性質を利用して，\n「\\(a\\)を\\(b\\)で割って余り\\(r_1\\)を算出」→「\\(b\\)を\\(r_1\\)で割って余り\\(r_2\\)を算出」→…\n→「\\(r_{n-1}\\)を\\(r_n\\)で割ると割り切れた」\n→\\(\\mathrm{GCD}(r_{n-1}, r_n) = \\mathrm{GCD}(r_{n-2}, n_{n-1})= ...=\\mathrm{GCD}(b, r_1) = \\mathrm{GCD}(a, b) = r_n\\)\nという形で最大公約数を求める"
  },
  {
    "objectID": "blog/posts/RとPython書き比べ（ユークリッドの互除法）.html#ユークリッドの互除法",
    "href": "blog/posts/RとPython書き比べ（ユークリッドの互除法）.html#ユークリッドの互除法",
    "title": "RとPython書き比べ（ユークリッドの互除法）",
    "section": "",
    "text": "ユークリッドの互除法は\n\n2つの自然数の最大公約数（GCD: Greatest Common Diviser）を求めるアルゴリズム\nそのために以下の性質を利用\n\n\\(a\\), \\(b\\)は自然数で\\(a \\neq 0\\)のとき\n等式：\\(a = bq + r\\)において，\\(\\mathrm{GCD}(a, b) = \\mathrm{GCD}(b, r)\\)が成り立つ\nこの性質を利用して，\n「\\(a\\)を\\(b\\)で割って余り\\(r_1\\)を算出」→「\\(b\\)を\\(r_1\\)で割って余り\\(r_2\\)を算出」→…\n→「\\(r_{n-1}\\)を\\(r_n\\)で割ると割り切れた」\n→\\(\\mathrm{GCD}(r_{n-1}, r_n) = \\mathrm{GCD}(r_{n-2}, n_{n-1})= ...=\\mathrm{GCD}(b, r_1) = \\mathrm{GCD}(a, b) = r_n\\)\nという形で最大公約数を求める"
  },
  {
    "objectID": "blog/posts/RとPython書き比べ（ユークリッドの互除法）.html#rで関数を実装",
    "href": "blog/posts/RとPython書き比べ（ユークリッドの互除法）.html#rで関数を実装",
    "title": "RとPython書き比べ（ユークリッドの互除法）",
    "section": "Rで関数を実装",
    "text": "Rで関数を実装\n\n関数定義\n\n\n# ユークリッドの互除法\ngcd &lt;- function(a, b){\n  if (!(a %% 1 == 0 & b %% 1 == 0 & a &gt; 0 & b &gt; 0)) {\n    cat('入力が自然数じゃないのでやり直し')\n  } \n  else if (a &lt; b) {\n    w &lt;- a\n    a &lt;- b\n    b &lt;- w\n  }\n  while (b != 0) {\n    r &lt;- a %% b\n    a &lt;- b\n    b &lt;- r\n  }\n  return(a)\n}\n\n\n実行結果\n\n\ngcd(50856, 96007)\n\n[1] 163"
  },
  {
    "objectID": "blog/posts/RとPython書き比べ（ユークリッドの互除法）.html#pythonで実装",
    "href": "blog/posts/RとPython書き比べ（ユークリッドの互除法）.html#pythonで実装",
    "title": "RとPython書き比べ（ユークリッドの互除法）",
    "section": "Pythonで実装",
    "text": "Pythonで実装\n\n関数定義\n\n\n# ユークリッドの互除法\ndef gcd(a, b):\n  if not (a % 1 == 0 and b % 1 == 0 and a &gt; 0 and b &gt; 0):\n    print('入力が自然数じゃないのでやり直し')\n  elif a &lt; b:\n    w = a\n    a = b\n    b = w\n  while not b == 0:\n    r = a % b\n    a = b\n    b = r\n  else:\n    return(a)\n\n\n実行結果\n\n\ngcd(50856, 96007)\n\n163"
  },
  {
    "objectID": "blog/posts/RとPython書き比べ（ユークリッドの互除法）.html#両言語の比較",
    "href": "blog/posts/RとPython書き比べ（ユークリッドの互除法）.html#両言語の比較",
    "title": "RとPython書き比べ（ユークリッドの互除法）",
    "section": "両言語の比較",
    "text": "両言語の比較\n\n1. 制御構文\n\n\n\n動作\nR\nPython\n\n\n\n\n関数定義\nname &lt;- function(引数){処理}\ndef name(引数):処理\n\n\n条件分岐1\nif(条件式){処理}\nif 条件式:処理\n\n\n条件分岐2\nelse if(条件式){処理}\nelif 条件式:処理\n\n\n繰り返し\nwhile(条件式){処理}\nwhile 条件式:処理\n\n\n\n\n\n2. 演算子など\n\n\n\n動作\nR\nPython\n\n\n\n\n整数商\n%/%\n//\n\n\n剰余\n%%\n%\n\n\n論理積\n&\nand\n\n\n論理和\n|\nor\n\n\n否定\n!\nnot\n\n\n\nRでは一貫して記号で演算子が与えられている一方， Pythonは条件分岐に関わる部分はアルファベットが用いられている。\nRの論理演算子がfilter処理とかで多用されることがイメージされている一方， Pythonはもっぱら条件分岐での使用がイメージされてそう？ （if not ～とかは自然に読みやすいけど，filter(a == 1 and b &lt;= 3 and ～)は長くなって読みにくいみたいな）"
  },
  {
    "objectID": "blog/posts/RとPython書き比べ（ユークリッドの互除法）.html#追記",
    "href": "blog/posts/RとPython書き比べ（ユークリッドの互除法）.html#追記",
    "title": "RとPython書き比べ（ユークリッドの互除法）",
    "section": "追記",
    "text": "追記\n\n制御フローの見直し\n\nR\n\n変数の代入の部分を;を用いて一列にできるらしい（やってることは変わらない）\nPythonみたいなa, b = b, aという書き方はできず，中間変数を使わざるを得ない\n\n\ngcd2 &lt;- function(a, b){\n  if (!(a %% 1 == 0 & b %% 1 == 0 & a &gt; 0 & b &gt; 0)) {\n    cat('入力が自然数じゃないのでやり直し')\n  } \n  else {\n    if(a &lt; b){\n      tmp &lt;- a; a &lt;- b; b &lt;- tmp\n    }\n    while(b != 0){\n      r &lt;- a %% b; a &lt;- b; b &lt;- r \n    }\n    return(a)\n  }\n}\n\n\n\nPython\n\na, b = b, aという記法が大変便利。スワップ処理とかで中間変数が必要ない。\n\n\ndef gcd2(a, b):\n  if not (a % 1 == 0 and b % 1 == 0 and a &gt; 0 and b &gt; 0):\n    print('入力が自然数じゃないのでやり直し')\n  else:\n    if a &lt; b:\n      a, b = b, a\n    while a % b != 0:\n      a, b = b, a % b\n    else:\n      return b\n\n\n\n\n再帰関数を用いた実装\n再帰関数をコメントで教えてもらったので実装してみた。\n注意点として，b == 0になるまで繰り返してしまうと，引数が自然数という条件に反してしまうので，その一回前（a % b == 0）まで繰り返すように書き換える必要がある。\n\nPython\n\n\ndef gcd3(a,b):\n  if not (a % 1 == 0 and b % 1 == 0 and a &gt; 0 and b &gt; 0):\n    print('入力が自然数じゃないのでやり直し')\n  else:\n    if a &lt; b:\n      a, b = b, a\n    if not a % b == 0:\n      return gcd3(b, a % b)\n    else:\n      return b\n\n\ngcd(50856, 96007)\n\n163\n\ngcd2(50856, 96007)\n\n163\n\ngcd3(50856, 96007)\n\n163\n\n\n\nR\n\n再帰関数を呼び出す用のRecallという関数もある\n\ngcd3 &lt;- function(a, b){\n  if (!(a %% 1 == 0 & b %% 1 == 0 & a &gt; 0 & b &gt; 0)) {\n    cat('入力が自然数じゃないのでやり直し')\n  } \n  else {\n    if (a &lt; b) {\n      tmp &lt;- a; a &lt;- b; b &lt;- tmp\n    }\n    if (a %% b != 0) {\n      return(Recall(b, a %% b)) # またはgcd(b, a %% b)\n    }\n    else return(b)\n  }\n}\n\n\ngcd(50856, 96007)\n\n[1] 163\n\ngcd2(50856, 96007)\n\n[1] 163\n\ngcd3(50856, 96007)\n\n[1] 163"
  },
  {
    "objectID": "blog/posts/移動平均のかゆいところに.html",
    "href": "blog/posts/移動平均のかゆいところに.html",
    "title": "移動平均のかゆいところに，partial = TRUE",
    "section": "",
    "text": "『前処理大全』の4章で移動平均の計算が出てきたのでメモ。\n移動平均の基本的な算出方法は「dplyrを使いこなす！Window関数編」などが詳しい。"
  },
  {
    "objectID": "blog/posts/移動平均のかゆいところに.html#はじめに",
    "href": "blog/posts/移動平均のかゆいところに.html#はじめに",
    "title": "移動平均のかゆいところに，partial = TRUE",
    "section": "",
    "text": "『前処理大全』の4章で移動平均の計算が出てきたのでメモ。\n移動平均の基本的な算出方法は「dplyrを使いこなす！Window関数編」などが詳しい。"
  },
  {
    "objectID": "blog/posts/移動平均のかゆいところに.html#移動平均の基本",
    "href": "blog/posts/移動平均のかゆいところに.html#移動平均の基本",
    "title": "移動平均のかゆいところに，partial = TRUE",
    "section": "移動平均の基本",
    "text": "移動平均の基本\nxにこんな感じでデータ入っているとき，移動平均は以下のように計算される。\n（以下ではすべて，3つずつ計算，右詰め，の前提で書く）\n\n\n\nx\nrollsum\nrollmean\n\n\n\n\n1\nNA\nNA\n\n\n2\nNA\nNA\n\n\n3\n1+2+3\n(1+2+3)/3\n\n\n4\n2+3+4\n(2+3+4)/3\n\n\n5\n3+4+5\n(3+4+5)/3\n\n\n6\n4+5+6\n(4+5+6)/3\n\n\n\nRで実装するとこんな感じになる\n\nlibrary(tidyverse)\n\ntibble(x = 1:6) |&gt; \n  mutate(\n    rollsum = RcppRoll::roll_sum(x, n = 3L, align = 'right', fill = NA),\n    rollmean = RcppRoll::roll_mean(x, n = 3L, align = 'right', fill = NA)\n  )\n\n# A tibble: 6 × 3\n      x rollsum rollmean\n  &lt;int&gt;   &lt;dbl&gt;    &lt;dbl&gt;\n1     1      NA       NA\n2     2      NA       NA\n3     3       6        2\n4     4       9        3\n5     5      12        4\n6     6      15        5"
  },
  {
    "objectID": "blog/posts/移動平均のかゆいところに.html#移動平均のかゆいところ",
    "href": "blog/posts/移動平均のかゆいところに.html#移動平均のかゆいところ",
    "title": "移動平均のかゆいところに，partial = TRUE",
    "section": "移動平均のかゆいところ",
    "text": "移動平均のかゆいところ\nここまでの移動平均は，値が3つ揃っているところだけ計算し，3つに満たない端っこのケースに関しては欠損で埋める，というルールで計算している。\nただし，場合によっては3件に満たない場合でも欠損値扱いにせずに平均値を計算したい，という場合もあるだろう。\n（イメージ）\n\n\n\nx\nrollsum\nrollmean\n\n\n\n\n1\n1\n1/1\n\n\n2\n1+2\n(1+2)/2\n\n\n3\n1+2+3\n(1+2+3)/3\n\n\n4\n2+3+4\n(2+3+4)/3\n\n\n5\n3+4+5\n(3+4+5)/3\n\n\n6\n4+5+6\n(4+5+6)/3\n\n\n\n『前処理大全』でもこのようなケースが扱われており，Rのサンプルコードではlagと条件式の組み合わせで突破していたが，あんまりきれいじゃない。"
  },
  {
    "objectID": "blog/posts/移動平均のかゆいところに.html#zoorollapplyを用いた実装",
    "href": "blog/posts/移動平均のかゆいところに.html#zoorollapplyを用いた実装",
    "title": "移動平均のかゆいところに，partial = TRUE",
    "section": "zoo::rollapplyを用いた実装",
    "text": "zoo::rollapplyを用いた実装\n以上の問題を解決するにはzoo::rollapply()が有効である。\nこの関数の引数で，partial = TRUEとすると，以上の問題に対処できる。\n詳しくはvignetteを参照。\n\ntibble(x = 1:6) |&gt; \n  mutate(\n    rollsum = zoo::rollapply(x, width = 3L, FUN = sum, align = 'right', partial = TRUE),\n    rollmean = zoo::rollapply(x, width = 3L, FUN = mean, align = 'right', partial = TRUE)\n  )\n\n# A tibble: 6 × 3\n      x rollsum rollmean\n  &lt;int&gt;   &lt;int&gt;    &lt;dbl&gt;\n1     1       1      1  \n2     2       3      1.5\n3     3       6      2  \n4     4       9      3  \n5     5      12      4  \n6     6      15      5  \n\n\nこの場合1行目の値は和でも平均でも，元のデータがダイレクトに反映されることになる。\n最低でも2つ以上の値の場合だけ計算したい！という時は，partial = 2Lのように，引数に整数を入れればよい。\n\ntibble(x = 1:6) |&gt; \n  mutate(\n    rollsum = zoo::rollapply(x, width = 3L, FUN = sum, align = 'right', fill = NA, partial = 2L),\n    rollmean = zoo::rollapply(x, width = 3L, FUN = mean, align = 'right', fill = NA, partial = 2L)\n  )\n\n# A tibble: 6 × 3\n      x rollsum rollmean\n  &lt;int&gt;   &lt;int&gt;    &lt;dbl&gt;\n1     1      NA     NA  \n2     2       3      1.5\n3     3       6      2  \n4     4       9      3  \n5     5      12      4  \n6     6      15      5  \n\n\nまた，なぜかzoo::rollsumやzoo::rollmeanでpartial = TRUEとしても上手くいかない。なぜ…？\n\nzoo::rollapply(1:6, width = 3L, FUN = mean, align = 'right', partial = TRUE)\n\n[1] 1.0 1.5 2.0 3.0 4.0 5.0\n\nzoo::rollmean(1:6, k = 3L, align = 'right', partial = TRUE)\n\n[1] 2 3 4 5"
  },
  {
    "objectID": "blog/posts/移動平均のかゆいところに.html#rcpprollにもpartialがあるが",
    "href": "blog/posts/移動平均のかゆいところに.html#rcpprollにもpartialがあるが",
    "title": "移動平均のかゆいところに，partial = TRUE",
    "section": "RcppRollにもpartialがあるが…",
    "text": "RcppRollにもpartialがあるが…\n移動平均といえばRcppRollを使う人が多いはず。速度もzooより断然速いので基本的にはこちらを使うべきと筆者も思っている。\n実はRcppRollの関数にもpartial引数が入っている。\nこれを見た時に「これで行けるやん」と思ったのだが…\n\npartial Partial application? Currently unimplemented. https://cran.r-project.org/web/packages/RcppRoll/RcppRoll.pdf\n\n実装されてませんでした…"
  },
  {
    "objectID": "blog/posts/移動平均のかゆいところに.html#まとめ",
    "href": "blog/posts/移動平均のかゆいところに.html#まとめ",
    "title": "移動平均のかゆいところに，partial = TRUE",
    "section": "まとめ",
    "text": "まとめ\nというわけで，移動平均の計算は通常時はRcppRollを使いつつ，ああいった特殊な場合にはzooを使えばよいということがわかった。RcppRollは早くpartialを実装してほしいところ。"
  },
  {
    "objectID": "blog/posts/信頼区間を理解したい.html",
    "href": "blog/posts/信頼区間を理解したい.html",
    "title": "信頼区間を理解したい",
    "section": "",
    "text": "信頼区間の意味がわかりやすいシミュレーション"
  },
  {
    "objectID": "blog/posts/信頼区間を理解したい.html#パッケージの読み込み",
    "href": "blog/posts/信頼区間を理解したい.html#パッケージの読み込み",
    "title": "信頼区間を理解したい",
    "section": "パッケージの読み込み",
    "text": "パッケージの読み込み\n\nlibrary(tidyverse)"
  },
  {
    "objectID": "blog/posts/信頼区間を理解したい.html#設定",
    "href": "blog/posts/信頼区間を理解したい.html#設定",
    "title": "信頼区間を理解したい",
    "section": "設定",
    "text": "設定\n\n100人の研究者がいる\n各研究者がそれぞれデータをとる。N=500の調査を行う\nデータは平均0.5、標準偏差0.1の正規分布に従うと仮定\n\n\nset.seed(1)\n\ndata &lt;- \n  tibble(\n    # 100人の研究者がいる\n    researcher_ID = 1:100,\n    # 各研究者がそれぞれデータをとる。N=500の調査を行う。\n    data = map(researcher_ID, \\(researcher_ID) rnorm(500, 0.5, 0.1))\n  )\n\ndata\n\n# A tibble: 100 × 2\n   researcher_ID data       \n           &lt;int&gt; &lt;list&gt;     \n 1             1 &lt;dbl [500]&gt;\n 2             2 &lt;dbl [500]&gt;\n 3             3 &lt;dbl [500]&gt;\n 4             4 &lt;dbl [500]&gt;\n 5             5 &lt;dbl [500]&gt;\n 6             6 &lt;dbl [500]&gt;\n 7             7 &lt;dbl [500]&gt;\n 8             8 &lt;dbl [500]&gt;\n 9             9 &lt;dbl [500]&gt;\n10            10 &lt;dbl [500]&gt;\n# ℹ 90 more rows"
  },
  {
    "objectID": "blog/posts/信頼区間を理解したい.html#データから信頼区間を計算",
    "href": "blog/posts/信頼区間を理解したい.html#データから信頼区間を計算",
    "title": "信頼区間を理解したい",
    "section": "データから信頼区間を計算",
    "text": "データから信頼区間を計算\n\n各研究者のデータの平均値、標準誤差、信頼区間を計算\n信頼区間が真値（0.5）を含むかどうかを判定\n\n\ndata2 &lt;- \n  data |&gt; \n  # 各研究者のデータの平均値、標準誤差、信頼区間を計算\n  mutate(\n    mean = map_dbl(data, \\(data) mean(data)),\n    se = map_dbl(data, \\(data) sd(data) / sqrt(length(data))),\n    lower = mean - 1.96 * se,\n    upper = mean + 1.96 * se\n  ) |&gt;  \n  # 信頼区間が0.5を含むかどうかを判定\n  mutate(\n    flg = case_when(\n      0.5 &lt; lower | upper &lt; 0.5  ~ 'Error',\n      .default = 'Not Error'\n    )\n  )\n\ndata2\n\n# A tibble: 100 × 7\n   researcher_ID data         mean      se lower upper flg      \n           &lt;int&gt; &lt;list&gt;      &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;    \n 1             1 &lt;dbl [500]&gt; 0.502 0.00453 0.493 0.511 Not Error\n 2             2 &lt;dbl [500]&gt; 0.495 0.00473 0.486 0.505 Not Error\n 3             3 &lt;dbl [500]&gt; 0.500 0.00452 0.491 0.509 Not Error\n 4             4 &lt;dbl [500]&gt; 0.497 0.00479 0.488 0.506 Not Error\n 5             5 &lt;dbl [500]&gt; 0.501 0.00464 0.492 0.510 Not Error\n 6             6 &lt;dbl [500]&gt; 0.502 0.00459 0.493 0.511 Not Error\n 7             7 &lt;dbl [500]&gt; 0.509 0.00476 0.500 0.518 Not Error\n 8             8 &lt;dbl [500]&gt; 0.494 0.00451 0.486 0.503 Not Error\n 9             9 &lt;dbl [500]&gt; 0.499 0.00440 0.491 0.508 Not Error\n10            10 &lt;dbl [500]&gt; 0.497 0.00446 0.488 0.505 Not Error\n# ℹ 90 more rows"
  },
  {
    "objectID": "blog/posts/信頼区間を理解したい.html#各研究者の信頼区間を図示",
    "href": "blog/posts/信頼区間を理解したい.html#各研究者の信頼区間を図示",
    "title": "信頼区間を理解したい",
    "section": "各研究者の信頼区間を図示",
    "text": "各研究者の信頼区間を図示\n\n点推定値は0.5に近いのもあれば、遠いのもある\n信頼区間が真値（0.5）を含まない、「不運」な研究者は、5人 / 100人（5％）\n\n95％の研究者は真の値を拾える、という意味\n\n実際には自分が「不運」な研究者かもしれないし、信頼区間の端ギリギリで真値を拾っているだけかもしれない\n\n\n# 図示\ndata2 |&gt; \n  ggplot(aes(mean, researcher_ID, xmin = lower, xmax = upper, color = flg))+\n  geom_vline(xintercept = 0.5, linetype = 'dashed')+\n  geom_pointrange()+\n  theme_bw()+\n  theme(legend.position = 'bottom')"
  },
  {
    "objectID": "blog/posts/how_to_change_BLAS.html",
    "href": "blog/posts/how_to_change_BLAS.html",
    "title": "BLASの変更によるRの高速化",
    "section": "",
    "text": "Do GPU-based Basic Linear Algebra Subprograms (BLAS) improve the performance of standard modeling techniques in R?\nEven faster matrix math in R on macOS with M1\nNVBLASを使って「R」の並列演算処理を高速化"
  },
  {
    "objectID": "blog/posts/how_to_change_BLAS.html#参考",
    "href": "blog/posts/how_to_change_BLAS.html#参考",
    "title": "BLASの変更によるRの高速化",
    "section": "",
    "text": "Do GPU-based Basic Linear Algebra Subprograms (BLAS) improve the performance of standard modeling techniques in R?\nEven faster matrix math in R on macOS with M1\nNVBLASを使って「R」の並列演算処理を高速化"
  },
  {
    "objectID": "blog/posts/how_to_change_BLAS.html#概要",
    "href": "blog/posts/how_to_change_BLAS.html#概要",
    "title": "BLASの変更によるRの高速化",
    "section": "概要",
    "text": "概要\nR言語の計算速度はそれほど速くはないが、いくつか工夫することで高速化が可能。\n\nベクトル演算の駆使\n並列化（paralell, future, furrrなど）\nBLAS (Basic Linear Algebra Subprograms) の変更\n\n今回はBLASの変更による高速化について。"
  },
  {
    "objectID": "blog/posts/how_to_change_BLAS.html#blasとは",
    "href": "blog/posts/how_to_change_BLAS.html#blasとは",
    "title": "BLASの変更によるRの高速化",
    "section": "BLASとは",
    "text": "BLASとは\nBLASは、行列演算を高速に行うためのライブラリ。たとえば分散や共分散を計算するときは、行列の掛け算をする必要がある。機械学習やニューラルネットワークでは行列の掛け算が多用されるため、BLASの高速化は全体としての計算速度の向上に大きく貢献する。\nBLASはオープンソースのものもあるが、各CPU、GPUメーカーがそれぞれのチップに適したライブラリを提供している。\n\nRについてるBLAS\n\nRにデフォルトでついてくるBLAS\n\nOpenBLAS\n\nオープンソースのBLASで、homebrewでインストール可能\npythonのnumpyで使われているらしい\n\nATLAS\n\nこちらもオープンソース\n\nMKL\n\nIntelが提供するBLAS\nMacだと使えない\n\nNVBLAS\n\nNVIDIAが提供するBLAS\nGPUを使用するのでめちゃくちゃ速いらしい\nNVIDIAのGPUじゃないと使えない…\n\nAccelerate\n\nAppleが提供\n今回使用するやつ"
  },
  {
    "objectID": "blog/posts/how_to_change_BLAS.html#blasの変更",
    "href": "blog/posts/how_to_change_BLAS.html#blasの変更",
    "title": "BLASの変更によるRの高速化",
    "section": "BLASの変更",
    "text": "BLASの変更\n以下、homebrewでRをインストールしていることを前提とする。\nterminalで以下のコマンドを実行する。\ncd /Library/Frameworks/R.framework/Resources/lib\nln -sf libRblas.veclib.dylib libRblas.dylib\n意味としては、RはlibRblas.dylibをBLASとして認識するので、libRblas.veclib.dylibをlibRblas.dylibにリンクすることで、libRblas.veclib.dylibをBLASとして使うようにする。\nもどすときはlibRblas.veclib.dylibの代わりに、libRblas.0.dylib（デフォルトのBLAS）をリンクすればよい。\ncd /Library/Frameworks/R.framework/Resources/lib\nln -sf libRblas.0.dylib libRblas.dylib"
  },
  {
    "objectID": "blog/posts/複利計算.html",
    "href": "blog/posts/複利計算.html",
    "title": "複利計算",
    "section": "",
    "text": "library(tidyverse)"
  },
  {
    "objectID": "blog/posts/複利計算.html#複利計算",
    "href": "blog/posts/複利計算.html#複利計算",
    "title": "複利計算",
    "section": "複利計算",
    "text": "複利計算\n\n年間\\(a\\)万円ずつ積み立てることを考える\n年利は\\(r\\)とする\n\n\\[\\begin{align*}\n& 1年目:ar^1 \\\\\n& 2年目:ar^1 + ar^2 \\\\\n& 3年目:ar^1 + ar^2 + ar^3 \\\\\n& n年目:\\sum_{i=1}^{n} ar^i = \\frac{ar(1 - r^n)}{1 - r}\n\\end{align*}\\]\n\n等比級数の和の導出\n\n\\[\\begin{align*}\nS =& ar + ar^2 + ar^3 + ... + ar^n \\\\\nrS =& \\qquad\\; ar^2 + ar^3 + ... + ar^n + ar^{n+1} \\\\\nS - rS =& ar - ar^{n+1} \\\\\nS(1 - r) =& ar(1 - r^n) \\\\\nS =& \\frac{ar(1 - r^n)}{1 - r}\n\\end{align*}\\]\n\nシミュレーション\n\n\\(n\\)：30年積み立てる\n\\(a\\)：年間40万円ずつ（月あたり3.3333万円）\n\\(r\\)：年利は5％と仮定\n\n\n\nn &lt;- 30\na &lt;- 40\nr &lt;- 1.05\n\ntibble(\n  year = 1:n,\n  cost = a*year,\n  gain = a*r^year,\n  return = a*r*(1 - r^year)/(1 - r),\n) |&gt; \n  ggplot()+\n  geom_line(aes(x = year, y = return), color = 'red')+\n  geom_col(aes(x = year, y = cost), alpha = .7)+\n  geom_line(aes(x = year, y = cost), color = 'blue')"
  },
  {
    "objectID": "blog/posts/複利計算.html#利率が年度ごとに変動する場合",
    "href": "blog/posts/複利計算.html#利率が年度ごとに変動する場合",
    "title": "複利計算",
    "section": "利率が年度ごとに変動する場合",
    "text": "利率が年度ごとに変動する場合\n\n年利は確率的に変動する値\n\n定数（\\(r\\)）ではなく、変数（\\(r_i\\)）として考える\n\n\n\\[\\begin{align*}\n& 1年目:ar_1 \\\\\n& 2年目:ar_1 + ar_1r_2\\\\\n& 3年目:ar_1 + ar_1r_2 + ar_1r_2r_3 = a(r_1 + r_1r_2 + r_1r_2r_3)\\\\\n& n年目:ar_1 + ar_1r_2 + ar_1r_2r_3 + ... + ar_1r_2r_3...r_n = a\\sum_{i=1}^{n} \\prod_{i=1}^{n}r_i\n\\end{align*}\\]\nここで、\\(r_i \\sim \\mathrm{Normal}(1.05, 0.1)\\)とする\n\ntibble(\n  year = 1:n,\n  rate = rnorm(n, r, .1),\n  cost = a*year,\n  return = a*(accumulate(rate, \\(x, y) x*y) |&gt; accumulate(\\(x, y) x + y)),\n) |&gt; \n  ggplot()+\n  geom_line(aes(x = year, y = return), color = 'red')+\n  geom_col(aes(x = year, y = cost), alpha = .7)+\n  geom_line(aes(x = year, y = cost), color = 'blue')"
  },
  {
    "objectID": "blog/posts/複利計算.html#利率がランダムウォークする場合",
    "href": "blog/posts/複利計算.html#利率がランダムウォークする場合",
    "title": "複利計算",
    "section": "利率がランダムウォークする場合",
    "text": "利率がランダムウォークする場合\n\n景気は各時点でランダムに発生するわけではない\nむしろ1時点前の状況を強く反映する\n年利がランダムウォークすると仮定する\n\n\\[\\begin{align*}\n& r_1 \\sim \\mathrm{Normal}(1.05, 0.1) \\\\\n& r_{i + 1} = r_i + e_i, \\; e_i \\sim \\mathrm{Normal}(0, 0.01)\n\\end{align*}\\]\nパラメータの設定が難しい…\n\ntibble(\n  year = 1:n,\n  error = rnorm(n, 0, 0.01),\n  rate = accumulate(error, \\(x, y) x + y, .init = rnorm(1, r, .1))[-n],\n  cost = a*year,\n  return = a*(accumulate(rate, \\(x, y) x*y) |&gt; accumulate(\\(x, y) x + y)),\n) |&gt; \n  ggplot()+\n  geom_line(aes(x = year, y = return), color = 'red')+\n  geom_col(aes(x = year, y = cost), alpha = .7)+\n  geom_line(aes(x = year, y = cost), color = 'blue')"
  },
  {
    "objectID": "blog/posts/新しいパソコンを買ったらやること.html",
    "href": "blog/posts/新しいパソコンを買ったらやること.html",
    "title": "新しいパソコンを買ったらやること",
    "section": "",
    "text": "Office365のインストール\nOnedriveのインストール\n\n特に問題なくできた\n\n\n\n\n定番のパッケージマネージャー\n\n[https://brew.sh/index_ja:embed:cite]\n\n\n\n\nR本体のインストール\n\nrigというツールを使う。参考\nbrew tap r-lib/rig\nbrew install --cask rig \nrig add 4.3.3\n\nRstudioのインストール\n\nbrew install --cask rstudio\n\n自作したRstudioテーマを持ってくる（あとでgithubにでも置いておく）\nエディタで使用するフォントを選択\n\nmyricaを使いたかったが，なぜかMacだとうまくいっていない（調査中）\n\nRtools（Windows）or Xcode（Mac）のインストール\n\ngithub産のパッケージのビルドなどに必要\n\n（Macのみ）Xquartzのインストール\n\n詳しくはわからないが，入れないとRmarkdownがちゃんと動かなかった\n\n\n\n\n\n\n（winのみ）gitをインストール\n\nmacだとgit自体がすでにインストールされていた\n\nssh keyの作成→githubへ登録\ngit clone で適宜必要なプロジェクトをダウンロード\n\n\n\n\n\ntinytexのインストール\ntinytex::install_tinytex()でtex本体をインストール\nlatex_engine: lualatexの文章を作成し，エラーを見ながら適宜tinytex::tlmgr_install()で足りないパッケージを追加\n\n（Mac）haranoajiで引っかかった\n\n\n\n\n\n\n同期の開始\n\n「環境設定」の「同期」からユーザー名，パスワードを打ち込んで同期を開始\n「詳細」→「ファイルとフォルダ」から，基本ディレクトリを正しく設定する→pdfファイルと文献情報が紐づいた形で設定が引き継がれる\n\n\nあとは以下を参考に設定する\n[https://sickle-sword.hatenablog.com/entry/2021/06/05/160818?_ga=2.261856205.1735845236.1644808541-16137706.1644808541:embed:cite]"
  },
  {
    "objectID": "blog/posts/新しいパソコンを買ったらやること.html#office関連のインストール",
    "href": "blog/posts/新しいパソコンを買ったらやること.html#office関連のインストール",
    "title": "新しいパソコンを買ったらやること",
    "section": "",
    "text": "Office365のインストール\nOnedriveのインストール\n\n特に問題なくできた"
  },
  {
    "objectID": "blog/posts/新しいパソコンを買ったらやること.html#homebrewのインストールmac",
    "href": "blog/posts/新しいパソコンを買ったらやること.html#homebrewのインストールmac",
    "title": "新しいパソコンを買ったらやること",
    "section": "",
    "text": "定番のパッケージマネージャー\n\n[https://brew.sh/index_ja:embed:cite]"
  },
  {
    "objectID": "blog/posts/新しいパソコンを買ったらやること.html#r関連のインストール",
    "href": "blog/posts/新しいパソコンを買ったらやること.html#r関連のインストール",
    "title": "新しいパソコンを買ったらやること",
    "section": "",
    "text": "R本体のインストール\n\nrigというツールを使う。参考\nbrew tap r-lib/rig\nbrew install --cask rig \nrig add 4.3.3\n\nRstudioのインストール\n\nbrew install --cask rstudio\n\n自作したRstudioテーマを持ってくる（あとでgithubにでも置いておく）\nエディタで使用するフォントを選択\n\nmyricaを使いたかったが，なぜかMacだとうまくいっていない（調査中）\n\nRtools（Windows）or Xcode（Mac）のインストール\n\ngithub産のパッケージのビルドなどに必要\n\n（Macのみ）Xquartzのインストール\n\n詳しくはわからないが，入れないとRmarkdownがちゃんと動かなかった"
  },
  {
    "objectID": "blog/posts/新しいパソコンを買ったらやること.html#gitのインストール",
    "href": "blog/posts/新しいパソコンを買ったらやること.html#gitのインストール",
    "title": "新しいパソコンを買ったらやること",
    "section": "",
    "text": "（winのみ）gitをインストール\n\nmacだとgit自体がすでにインストールされていた\n\nssh keyの作成→githubへ登録\ngit clone で適宜必要なプロジェクトをダウンロード"
  },
  {
    "objectID": "blog/posts/新しいパソコンを買ったらやること.html#rmarkdown関連のインストール",
    "href": "blog/posts/新しいパソコンを買ったらやること.html#rmarkdown関連のインストール",
    "title": "新しいパソコンを買ったらやること",
    "section": "",
    "text": "tinytexのインストール\ntinytex::install_tinytex()でtex本体をインストール\nlatex_engine: lualatexの文章を作成し，エラーを見ながら適宜tinytex::tlmgr_install()で足りないパッケージを追加\n\n（Mac）haranoajiで引っかかった"
  },
  {
    "objectID": "blog/posts/新しいパソコンを買ったらやること.html#zoteroのインストール",
    "href": "blog/posts/新しいパソコンを買ったらやること.html#zoteroのインストール",
    "title": "新しいパソコンを買ったらやること",
    "section": "",
    "text": "同期の開始\n\n「環境設定」の「同期」からユーザー名，パスワードを打ち込んで同期を開始\n「詳細」→「ファイルとフォルダ」から，基本ディレクトリを正しく設定する→pdfファイルと文献情報が紐づいた形で設定が引き継がれる\n\n\nあとは以下を参考に設定する\n[https://sickle-sword.hatenablog.com/entry/2021/06/05/160818?_ga=2.261856205.1735845236.1644808541-16137706.1644808541:embed:cite]"
  },
  {
    "objectID": "blog/posts/tensorflow_with_Rstudio.html",
    "href": "blog/posts/tensorflow_with_Rstudio.html",
    "title": "tensorflowをRstudioで",
    "section": "",
    "text": "pythonをRStudioで使うのはreticulateパッケージによりだいぶ楽になった。\nただpythonにおけるdeep learningの代表的パッケージであるtensorflowをRStudioで使う際に少しつまづいたのでメモです。"
  },
  {
    "objectID": "blog/posts/tensorflow_with_Rstudio.html#pythonの環境をどうやって構築するか",
    "href": "blog/posts/tensorflow_with_Rstudio.html#pythonの環境をどうやって構築するか",
    "title": "tensorflowをRstudioで",
    "section": "pythonの環境をどうやって構築するか？",
    "text": "pythonの環境をどうやって構築するか？\nRではrenvパッケージを使っている。ではpythonでは？\npythonはRよりも環境が複雑で変化も早そうなので、環境構築は重要。\n今回はpython公式が推奨しているっぽいvenvを使う。\n\n\n\n\n\n\nNote\n\n\n\n最初global環境でそのままやろうとしたが上手くいかず挫折。Dockerとかも有力だと思う。"
  },
  {
    "objectID": "blog/posts/tensorflow_with_Rstudio.html#venvを使って仮想環境を用意",
    "href": "blog/posts/tensorflow_with_Rstudio.html#venvを使って仮想環境を用意",
    "title": "tensorflowをRstudioで",
    "section": "venvを使って仮想環境を用意",
    "text": "venvを使って仮想環境を用意\n\npythonがインストールされているか確認\n\nhomebrewなりでインストールしておく\nPATHを通しておく\n\n作業ディレクトリにて、terminalで以下のコマンドを実行し.venvフォルダを作成\n\n\n\n\nterminal\n\npython3 -m venv .venv"
  },
  {
    "objectID": "blog/posts/tensorflow_with_Rstudio.html#pythonライブラリをインストール",
    "href": "blog/posts/tensorflow_with_Rstudio.html#pythonライブラリをインストール",
    "title": "tensorflowをRstudioで",
    "section": "pythonライブラリをインストール",
    "text": "pythonライブラリをインストール\n\n以下のコマンドを実行し、仮想環境を起動\n\npythonの場所が.venv内のフォルダになる\n\n\n\n\n\nterminal\n\nsource .venv/bin/activate\n\n\n\n\n\n\n\n\nNote\n\n\n\nsource .venv/bin/activateは、ライブラリをインストールするときは毎回実行する必要があります！\nまちがってglobal環境を汚さないように！\n\n\n\nライブラリをインストール\n\ntensorflowのほか、numpyも必須（tensorflowをインストールしたときに入る）\nM1 macでGPUを使いたい場合はtensorflow-metalもインストールする\n\n\n\n\n\nterminal\n\npip install tensorflow-macos\npip install tensorflow-metal\n\n\n\n仮想環境を抜けるときは以下のコマンドを実行\n\n\n\n\nterminal\n\ndeactivate"
  },
  {
    "objectID": "blog/posts/tensorflow_with_Rstudio.html#rstudioでpythonを使う",
    "href": "blog/posts/tensorflow_with_Rstudio.html#rstudioでpythonを使う",
    "title": "tensorflowをRstudioで",
    "section": "RStudioでpythonを使う",
    "text": "RStudioでpythonを使う\n\npythonの場所を指定\n\nRStudioでpythonを使うには、RStudioにpythonの場所を教えてやる必要がある\n\n今回は.venv内のpythonを使いたいので、作業ディレクトリに.Rprofileを作成し、以下のコードを書く\nrenvを使っている場合は、.Rprofileが既に作成されていると思うので、その中に追記する\n\n\n\n\n\n.Rprofile\n\n# renv起動\nsource(\"renv/activate.R\")\n# pythonの場所を指定\nSys.setenv(RETICULATE_PYTHON = \".venv/bin/python3\")\n\n\n\n\n\n\n\n\nNote\n\n\n\npythonの場所の指定はrenvの起動より後に行う必要がある。もし先にpythonの場所を指定してしまうと、renvを起動したときにpythonの場所が上書きされてしまい、上手くいかなくなる。\n\n\n\n\npythonを起動\n\nRStudioでpythonのコードを実行すると、reticulate::repl_python()が走ってコンソールがpythonになる\n\n\n\n\npython\n\nimport sys\nprint(sys.version)\n\n\n3.11.10 (main, Sep  7 2024, 01:03:31) [Clang 15.0.0 (clang-1500.3.9.4)]\n\n\n\ntensorflowを起動して、GPUが使えるかを確認\nphysical_deviceでCPUとGPUの両方が表示されてればOK\n\n\n\n\npython\n\nimport tensorflow as tf\ntf.config.list_physical_devices()\n\n\n[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
  },
  {
    "objectID": "blog/posts/tensorflow_with_Rstudio.html#ニューラルネットをやってみる",
    "href": "blog/posts/tensorflow_with_Rstudio.html#ニューラルネットをやってみる",
    "title": "tensorflowをRstudioで",
    "section": "ニューラルネットをやってみる",
    "text": "ニューラルネットをやってみる\n\nチュートリアルとして、手書きのアルファベットを分類するモデルを作成してみる\nRでデータを読み込んで、pythonに渡してtensorflowで分析\n参考\n\n\n\n\nR\n\nlibrary(tidyverse)\nlibrary(rsample)\nlibrary(reticulate)\n\n\n\nデータのダウンロード\n\n\n\nR\n\n# データのダウンロード\nif (!dir.exists('data')) dir.create('data')\ndownload.file('https://ai.stanford.edu/~btaskar/ocr/letter.data.gz', destfile = 'data/letter.data')\n\n\n\n\nデータの読み込み\n\n列名はhttps://ai.stanford.edu/~btaskar/ocr/letter.namesより取得できる\n詳細は以下の通り\n\nid: each letter is assigned a unique integer id\nletter: a-z\nnext_id: id for next letter in the word, -1 if last letter\nword_id: each word is assigned a unique integer id (not used)\nposition: position of letter in the word (not used)\nfold: 0-9 – cross-validation fold\np_i_j: 0/1 – value of pixel in row i, column j\n\nletterとp_i_j（pixel）のみ使うのでそれ以外は削除\n\n\n\n\nR\n\ncolname &lt;- read_lines('https://ai.stanford.edu/~btaskar/ocr/letter.names')\ndata &lt;- \n  read_tsv('data/letter.data', col_names = colname) |&gt; \n  # letterとpixelだけ残す\n  select(id, letter, matches('p_\\\\d+_\\\\d+'))\n\n\n\n\nデータの前処理\n\nletterを数値に変換\n\n\n\n\nR\n\ndf &lt;- \n  data |&gt; \n  mutate(\n    letter = factor(letter),\n    # pythonは0からはじまるので-1する（Rは1から）\n    letter_num = as.numeric(letter) - 1\n  )\n\n\n\n訓練データとテストデータに分割\n\n\n\n\nR\n\nsplit &lt;- rsample::initial_split(df, prop = 0.9)\ntrain &lt;- rsample::training(split)\ntest &lt;- rsample::testing(split)\n\n\n\npythonに渡せる形にデータを変換\n\nreticulate::np_array()であらかじめndarrayにしておくと便利\n\n\n\n\n\nR\n\ntrain_x &lt;- \n  train |&gt; \n  select(!c(id, letter, letter_num)) |&gt;\n  as.matrix() |&gt; \n  np_array()\n\ntrain_y &lt;-\n  train |&gt;\n  pull(letter_num) |&gt;\n  np_array()\n\ntest_x &lt;-\n  test |&gt; \n  select(!c(id, letter, letter_num)) |&gt;\n  as.matrix() |&gt; \n  np_array()\n\ntest_y &lt;-\n  test |&gt;\n  pull(letter_num) |&gt;\n  np_array()\n\n\n\n\nニューラルネットで学習\n\n中間層が1つのニューラルネットを作成\n\n入力層は16x8=128（ピクセル）\n中間層は64次元\n出力層はa-zの26分類\n\n\n\n\n\npython\n\nimport tensorflow as tf\nimport numpy as np\n\n# モデルの作成\nmodel = tf.keras.Sequential([\n  tf.keras.layers.Dense(64, activation='relu'),\n  tf.keras.layers.Dense(26, activation='softmax')\n])\n\nmodel.compile(\n  optimizer='adam',\n  loss='sparse_categorical_crossentropy',\n  metrics=['accuracy']\n)\n\n\n\nrで作成したデータをpythonに渡して学習\nr.train_xでRのtrain_xを参照できる\npythonのオブジェクトを渡すときはpy$train_x\n\n\n\n\npython\n\n# モデルの学習\nmodel.fit(r.train_x, r.train_y, epochs=10, verbose=2)\n\n\nEpoch 1/10\n1467/1467 - 5s - loss: 1.3436 - accuracy: 0.6342 - 5s/epoch - 4ms/step\nEpoch 2/10\n1467/1467 - 5s - loss: 0.9296 - accuracy: 0.7519 - 5s/epoch - 3ms/step\nEpoch 3/10\n1467/1467 - 5s - loss: 0.8666 - accuracy: 0.7679 - 5s/epoch - 3ms/step\nEpoch 4/10\n1467/1467 - 5s - loss: 0.8371 - accuracy: 0.7766 - 5s/epoch - 3ms/step\nEpoch 5/10\n1467/1467 - 5s - loss: 0.8229 - accuracy: 0.7802 - 5s/epoch - 3ms/step\nEpoch 6/10\n1467/1467 - 5s - loss: 0.8151 - accuracy: 0.7819 - 5s/epoch - 3ms/step\nEpoch 7/10\n1467/1467 - 5s - loss: 0.8121 - accuracy: 0.7816 - 5s/epoch - 3ms/step\nEpoch 8/10\n1467/1467 - 5s - loss: 0.8105 - accuracy: 0.7812 - 5s/epoch - 3ms/step\nEpoch 9/10\n1467/1467 - 5s - loss: 0.8127 - accuracy: 0.7813 - 5s/epoch - 3ms/step\nEpoch 10/10\n1467/1467 - 5s - loss: 0.8137 - accuracy: 0.7803 - 5s/epoch - 3ms/step\n&lt;keras.src.callbacks.History object at 0x30efc0f90&gt;\n\n\n\n\nテストデータで評価\n\nテストデータでの正答率は77%くらい\n\n\n\n\npython\n\n# モデルの評価\nmodel.evaluate(r.test_x, r.test_y, verbose=2)\n\n\n163/163 - 0s - loss: 0.8884 - accuracy: 0.7651 - 475ms/epoch - 3ms/step\n[0.8884354829788208, 0.7651457190513611]"
  },
  {
    "objectID": "blog/posts/TMLE.html",
    "href": "blog/posts/TMLE.html",
    "title": "Targeted Maximum Likelihood Estimation / Targeted Minimum Loss-based Estimation (TMLE)",
    "section": "",
    "text": "参考ページ：https://www.khstats.com/blog/tmle/tutorial-pt2\n\n\n\n\n\nlibrary(tidyverse)\nlibrary(sl3)\nlibrary(tmle3)\nlibrary(future)\n\nkable &lt;- partial(\n  knitr::kable,\n  digits = 3\n)\n\nplan(multisession, workers = 8)\n\nset.seed(7)\n\n\n\n\n\n元のサイトでは\\(Y\\)が2値変数になっていて、推定が真値と一致しているかどうかの評価が難しいので、ここでは\\(Y\\)を連続変数としている。\nATEの真値は0.3に設定\n\n\ngenerate_data &lt;- function(n){ \n    W1 &lt;- rbinom(n, size=1, prob=0.2) # binary confounder\n    W2 &lt;- rbinom(n, size=1, prob=0.5) # binary confounder\n    W3 &lt;- round(runif(n, min=2, max=7)) # continuous confounder\n    W4 &lt;- round(runif(n, min=0, max=4)) # continuous confounder\n    A  &lt;- rbinom(n, size=1, prob= plogis(-0.2 + 0.2*W2 + log(0.1*W3) + 0.3*W4 + 0.2*W1*W4)) # binary treatment depends on confounders\n    Y &lt;- -1 + 0.3*A - 0.1*W1 + 0.2*W2 + 0.3*W3 - 0.1*W4 + sin(0.1*W2*W4) # continuous outcome depends on confounders\n    return(tibble(Y, W1, W2, W3, W4, A))\n}\n\nn &lt;- 1000\ndat_obs &lt;- generate_data(n) # generate a data set with n observations\n\n\ndat_obs |&gt; \n  summarise(samplemean = mean(Y), .by = A)\n\n# A tibble: 2 × 2\n      A samplemean\n  &lt;int&gt;      &lt;dbl&gt;\n1     1      0.686\n2     0      0.237\n\n\n\n\n\n\nglm、Lasso、Random forest、Multivariate adaptive regression splineをスタッキング\n\n\nsl_libs &lt;-\n  Lrnr_sl$new(\n    learners = Stack$new(\n      Lrnr_glm$new(), \n      Lrnr_glmnet$new(alpha = 1), \n      Lrnr_ranger$new(num.trees = 2000, max.depth = 3), \n      Lrnr_earth$new()\n    )\n  )\n\n\n\n\n\n\n\n\\[\n{\\mathrm E}[Y | A, W]\n\\]\n\ntask &lt;- \n  sl3_Task$new(\n    dat_obs, \n    covariates = select(dat_obs, !Y) |&gt; names(), \n    outcome = 'Y', \n    outcome_type = 'continuous', \n    folds = 8L\n  )\n\ntask_A1 &lt;- \n  sl3_Task$new(\n    dat_obs |&gt; mutate(A = 1), \n    covariates = select(dat_obs, !Y) |&gt; names(), \n    outcome = 'Y', \n    outcome_type = 'continuous', \n    folds = 8L\n  )\n\ntask_A0 &lt;- \n  sl3_Task$new(\n    dat_obs |&gt; mutate(A = 0), \n    covariates = select(dat_obs, !Y) |&gt; names(), \n    outcome = 'Y', \n    outcome_type = 'continuous', \n    folds = 8L\n  )\n\n# 全サンプルで学習\nsl_fit &lt;- sl_libs$train(task)\n\n\n以下の3つの予測値を算出\n\\(A := 1\\)は全サンプルでAを1にする（\\(A = 1\\)はサンプルのうちのAが1となる部分集団）\n\n\\[\\begin{align}\n&{\\mathrm E}[Y | A, W] \\\\\n&{\\mathrm E}[Y | A := 1, W] \\\\\n&{\\mathrm E}[Y | A := 0, W]\n\\end{align}\\]\n\ndat_tmle1 &lt;- \n  dat_obs |&gt; \n  mutate(\n    # 観測サンプルについての予測値\n    Q_A = sl_fit$predict(task),\n    # 全てのサンプルでA = 1に固定したときの予測値\n    Q_A1 = sl_fit$predict(task_A1),\n    # 全てのサンプルでA = 0に固定した時の予測値\n    Q_A0 = sl_fit$predict(task_A0)\n  )\n\n\nstandardization（g-computation）によるATE\n\n\\[\nATE_{g \\mathrm{-}comp} = {\\mathrm E}[ {\\mathrm E}[Y | A := 1, W] - {\\mathrm E}[Y | A := 0, W]]\n\\]\n\ndat_tmle1 |&gt; \n  summarise(ATE = mean(Q_A1 - Q_A0))\n\n# A tibble: 1 × 1\n    ATE\n  &lt;dbl&gt;\n1 0.300\n\n\n\n\n\n\n\n傾向スコアを機械学習モデルにより予測\n\n\\[\n\\mathrm{Pr}(A = 1 | W)\n\\]\n\ntask_g &lt;- \n  sl3_Task$new(\n    data = dat_obs, \n    covariates = select(dat_obs, !c(Y, A)) |&gt; names(), \n    outcome = 'A', \n    outcome_type = 'binomial', \n    folds = 8\n  )\n\nsl_fit_g &lt;- sl_libs$train(task_g)\n\n\n\n\n傾向スコアからClever Covariateと呼ばれる情報を作成（IPWに似ている）\n\n\\[\\begin{align}\n&H(A,W) &= \\frac{A}{\\mathrm{Pr}(A = 1 | W)} - \\frac{1 - A}{1 - \\mathrm{Pr}(A = 1 | W)} \\\\\n&H(1,W) &= \\frac{A}{\\mathrm{Pr}(A = 1 | W)} \\\\\n&H(0,W) &= - \\frac{1 - A}{1 - \\mathrm{Pr}(A = 1 | W)}\n\\end{align}\\]\n\ndat_tmle2 &lt;- \n  dat_tmle1 |&gt; \n  mutate(\n    # Propensity Scoreの予測\n    ps = sl_fit_g$predict(task_g),\n    # ipw (Inverse Probability Weight)\n    ipw = case_when(\n      A == 1 ~ 1 / ps, \n      A == 0 ~ 1 / (1 - ps)\n    ),\n    # Clever Covariates\n    H_A = case_when(\n      A == 1 ~ 1 / ps,\n      A == 0 ~ -1 / (1 - ps)\n    ),\n    H_A1 = case_when(\n      A == 1 ~ H_A,\n      A == 0 ~ 0\n    ),\n    H_A0 = case_when(\n      A == 1 ~ 0,\n      A == 0 ~ H_A\n    )\n  )\n\n\nIPWによるATE\n\n\\[\nATE_{ipw} = {\\mathrm E}[\\frac{A}{\\mathrm{Pr}(A = 1 | W)}Y - \\frac{1 - A}{1 - \\mathrm{Pr}(A = 1 | W)}Y]\n\\]\n\ndat_tmle2 |&gt; \n  summarise(CFmean = sum(Y*ipw) / sum(ipw), .by = A) |&gt; \n  arrange(A) |&gt; \n  summarise(ATE = diff(CFmean))\n\n# A tibble: 1 × 1\n    ATE\n  &lt;dbl&gt;\n1 0.298\n\n\n\nAugumented IPWによるATE\n\n（ほんとは関数推定時にcross-fitをする）\n\n\n\\[\nATE_{aipw} = \\mathrm E[\\mathrm E[Y | A := 1, W] - \\mathrm E[Y | A := 0, W] + \\frac{A}{\\mathrm Pr(A = 1 | W)}(Y - {\\mathrm E}[Y | A := 1, W]) - \\frac{1 - A}{1 - \\mathrm Pr(A = 1 | W)}(Y - \\mathrm E[Y | A := 0, W])]\n\\]\n\ndat_tmle2 |&gt; \n  summarise(ATE = mean(Q_A1 - Q_A0 + ipw*A*(Y - Q_A1) - ipw*(1 - A)*(Y - Q_A0)))\n\n# A tibble: 1 × 1\n    ATE\n  &lt;dbl&gt;\n1 0.300\n\n\n\n\n\n\n\nAIPWの問題点：統計的最適化がターゲットのパラメータ（ATE）に対してではなく、母平均関数\\({\\mathrm E}[Y | A,W]\\)および傾向スコア関数\\(\\mathrm{Pr}(A = 1 | W)\\)のパラメータについて最適化されている点\n推定したいパラメータ（ATE）のEIF（Efficient Influence Function）を解くことがこのステップのポイントらしい\n具体的には、Step1で推定した\\(\\mathrm{E}[Y | A, W]\\)と、Step2で推定したClever Covariate\\(H(A, W)\\)を用いて、以下の回帰式の\\(\\epsilon\\)（変動パラメータ）を推定する\n\n\\[\nY = \\mathrm{E}[Y | A, W] + \\epsilon H(A,W)\n\\]\n\n切片が0で、Step1の推定値の係数を1に固定するために、-1とoffsetを利用する\n\n\nfit &lt;- glm(Y ~ -1 + offset(Q_A) + H_A, data = dat_tmle2, family = gaussian())\n\n\n変動パラメータの推定値\n\n\nepsilon &lt;- coef(fit)\n\nepsilon\n\n        H_A \n6.40031e-07 \n\n\n\n\n\n\n推定したepsilonと\\(Y\\)の予測値をもとに、\\(Y\\)の予測値を更新\n\n\ndat_tmle3 &lt;- \n  dat_tmle2 |&gt; \n  mutate(\n    Q_A_update = Q_A + epsilon*H_A,\n    Q_A1_update = Q_A1 + epsilon*H_A1,\n    Q_A0_update = Q_A0 + epsilon*H_A0,\n  )\n\n\n\n\n\n更新されたアウトカムの予測値を用いて、Standardizationの要領でATEを推定\n\n\ndat_tmle3 |&gt; \n  summarise(ATE = mean(Q_A1_update - Q_A0_update))\n\n# A tibble: 1 × 1\n    ATE\n  &lt;dbl&gt;\n1 0.300\n\nATE &lt;- mean(dat_tmle3$Q_A1_update - dat_tmle3$Q_A0_update)\n\n\n\n\n\nTMLEではbootstrapによらずとも標準誤差を算出できる（！）\nまずは、Influence Functionを推定する\nInfluence Function：各サンプルがどれだけATEに影響をあたえるか？\n\n\ndat_tmle4 &lt;- \n  dat_tmle3 |&gt; \n  mutate(\n    IF = (Y - Q_A_update)*H_A + Q_A1_update - Q_A0_update - ATE\n  )\n\nATEの標準誤差はIFを用いて\n\\[\nSE = \\sqrt{\\frac{\\mathrm{var}(IF)}{N}}\n\\]\n\ndat_tmle4 |&gt; \n  summarise(SE = sqrt(var(IF) / 1000))\n\n# A tibble: 1 × 1\n         SE\n      &lt;dbl&gt;\n1 0.0000217"
  },
  {
    "objectID": "blog/posts/TMLE.html#下準備",
    "href": "blog/posts/TMLE.html#下準備",
    "title": "Targeted Maximum Likelihood Estimation / Targeted Minimum Loss-based Estimation (TMLE)",
    "section": "",
    "text": "library(tidyverse)\nlibrary(sl3)\nlibrary(tmle3)\nlibrary(future)\n\nkable &lt;- partial(\n  knitr::kable,\n  digits = 3\n)\n\nplan(multisession, workers = 8)\n\nset.seed(7)\n\n\n\n\n\n元のサイトでは\\(Y\\)が2値変数になっていて、推定が真値と一致しているかどうかの評価が難しいので、ここでは\\(Y\\)を連続変数としている。\nATEの真値は0.3に設定\n\n\ngenerate_data &lt;- function(n){ \n    W1 &lt;- rbinom(n, size=1, prob=0.2) # binary confounder\n    W2 &lt;- rbinom(n, size=1, prob=0.5) # binary confounder\n    W3 &lt;- round(runif(n, min=2, max=7)) # continuous confounder\n    W4 &lt;- round(runif(n, min=0, max=4)) # continuous confounder\n    A  &lt;- rbinom(n, size=1, prob= plogis(-0.2 + 0.2*W2 + log(0.1*W3) + 0.3*W4 + 0.2*W1*W4)) # binary treatment depends on confounders\n    Y &lt;- -1 + 0.3*A - 0.1*W1 + 0.2*W2 + 0.3*W3 - 0.1*W4 + sin(0.1*W2*W4) # continuous outcome depends on confounders\n    return(tibble(Y, W1, W2, W3, W4, A))\n}\n\nn &lt;- 1000\ndat_obs &lt;- generate_data(n) # generate a data set with n observations\n\n\ndat_obs |&gt; \n  summarise(samplemean = mean(Y), .by = A)\n\n# A tibble: 2 × 2\n      A samplemean\n  &lt;int&gt;      &lt;dbl&gt;\n1     1      0.686\n2     0      0.237\n\n\n\n\n\n\nglm、Lasso、Random forest、Multivariate adaptive regression splineをスタッキング\n\n\nsl_libs &lt;-\n  Lrnr_sl$new(\n    learners = Stack$new(\n      Lrnr_glm$new(), \n      Lrnr_glmnet$new(alpha = 1), \n      Lrnr_ranger$new(num.trees = 2000, max.depth = 3), \n      Lrnr_earth$new()\n    )\n  )"
  },
  {
    "objectID": "blog/posts/TMLE.html#step1アウトカムの予測",
    "href": "blog/posts/TMLE.html#step1アウトカムの予測",
    "title": "Targeted Maximum Likelihood Estimation / Targeted Minimum Loss-based Estimation (TMLE)",
    "section": "",
    "text": "\\[\n{\\mathrm E}[Y | A, W]\n\\]\n\ntask &lt;- \n  sl3_Task$new(\n    dat_obs, \n    covariates = select(dat_obs, !Y) |&gt; names(), \n    outcome = 'Y', \n    outcome_type = 'continuous', \n    folds = 8L\n  )\n\ntask_A1 &lt;- \n  sl3_Task$new(\n    dat_obs |&gt; mutate(A = 1), \n    covariates = select(dat_obs, !Y) |&gt; names(), \n    outcome = 'Y', \n    outcome_type = 'continuous', \n    folds = 8L\n  )\n\ntask_A0 &lt;- \n  sl3_Task$new(\n    dat_obs |&gt; mutate(A = 0), \n    covariates = select(dat_obs, !Y) |&gt; names(), \n    outcome = 'Y', \n    outcome_type = 'continuous', \n    folds = 8L\n  )\n\n# 全サンプルで学習\nsl_fit &lt;- sl_libs$train(task)\n\n\n以下の3つの予測値を算出\n\\(A := 1\\)は全サンプルでAを1にする（\\(A = 1\\)はサンプルのうちのAが1となる部分集団）\n\n\\[\\begin{align}\n&{\\mathrm E}[Y | A, W] \\\\\n&{\\mathrm E}[Y | A := 1, W] \\\\\n&{\\mathrm E}[Y | A := 0, W]\n\\end{align}\\]\n\ndat_tmle1 &lt;- \n  dat_obs |&gt; \n  mutate(\n    # 観測サンプルについての予測値\n    Q_A = sl_fit$predict(task),\n    # 全てのサンプルでA = 1に固定したときの予測値\n    Q_A1 = sl_fit$predict(task_A1),\n    # 全てのサンプルでA = 0に固定した時の予測値\n    Q_A0 = sl_fit$predict(task_A0)\n  )\n\n\nstandardization（g-computation）によるATE\n\n\\[\nATE_{g \\mathrm{-}comp} = {\\mathrm E}[ {\\mathrm E}[Y | A := 1, W] - {\\mathrm E}[Y | A := 0, W]]\n\\]\n\ndat_tmle1 |&gt; \n  summarise(ATE = mean(Q_A1 - Q_A0))\n\n# A tibble: 1 × 1\n    ATE\n  &lt;dbl&gt;\n1 0.300"
  },
  {
    "objectID": "blog/posts/TMLE.html#step2処置確率傾向スコアの予測",
    "href": "blog/posts/TMLE.html#step2処置確率傾向スコアの予測",
    "title": "Targeted Maximum Likelihood Estimation / Targeted Minimum Loss-based Estimation (TMLE)",
    "section": "",
    "text": "傾向スコアを機械学習モデルにより予測\n\n\\[\n\\mathrm{Pr}(A = 1 | W)\n\\]\n\ntask_g &lt;- \n  sl3_Task$new(\n    data = dat_obs, \n    covariates = select(dat_obs, !c(Y, A)) |&gt; names(), \n    outcome = 'A', \n    outcome_type = 'binomial', \n    folds = 8\n  )\n\nsl_fit_g &lt;- sl_libs$train(task_g)\n\n\n\n\n傾向スコアからClever Covariateと呼ばれる情報を作成（IPWに似ている）\n\n\\[\\begin{align}\n&H(A,W) &= \\frac{A}{\\mathrm{Pr}(A = 1 | W)} - \\frac{1 - A}{1 - \\mathrm{Pr}(A = 1 | W)} \\\\\n&H(1,W) &= \\frac{A}{\\mathrm{Pr}(A = 1 | W)} \\\\\n&H(0,W) &= - \\frac{1 - A}{1 - \\mathrm{Pr}(A = 1 | W)}\n\\end{align}\\]\n\ndat_tmle2 &lt;- \n  dat_tmle1 |&gt; \n  mutate(\n    # Propensity Scoreの予測\n    ps = sl_fit_g$predict(task_g),\n    # ipw (Inverse Probability Weight)\n    ipw = case_when(\n      A == 1 ~ 1 / ps, \n      A == 0 ~ 1 / (1 - ps)\n    ),\n    # Clever Covariates\n    H_A = case_when(\n      A == 1 ~ 1 / ps,\n      A == 0 ~ -1 / (1 - ps)\n    ),\n    H_A1 = case_when(\n      A == 1 ~ H_A,\n      A == 0 ~ 0\n    ),\n    H_A0 = case_when(\n      A == 1 ~ 0,\n      A == 0 ~ H_A\n    )\n  )\n\n\nIPWによるATE\n\n\\[\nATE_{ipw} = {\\mathrm E}[\\frac{A}{\\mathrm{Pr}(A = 1 | W)}Y - \\frac{1 - A}{1 - \\mathrm{Pr}(A = 1 | W)}Y]\n\\]\n\ndat_tmle2 |&gt; \n  summarise(CFmean = sum(Y*ipw) / sum(ipw), .by = A) |&gt; \n  arrange(A) |&gt; \n  summarise(ATE = diff(CFmean))\n\n# A tibble: 1 × 1\n    ATE\n  &lt;dbl&gt;\n1 0.298\n\n\n\nAugumented IPWによるATE\n\n（ほんとは関数推定時にcross-fitをする）\n\n\n\\[\nATE_{aipw} = \\mathrm E[\\mathrm E[Y | A := 1, W] - \\mathrm E[Y | A := 0, W] + \\frac{A}{\\mathrm Pr(A = 1 | W)}(Y - {\\mathrm E}[Y | A := 1, W]) - \\frac{1 - A}{1 - \\mathrm Pr(A = 1 | W)}(Y - \\mathrm E[Y | A := 0, W])]\n\\]\n\ndat_tmle2 |&gt; \n  summarise(ATE = mean(Q_A1 - Q_A0 + ipw*A*(Y - Q_A1) - ipw*(1 - A)*(Y - Q_A0)))\n\n# A tibble: 1 × 1\n    ATE\n  &lt;dbl&gt;\n1 0.300"
  },
  {
    "objectID": "blog/posts/TMLE.html#step3変動パラメータの推定",
    "href": "blog/posts/TMLE.html#step3変動パラメータの推定",
    "title": "Targeted Maximum Likelihood Estimation / Targeted Minimum Loss-based Estimation (TMLE)",
    "section": "",
    "text": "AIPWの問題点：統計的最適化がターゲットのパラメータ（ATE）に対してではなく、母平均関数\\({\\mathrm E}[Y | A,W]\\)および傾向スコア関数\\(\\mathrm{Pr}(A = 1 | W)\\)のパラメータについて最適化されている点\n推定したいパラメータ（ATE）のEIF（Efficient Influence Function）を解くことがこのステップのポイントらしい\n具体的には、Step1で推定した\\(\\mathrm{E}[Y | A, W]\\)と、Step2で推定したClever Covariate\\(H(A, W)\\)を用いて、以下の回帰式の\\(\\epsilon\\)（変動パラメータ）を推定する\n\n\\[\nY = \\mathrm{E}[Y | A, W] + \\epsilon H(A,W)\n\\]\n\n切片が0で、Step1の推定値の係数を1に固定するために、-1とoffsetを利用する\n\n\nfit &lt;- glm(Y ~ -1 + offset(Q_A) + H_A, data = dat_tmle2, family = gaussian())\n\n\n変動パラメータの推定値\n\n\nepsilon &lt;- coef(fit)\n\nepsilon\n\n        H_A \n6.40031e-07"
  },
  {
    "objectID": "blog/posts/TMLE.html#step4アウトカムの予測値を更新",
    "href": "blog/posts/TMLE.html#step4アウトカムの予測値を更新",
    "title": "Targeted Maximum Likelihood Estimation / Targeted Minimum Loss-based Estimation (TMLE)",
    "section": "",
    "text": "推定したepsilonと\\(Y\\)の予測値をもとに、\\(Y\\)の予測値を更新\n\n\ndat_tmle3 &lt;- \n  dat_tmle2 |&gt; \n  mutate(\n    Q_A_update = Q_A + epsilon*H_A,\n    Q_A1_update = Q_A1 + epsilon*H_A1,\n    Q_A0_update = Q_A0 + epsilon*H_A0,\n  )"
  },
  {
    "objectID": "blog/posts/TMLE.html#step5推定したい統計量を推定",
    "href": "blog/posts/TMLE.html#step5推定したい統計量を推定",
    "title": "Targeted Maximum Likelihood Estimation / Targeted Minimum Loss-based Estimation (TMLE)",
    "section": "",
    "text": "更新されたアウトカムの予測値を用いて、Standardizationの要領でATEを推定\n\n\ndat_tmle3 |&gt; \n  summarise(ATE = mean(Q_A1_update - Q_A0_update))\n\n# A tibble: 1 × 1\n    ATE\n  &lt;dbl&gt;\n1 0.300\n\nATE &lt;- mean(dat_tmle3$Q_A1_update - dat_tmle3$Q_A0_update)"
  },
  {
    "objectID": "blog/posts/TMLE.html#step6標準誤差の推定",
    "href": "blog/posts/TMLE.html#step6標準誤差の推定",
    "title": "Targeted Maximum Likelihood Estimation / Targeted Minimum Loss-based Estimation (TMLE)",
    "section": "",
    "text": "TMLEではbootstrapによらずとも標準誤差を算出できる（！）\nまずは、Influence Functionを推定する\nInfluence Function：各サンプルがどれだけATEに影響をあたえるか？\n\n\ndat_tmle4 &lt;- \n  dat_tmle3 |&gt; \n  mutate(\n    IF = (Y - Q_A_update)*H_A + Q_A1_update - Q_A0_update - ATE\n  )\n\nATEの標準誤差はIFを用いて\n\\[\nSE = \\sqrt{\\frac{\\mathrm{var}(IF)}{N}}\n\\]\n\ndat_tmle4 |&gt; \n  summarise(SE = sqrt(var(IF) / 1000))\n\n# A tibble: 1 × 1\n         SE\n      &lt;dbl&gt;\n1 0.0000217"
  },
  {
    "objectID": "blog/posts/Borusyak_2024.html",
    "href": "blog/posts/Borusyak_2024.html",
    "title": "Borusyak et al. (2024)",
    "section": "",
    "text": "Borusyak, Kirill, Xavier Jaravel & Jann Spiess, 2024, “Revisiting Event-Study Designs: Robust and Efficient Estimation,” The Review of Economic Studies, (10.1093/restud/rdae007)."
  },
  {
    "objectID": "blog/posts/Borusyak_2024.html#文献",
    "href": "blog/posts/Borusyak_2024.html#文献",
    "title": "Borusyak et al. (2024)",
    "section": "",
    "text": "Borusyak, Kirill, Xavier Jaravel & Jann Spiess, 2024, “Revisiting Event-Study Designs: Robust and Efficient Estimation,” The Review of Economic Studies, (10.1093/restud/rdae007)."
  },
  {
    "objectID": "blog/posts/Borusyak_2024.html#ざっくりとした要約",
    "href": "blog/posts/Borusyak_2024.html#ざっくりとした要約",
    "title": "Borusyak et al. (2024)",
    "section": "ざっくりとした要約",
    "text": "ざっくりとした要約\nDiD推定をg-computationのようなアイデアで行う手法を提案した論文です。従来のTwo-way fixed effectによる推定では問題が生じることが指摘されてきた、処置のタイミングによる効果の異質性（staggerd DiD design）の問題などに対処しています。通常g-computationによる推定量の標準誤差はBootstrapで計算することが多いと思うのですが、この論文では推定量の漸近正規性などを示しつつ、Bootstrapを使わずに標準誤差を計算する方法を提案している点が興味深いです。"
  },
  {
    "objectID": "blog/posts/Borusyak_2024.html#estimand",
    "href": "blog/posts/Borusyak_2024.html#estimand",
    "title": "Borusyak et al. (2024)",
    "section": "Estimand",
    "text": "Estimand\n\n\\(Y_{it}(1), Y_{it}(0)\\)：個人\\(i\\)、時点\\(t\\)における介入を受けた場合と受けていない場合のpotential outcome\n\\(D_{it}\\)：介入を受けたかどうかのバイナリ変数\n\n個人レベルの因果効果\n\\[\\begin{align*}\n\\tau_{it} = Y_{it}(1) - Y_{it}(0) = Y_{it} - Y_{it}(0)\n\\end{align*}\\]\nこれを関心のある集団について平均する\n\nATT：\\(\\mathrm{E}[\\tau_{it}|D_{it} = 1] = \\mathrm{E}[Y_{it} - Y_{it}(0) | D_{it} = 1]\\)\nevent study：\\(\\mathrm{E}[\\tau_{it}|D_{it} = 1, T = t] = \\mathrm{E}[Y_{it} - Y_{it}(0) | D_{it} = 1, T = t]\\)\n特定の集団におけるATT：\\(\\mathrm{E}[\\tau_{it}|D_{it} = 1, Gender] = \\mathrm{E}[Y_{it} - Y_{it}(0) | D_{it} = 1, Gender]\\)\n\n論文ではウェイトを使った書き方をしているのでややこしいが、おそらくはこういうことだと思われる。"
  },
  {
    "objectID": "blog/posts/Borusyak_2024.html#識別",
    "href": "blog/posts/Borusyak_2024.html#識別",
    "title": "Borusyak et al. (2024)",
    "section": "識別",
    "text": "識別\n因果効果を識別するための仮定は以下の通り\n\nGeneral moodel of \\(Y(0)\\)：\\(\\mathrm{E}[Y_{it}(0)] = A'_{it}\\lambda_i + X'_{it}\\delta\\)の形で書けること\n\n\n\\(A'_{it}\\lambda_i\\)：個体の固定効果＋固定効果と共変量の交互作用\n\\(X'_{it}\\delta\\)：時点の固定効果＋時変の共変量\n要するに\\(\\mathrm{E}[Y_{it}(0)]\\)が適切のモデル化できていること\n\n\nNo-anticipation effects：\\(Y_{it} = Y_{it}(0) \\ if \\ D_{it} = 0\\)\n\n\nConsistencyと同じ？\n\n\nModel of causal effects\n\n\n効果の異質性がパラメトリックモデルで書けること（？）\n複雑すぎるとダメということ？このあたりちょっとわからなかったです"
  },
  {
    "objectID": "blog/posts/Borusyak_2024.html#推定",
    "href": "blog/posts/Borusyak_2024.html#推定",
    "title": "Borusyak et al. (2024)",
    "section": "推定",
    "text": "推定\n\n\\(\\mathrm{E}[Y_{it}(0)]\\)の推定\n\n\n処置を受けていないサンプル（\\(D_{it} = 0\\)）を用いて、以下のモデルをOLSで推定\n\n\\[\nY_{it} = A'_{it}\\lambda_i + X'_{it}\\delta + \\varepsilon_{it}\n\\]\n\n\\(\\hat{Y}_{it}(0) | D_{it} = 1\\) の推定\n\n\n推定したモデルに、処置を受けたサンプル（\\(D_{it} = 1\\)）を代入し、処置を受けたサンプルにおいて、処置を受けていなかった場合のpotential outcomeを予測\n\n\n\\(\\hat{\\tau}_{it}\\)の推定\n\n\n処置を受けたサンプルの、観察データ\\(Y_{it}\\)と予測したpotential outcome\\(\\hat{Y}_{it}(0)\\)の差を取る\n\n\n関心のある集団について平均値をとる\n\n\\[\n\\mathrm{ATT} = \\mathrm{E}[\\hat{\\tau}_{it}|D_{it} = 1]\n\\]\n標準誤差の推定も説明があるが、理解できなかった…"
  },
  {
    "objectID": "blog/posts/Borusyak_2024.html#分析例",
    "href": "blog/posts/Borusyak_2024.html#分析例",
    "title": "Borusyak et al. (2024)",
    "section": "分析例",
    "text": "分析例\n著者によるStataパッケージの他、RではKyle Buttsによるdidimputationパッケージがある。\nここではdidimputationパッケージのトイデータを用いて、パッケージによる結果と、手計算による結果を比較する。\n\n# remotes::install_github('kylebutts/didimputation')\nlibrary(tidyverse)\nlibrary(fixest)\nlibrary(didimputation)"
  },
  {
    "objectID": "blog/posts/Borusyak_2024.html#パッケージの結果",
    "href": "blog/posts/Borusyak_2024.html#パッケージの結果",
    "title": "Borusyak et al. (2024)",
    "section": "パッケージの結果",
    "text": "パッケージの結果\n\ndid_imputation(data = df_het, yname = \"dep_var\", gname = \"g\", tname = \"year\", idname = \"unit\") |&gt; \n  knitr::kable()\n\n\n\n\nterm\nestimate\nstd.error\nconf.low\nconf.high\n\n\n\n\ntreat\n2.262952\n0.0313968\n2.201414\n2.32449"
  },
  {
    "objectID": "blog/posts/Borusyak_2024.html#手計算",
    "href": "blog/posts/Borusyak_2024.html#手計算",
    "title": "Borusyak et al. (2024)",
    "section": "手計算",
    "text": "手計算\n\n# OLSで処置を受けていないサンプルにおけるモデルを推定\nmodel &lt;- feols(dep_var ~ 1 | year + unit, data = df_het |&gt; filter(!treat), combine.quick = TRUE)\n\nprediction &lt;- \n  # 処置を受けた集団の予測値（potential outcome）を計算\n  broom::augment(model, newdata = df_het |&gt; filter(treat)) |&gt; \n  # 差分を計算\n  mutate(tau = dep_var - .fitted) \n\n# 集計\nprediction |&gt; \n  summarise(\n    ATT = mean(tau)\n  ) |&gt; \n  knitr::kable()\n\n\n\n\nATT\n\n\n\n\n2.262952\n\n\n\n\n\nATTの点推定値は一致する。"
  },
  {
    "objectID": "blog/posts/Borusyak_2024.html#集団による異質性",
    "href": "blog/posts/Borusyak_2024.html#集団による異質性",
    "title": "Borusyak et al. (2024)",
    "section": "集団による異質性",
    "text": "集団による異質性\n次に、処置のタイミングにより効果が異なる場合の分析を考える。\nパッケージでは以下のように、処置のタイミングが同じグループごとにウェイトを作成してやることで、集団ごとの効果を推定できる。\n\ndf_het |&gt; \n  mutate(\n    group1 = case_when(\n      group == 'Group 1' & treat ~ 1,\n      .default = 0\n    ),\n    group2 = case_when(\n      group == 'Group 2' & treat ~ 1,\n      .default = 0\n    )\n  ) |&gt; \n  did_imputation(yname = \"dep_var\", gname = \"g\", tname = \"year\", idname = \"unit\", wtr = c('group1', 'group2')) |&gt; \n  knitr::kable()\n\n\n\n\nterm\nestimate\nstd.error\nconf.low\nconf.high\n\n\n\n\ngroup1\n2.513439\n0.0381686\n2.438629\n2.588249\n\n\ngroup2\n1.795048\n0.0389609\n1.718685\n1.871412\n\n\n\n\n\n手計算では、集計をグループごとに行えばよい。\n\n# 集計\nprediction |&gt; \n  summarise(\n    ATT = mean(tau),\n    .by = c(group)\n  ) |&gt; \n  knitr::kable()\n\n\n\n\ngroup\nATT\n\n\n\n\nGroup 1\n2.513439\n\n\nGroup 2\n1.795048\n\n\n\n\n\nこちらも点推定値は一致する。"
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Research",
    "section": "",
    "text": "鎌田健太郎，2024，「初年次教育の導入・改革は学生の学習を促進したか——国立総合大学における因果効果の検討」『教育学研究』91(1): 1–12．\n\n Appendix  再現コード\n\n鎌田健太郎，2022，「高等教育再拡大期における学生活動の変化——Typological Approach による学生下位集団の時代比較」『高等教育研究』25: 257–74．"
  },
  {
    "objectID": "research.html#査読付き論文",
    "href": "research.html#査読付き論文",
    "title": "Research",
    "section": "",
    "text": "鎌田健太郎，2024，「初年次教育の導入・改革は学生の学習を促進したか——国立総合大学における因果効果の検討」『教育学研究』91(1): 1–12．\n\n Appendix  再現コード\n\n鎌田健太郎，2022，「高等教育再拡大期における学生活動の変化——Typological Approach による学生下位集団の時代比較」『高等教育研究』25: 257–74．"
  },
  {
    "objectID": "profile.en.html",
    "href": "profile.en.html",
    "title": "Profile",
    "section": "",
    "text": "April 2023 - Present: Research Fellow (DC1), Japan Society for the Promotion of Science (JSPS)\nApril 2021 - Present: Project Academic Specialist, Center for Social Research and Data Archives, Institute of Social Science, The University of Tokyo"
  },
  {
    "objectID": "profile.en.html#position",
    "href": "profile.en.html#position",
    "title": "Profile",
    "section": "",
    "text": "April 2023 - Present: Research Fellow (DC1), Japan Society for the Promotion of Science (JSPS)\nApril 2021 - Present: Project Academic Specialist, Center for Social Research and Data Archives, Institute of Social Science, The University of Tokyo"
  },
  {
    "objectID": "profile.en.html#education",
    "href": "profile.en.html#education",
    "title": "Profile",
    "section": "Education",
    "text": "Education\n\nApril 2023 - Present: Ph.D. Student in Division of Social Sciences in Education, Graduate School of Education, The University of Tokyo\nApril 2021 - March 2023: M.A. in Division of Social Sciences in Education, Graduate School of Education, The University of Tokyo\nApril 2019 - March 2021: B.A. in Division of Social Sciences in Education, Faculty of Education, The University of Tokyo"
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Blog",
    "section": "",
    "text": "学校外教育の参加率\n\n\n\nOthers\n\n\n\n\n\n\n\nKentaro Kamada\n\n\nNov 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n手動でSuperlearner\n\n\n\nOthers\n\n\n\n\n\n\n\nKentaro Kamada\n\n\nNov 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n信頼区間を理解したい\n\n\n\nStatistics\n\n\n\n\n\n\n\nKentaro Kamada\n\n\nSep 21, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nPIAAC、PISAの標準誤差\n\n\n\nStatistics\n\n\n\n\n\n\n\nKentaro Kamada\n\n\nSep 21, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nBorusyak et al. (2024)\n\n\n\nCausal Inference\n\n\n\n\n\n\n\nKentaro Kamada\n\n\nAug 19, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n複利計算\n\n\n\nOthers\n\n\n\n\n\n\n\nKentaro Kamada\n\n\nAug 15, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nマルチアンサーの処理\n\n\n\nOthers\n\n\n\n\n\n\n\nKentaro Kamada\n\n\nJun 5, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nバージョンを指定したRのインストール\n\n\n\nR\n\n\n\n\n\n\n\nKentaro Kamada\n\n\nApr 30, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nBLASの変更によるRの高速化\n\n\n\nR\n\n\n\n\n\n\n\nKentaro Kamada\n\n\nMar 10, 2024\n\n\n\n\n\n\n\n\n\n\n\n\ntensorflowをRstudioで\n\n\n\nR Tips\n\n\n\n\n\n\n\nKentaro Kamada\n\n\nFeb 23, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nCausal Decomposition Analysis\n\n\n\nCausal Inference\n\n\n\n\n\n\n\nKentaro Kamada\n\n\nFeb 22, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nTargeted Maximum Likelihood Estimation / Targeted Minimum Loss-based Estimation (TMLE)\n\n\n\nCausal Inference\n\n\n\n\n\n\n\nKentaro Kamada\n\n\nFeb 22, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nマルチレベルモデルは点推定値に影響を及ぼすか？\n\n\n\nOthers\n\n\n\n\n\n\n\nKentaro Kamada\n\n\nFeb 22, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nrlang学習\n\n\n\nOthers\n\n\n\n\n\n\n\nKentaro Kamada\n\n\nFeb 22, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nSudharsanan & Bijlsma (2021)\n\n\n\nCausal Inference\n\n\n\n\n\n\n\nKentaro Kamada\n\n\nFeb 22, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nLongitudinal TMLE\n\n\n\nCausal Inference\n\n\n\n\n\n\n\nKentaro Kamada\n\n\nFeb 22, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n観測度数・期待度数から対数尤度を計算する方法\n\n\n\nOthers\n\n\n\n\n\n\n\nKentaro Kamada\n\n\nFeb 22, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n学生生活実態調査の文字コード\n\n\n\nOthers\n\n\n\n\n\n\n\nKentaro Kamada\n\n\nJun 25, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n新しいパソコンを買ったらやること\n\n\n\nSetup\n\n\n\n\n\n\n\nKentaro Kamada\n\n\nFeb 14, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nZoteroの設定備忘録\n\n\n\nSetup\n\n\nZotero\n\n\n\n\n\n\n\nKentaro Kamada\n\n\nJun 5, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nRでロジットモデル\n\n\n\nStatistics\n\n\nR\n\n\n\n\n\n\n\nKentaro Kamada\n\n\nApr 29, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n移動平均のかゆいところに，partial = TRUE\n\n\n\nR\n\n\n\n\n\n\n\nKentaro Kamada\n\n\nOct 25, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nRとPython書き比べ（ユークリッドの互除法）\n\n\n\nR\n\n\nPython\n\n\n\n\n\n\n\nKentaro Kamada\n\n\nJul 31, 2020\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/posts/Causal_Decomposition.html",
    "href": "blog/posts/Causal_Decomposition.html",
    "title": "Causal Decomposition Analysis",
    "section": "",
    "text": "library(tidyverse)\nlibrary(cfdecomp)\nlibrary(gapclosing)\nlibrary(causal.decomp)\n\n\nd &lt;- \n  sMIDUS |&gt; \n  transmute(Y = health |&gt; as.numeric(),  # outcome\n            T = edu |&gt; as.numeric(),   # treatment (continuous)\n            T2 = edu |&gt; case_match(4:6 ~ 0,   # treatment (binary)\n                                   7:9 ~ 1,\n                                   .default = NA) |&gt; factor(),\n            X = racesex |&gt; factor(levels = c(\"1\", \"4\", \"2\", \"3\")),  # note!\n            L1 = lowchildSES |&gt; as.numeric(),\n            L2 = abuse |&gt; as.numeric(),\n            C1 = age |&gt; as.numeric(),\n            C2 = stroke |&gt; as.numeric(),\n            C3 = T2DM |&gt; as.numeric(),\n            C4 = heart |&gt; as.numeric()) |&gt; \n  mutate(across(L1:C4, \\(.x){.x - mean(.x, na.rm = TRUE)})) |&gt; \n  tibble()"
  },
  {
    "objectID": "blog/posts/Causal_Decomposition.html#前準備",
    "href": "blog/posts/Causal_Decomposition.html#前準備",
    "title": "Causal Decomposition Analysis",
    "section": "",
    "text": "library(tidyverse)\nlibrary(cfdecomp)\nlibrary(gapclosing)\nlibrary(causal.decomp)\n\n\nd &lt;- \n  sMIDUS |&gt; \n  transmute(Y = health |&gt; as.numeric(),  # outcome\n            T = edu |&gt; as.numeric(),   # treatment (continuous)\n            T2 = edu |&gt; case_match(4:6 ~ 0,   # treatment (binary)\n                                   7:9 ~ 1,\n                                   .default = NA) |&gt; factor(),\n            X = racesex |&gt; factor(levels = c(\"1\", \"4\", \"2\", \"3\")),  # note!\n            L1 = lowchildSES |&gt; as.numeric(),\n            L2 = abuse |&gt; as.numeric(),\n            C1 = age |&gt; as.numeric(),\n            C2 = stroke |&gt; as.numeric(),\n            C3 = T2DM |&gt; as.numeric(),\n            C4 = heart |&gt; as.numeric()) |&gt; \n  mutate(across(L1:C4, \\(.x){.x - mean(.x, na.rm = TRUE)})) |&gt; \n  tibble()"
  },
  {
    "objectID": "blog/posts/Causal_Decomposition.html#cfdecomp",
    "href": "blog/posts/Causal_Decomposition.html#cfdecomp",
    "title": "Causal Decomposition Analysis",
    "section": "cfdecomp",
    "text": "cfdecomp\n\nSudharsanan and Bijlsma (2021) の方法。mediatorの値をシミュレーションで複数生成するのが特徴\n\n\n# cfd.mean\nfit_cfdecomp &lt;-\n  cfdecomp::cfd.mean(\n    formula.y = 'Y ~ X + T + X:T + L1 + L2 + C1 + C2 + C3 + C4',\n    formula.m = 'T ~ X + C1 + C2 + C3 + C4',\n    mediator = 'T',\n    group = 'X',\n    data = d |&gt; data.frame(),\n    family.y = 'gaussian',\n    family.m = 'gaussian',\n    bs.size = 50,\n    mc.size = 10,\n    alpha = 0.05\n  )\n\n\nfit_cfdecomp\n\n$out_nc_m\n          1        4        2        3\n1  7.711633 5.888257 7.125844 6.352904\n2  7.711335 5.888175 7.067714 6.367545\n3  7.680955 5.879973 7.083105 6.290285\n4  7.731151 5.883833 7.054332 6.314513\n5  7.741197 5.919269 7.068209 6.411571\n6  7.679647 5.874477 7.071199 6.442494\n7  7.743790 5.900932 7.083522 6.434297\n8  7.718472 5.884156 7.099022 6.309715\n9  7.678637 5.804392 7.116342 6.344669\n10 7.707989 5.879045 7.084649 6.386724\n11 7.665000 5.854644 7.082168 6.404390\n12 7.687069 5.879753 7.062328 6.411204\n13 7.713308 5.892926 7.065359 6.375275\n14 7.705918 5.886559 7.081125 6.461115\n15 7.749694 5.860974 7.105595 6.348651\n16 7.738547 5.872752 7.087563 6.404910\n17 7.693606 5.850646 7.098890 6.459896\n18 7.713330 5.893088 7.091596 6.376642\n19 7.709143 5.823530 7.029086 6.413343\n20 7.733210 5.858471 7.087219 6.467162\n21 7.738161 5.911049 7.092461 6.335316\n22 7.746965 5.920713 7.089514 6.283847\n23 7.694358 5.888971 7.102488 6.363968\n24 7.693684 5.908104 7.052797 6.421523\n25 7.679553 5.875793 7.105435 6.411341\n26 7.659899 5.884256 7.051559 6.460068\n27 7.686961 5.894971 7.072704 6.403770\n28 7.710311 5.869419 7.055817 6.434180\n29 7.716763 5.903774 7.081851 6.455534\n30 7.693189 5.868528 7.081334 6.421027\n31 7.742753 5.925176 7.080192 6.333420\n32 7.751380 5.894483 7.097142 6.455502\n33 7.711411 5.809214 7.071902 6.437017\n34 7.740161 5.911441 7.105231 6.511967\n35 7.705447 5.909566 7.095954 6.380936\n36 7.702558 5.903924 7.081581 6.407733\n37 7.700893 5.881401 7.086621 6.461621\n38 7.738122 5.904513 7.063869 6.395699\n39 7.746447 5.870571 7.026998 6.484040\n40 7.759368 5.910303 7.092030 6.406555\n41 7.718450 5.831043 7.054374 6.358089\n42 7.695918 5.835081 7.075573 6.416062\n43 7.731954 5.813791 7.036193 6.421428\n44 7.712310 5.893671 7.025246 6.455531\n45 7.680701 5.846358 7.061539 6.407396\n46 7.694061 5.845573 7.072832 6.391219\n47 7.668851 5.817726 7.080903 6.406440\n48 7.711376 5.949511 7.053901 6.456014\n49 7.716736 5.826249 7.080758 6.422543\n50 7.654022 5.866163 7.131136 6.413449\n\n$out_cf_m\n          1        4        2        3\n1  7.710530 7.707118 7.729085 7.689582\n2  7.722146 7.715962 7.718530 7.694744\n3  7.685399 7.679285 7.699618 7.684752\n4  7.710964 7.722139 7.724466 7.721053\n5  7.730490 7.752241 7.751842 7.799405\n6  7.664568 7.663683 7.674068 7.663177\n7  7.724971 7.749857 7.752868 7.706792\n8  7.714129 7.716862 7.722281 7.723609\n9  7.683756 7.671424 7.702381 7.617404\n10 7.695261 7.735147 7.719256 7.703986\n11 7.670528 7.683359 7.689876 7.687277\n12 7.706871 7.707688 7.704846 7.672708\n13 7.704887 7.736651 7.703969 7.675736\n14 7.709934 7.709054 7.727723 7.682492\n15 7.740935 7.721385 7.747651 7.710687\n16 7.742387 7.735513 7.746277 7.728202\n17 7.691813 7.700624 7.705163 7.669490\n18 7.695803 7.723519 7.715414 7.705193\n19 7.714527 7.724800 7.728529 7.731613\n20 7.752440 7.731310 7.731205 7.740443\n21 7.747701 7.751927 7.746104 7.736706\n22 7.744353 7.753527 7.757974 7.699908\n23 7.698835 7.707310 7.733331 7.642548\n24 7.688320 7.678430 7.695832 7.700293\n25 7.678879 7.703020 7.682947 7.674868\n26 7.662760 7.686827 7.677796 7.669470\n27 7.667674 7.688950 7.702769 7.645594\n28 7.708490 7.720959 7.712414 7.715071\n29 7.717942 7.738635 7.736016 7.725968\n30 7.696543 7.693711 7.704273 7.674739\n31 7.732255 7.762128 7.749186 7.722042\n32 7.746046 7.742624 7.751110 7.730741\n33 7.713978 7.723498 7.716193 7.716831\n34 7.745275 7.731578 7.746356 7.749622\n35 7.705166 7.686891 7.714852 7.647096\n36 7.700198 7.701480 7.708816 7.661288\n37 7.711325 7.716404 7.731762 7.711473\n38 7.712923 7.725907 7.735120 7.730548\n39 7.750298 7.756705 7.764771 7.732673\n40 7.766789 7.768882 7.776410 7.764838\n41 7.717941 7.713504 7.723617 7.715233\n42 7.679975 7.704309 7.704923 7.687836\n43 7.737060 7.735301 7.739600 7.718730\n44 7.708164 7.721542 7.701967 7.645525\n45 7.682980 7.686698 7.681756 7.670019\n46 7.695651 7.713801 7.706790 7.662348\n47 7.665926 7.714450 7.687832 7.659873\n48 7.703820 7.719615 7.695535 7.688421\n49 7.726563 7.701374 7.736214 7.689662\n50 7.645600 7.695163 7.665097 7.640491\n\n$out_nc_quantile_m\n             1        4        2        3\n2.5%  7.661047 5.810243 7.027468 6.294657\n50%   7.711356 5.882617 7.081230 6.409468\n97.5% 7.751000 5.924172 7.123706 6.480242\n\n$out_cf_quantile_m\n             1        4        2        3\n2.5%  7.663167 7.673000 7.674907 7.640954\n50%   7.709212 7.716633 7.718893 7.697326\n97.5% 7.751958 7.760908 7.763242 7.761414\n\n$out_nc_y\n          1        4        2        3\n1  7.643030 6.732799 7.375613 6.910479\n2  7.607577 6.717165 7.346198 7.052359\n3  7.607693 6.599873 7.269480 6.873304\n4  7.610145 6.723795 7.289715 6.952498\n5  7.616292 6.678440 7.333097 7.001349\n6  7.557731 6.704395 7.250811 6.992294\n7  7.599877 6.719521 7.340502 7.077658\n8  7.628274 6.719762 7.296094 6.900450\n9  7.519645 6.578178 7.334936 6.963192\n10 7.636567 6.695734 7.262511 7.051596\n11 7.611775 6.700613 7.292579 7.049620\n12 7.631828 6.683791 7.331269 7.083689\n13 7.563892 6.674326 7.316754 6.908562\n14 7.603895 6.737479 7.327165 6.927612\n15 7.618607 6.670229 7.255864 6.985934\n16 7.622240 6.704231 7.320607 7.053559\n17 7.585641 6.734660 7.341138 6.945470\n18 7.604256 6.707089 7.302909 7.041279\n19 7.524772 6.703693 7.298052 7.103668\n20 7.553036 6.722339 7.308819 7.009432\n21 7.574128 6.759331 7.311300 6.899021\n22 7.609917 6.680900 7.350035 6.967131\n23 7.607101 6.731611 7.357399 6.922846\n24 7.620362 6.689117 7.271719 6.909727\n25 7.600291 6.721549 7.343278 7.091111\n26 7.580349 6.750467 7.322421 7.079178\n27 7.588436 6.694641 7.341734 6.990717\n28 7.573369 6.765644 7.320928 6.946950\n29 7.578826 6.691312 7.331328 6.974224\n30 7.601308 6.629131 7.267103 6.999810\n31 7.603843 6.783977 7.376238 6.952896\n32 7.603542 6.734427 7.370235 6.965581\n33 7.620574 6.695155 7.328574 6.988810\n34 7.631733 6.629085 7.316282 7.039475\n35 7.614603 6.723767 7.332842 6.990229\n36 7.587058 6.682920 7.313699 6.875552\n37 7.590790 6.698495 7.371529 7.092733\n38 7.599795 6.681993 7.317343 6.869174\n39 7.585488 6.704832 7.335817 7.077249\n40 7.609017 6.707175 7.327749 6.978417\n41 7.570631 6.681757 7.330553 6.931524\n42 7.622285 6.650356 7.320067 6.998195\n43 7.620196 6.697024 7.337561 6.933217\n44 7.625899 6.619717 7.265149 7.086506\n45 7.541506 6.675692 7.294960 6.897559\n46 7.634605 6.657319 7.288395 6.937194\n47 7.590675 6.667401 7.355573 6.968337\n48 7.640129 6.790121 7.279111 6.935163\n49 7.598317 6.712968 7.344339 7.088489\n50 7.609962 6.684222 7.339409 7.026853\n\n$out_cf_y\n          1        4        2        3\n1  7.642828 7.228655 7.472077 7.044363\n2  7.610666 7.217077 7.479253 7.210653\n3  7.608739 7.042916 7.414806 7.153437\n4  7.605145 7.217781 7.452585 7.336006\n5  7.613874 7.287004 7.518205 7.143872\n6  7.554566 7.294251 7.384723 7.342034\n7  7.595708 7.312383 7.507010 7.324459\n8  7.627264 7.353019 7.429607 7.023240\n9  7.520749 7.217301 7.498648 7.195415\n10 7.634141 7.158135 7.437276 7.380509\n11 7.613326 7.312547 7.438580 7.217391\n12 7.636896 7.233261 7.494746 7.434478\n13 7.561744 7.346572 7.457141 7.021547\n14 7.604743 7.324411 7.453311 7.211474\n15 7.616689 7.236342 7.432433 7.312738\n16 7.623216 7.294132 7.488400 7.301480\n17 7.585169 7.293564 7.492201 7.007174\n18 7.599991 7.097552 7.472330 7.256049\n19 7.525761 7.140974 7.451360 7.495060\n20 7.557790 7.315725 7.505289 7.214017\n21 7.576391 7.363599 7.451759 7.137034\n22 7.609120 7.228905 7.483233 7.328490\n23 7.608396 7.173141 7.499450 7.104582\n24 7.619150 7.134299 7.413481 7.102330\n25 7.600139 7.295098 7.489918 7.308457\n26 7.581097 7.396744 7.491733 7.300458\n27 7.582923 7.368377 7.515990 7.358491\n28 7.572931 7.244174 7.466398 7.172318\n29 7.579088 7.366221 7.480393 7.158943\n30 7.601925 7.050239 7.405865 7.281161\n31 7.601097 7.217883 7.501238 7.079388\n32 7.602343 7.215997 7.486182 7.362801\n33 7.621057 7.371830 7.477008 7.321741\n34 7.632899 7.159265 7.465772 7.294746\n35 7.614549 7.213676 7.464199 7.014393\n36 7.586656 7.270587 7.435904 7.400305\n37 7.593428 7.108840 7.522947 7.414827\n38 7.593836 7.193211 7.492250 7.175875\n39 7.586345 7.250112 7.477559 7.152434\n40 7.610498 7.269978 7.452130 7.254088\n41 7.570510 7.347217 7.476158 7.220753\n42 7.618182 7.287882 7.412706 7.084224\n43 7.621420 7.233066 7.507654 7.124081\n44 7.624912 7.074213 7.438136 7.212636\n45 7.541945 7.178940 7.451000 7.109039\n46 7.635028 7.263358 7.453537 7.242004\n47 7.590112 7.170538 7.506422 6.924094\n48 7.638272 7.468543 7.431446 7.129897\n49 7.600589 7.215825 7.450062 7.366063\n50 7.607752 7.131514 7.427264 7.360753\n\n$out_nc_quantile_y\n             1        4        2        3\n2.5%  7.528537 6.604338 7.257360 6.873810\n50%   7.604075 6.699554 7.324793 6.982175\n97.5% 7.639328 6.779852 7.374694 7.092368\n\n$out_cf_quantile_y\n             1        4        2        3\n2.5%  7.529402 7.055633 7.407404 7.008799\n50%   7.603543 7.234802 7.469238 7.215704\n97.5% 7.637962 7.391138 7.517707 7.430056\n\n$mediation\n        4         2         3 \n0.6073567 0.5348774 0.3926196 \n\n$mediation_quantile\n              4         2          3\n2.5%  0.4362047 0.3328234 0.05168082\n97.5% 0.7746384 0.7824060 0.71708929\n\n$mc_conv_info_m\n          [,1]     [,2]     [,3]     [,4]\n [1,] 7.723179 5.928527 7.115377 6.273738\n [2,] 7.703455 5.931829 7.113009 6.283259\n [3,] 7.707302 5.893149 7.113309 6.302433\n [4,] 7.705497 5.903295 7.112800 6.309729\n [5,] 7.706187 5.906655 7.120460 6.311052\n [6,] 7.712124 5.901055 7.122027 6.330075\n [7,] 7.710921 5.900642 7.118638 6.347855\n [8,] 7.708649 5.901588 7.121293 6.351281\n [9,] 7.712484 5.896719 7.121812 6.349336\n[10,] 7.711633 5.888257 7.125844 6.352904\n\n$mc_conv_info_y\n          [,1]     [,2]     [,3]     [,4]\n [1,] 7.645142 6.743777 7.373939 6.902550\n [2,] 7.641534 6.744677 7.373561 6.903504\n [3,] 7.642238 6.734132 7.373609 6.905424\n [4,] 7.641907 6.736898 7.373527 6.906155\n [5,] 7.642034 6.737814 7.374752 6.906288\n [6,] 7.643120 6.736288 7.375003 6.908193\n [7,] 7.642899 6.736175 7.374461 6.909974\n [8,] 7.642484 6.736433 7.374885 6.910317\n [9,] 7.643185 6.735106 7.374968 6.910122\n[10,] 7.643030 6.732799 7.375613 6.910479\n\nmean(fit_cfdecomp$out_nc_y[,2] - fit_cfdecomp$out_nc_y[,1])\n\n[1] -0.9016258\n\nmean(fit_cfdecomp$out_cf_y[,2] - fit_cfdecomp$out_nc_y[,1])\n\n[1] -0.3558926\n\nmean(fit_cfdecomp$out_nc_y[,2] - fit_cfdecomp$out_cf_y[,2])\n\n[1] -0.5457332\n\nfit_cfdecomp$mediation\n\n        4         2         3 \n0.6073567 0.5348774 0.3926196 \n\nmean(fit_cfdecomp$out_nc_y[,3] - fit_cfdecomp$out_nc_y[,1])\n\n[1] -0.2804946\n\nmean(fit_cfdecomp$out_cf_y[,3] - fit_cfdecomp$out_nc_y[,1])\n\n[1] -0.1334747\n\nmean(fit_cfdecomp$out_nc_y[,3] - fit_cfdecomp$out_cf_y[,3])\n\n[1] -0.1470199\n\nmean(fit_cfdecomp$out_nc_y[,4] - fit_cfdecomp$out_nc_y[,1])\n\n[1] -0.6136728\n\nmean(fit_cfdecomp$out_cf_y[,4] - fit_cfdecomp$out_nc_y[,1])\n\n[1] -0.3771942\n\nmean(fit_cfdecomp$out_nc_y[,4] - fit_cfdecomp$out_cf_y[,4])\n\n[1] -0.2364786"
  },
  {
    "objectID": "blog/posts/Causal_Decomposition.html#causal.decomp",
    "href": "blog/posts/Causal_Decomposition.html#causal.decomp",
    "title": "Causal Decomposition Analysis",
    "section": "causal.decomp",
    "text": "causal.decomp\n\nPark et al. (2023) の方法。\n\n\n# smi \nfit.y &lt;- lm(Y ~ X + T + X:T + L1 + L2 + C1 + C2 + C3 + C4, data = d)\nfit.m &lt;- lm(T ~ X + C1 + C2 + C3 + C4, data = d)\n\nfit_smi &lt;- smi(fit.y = fit.y,\n    fit.m = fit.m,\n    treat = \"X\", \n    sims = 100, \n    conf.level = .95,\n    conditional = TRUE,\n    covariates = 1,\n    # baseline covariatesを調整できる\n    #covariates = c(\"C1\", \"C2\", \"C3\", \"C4\"),\n    seed = 227,\n    )\n\nfit_smi\n\n\nResults:\n\n                               estimate 95% CI Lower 95% CI Upper\nInitial Disparity   (1 vs 4) -0.8993401   -0.9925190  -0.79813505\nDisparity Remaining (1 vs 4) -0.3384430   -0.4880863  -0.14795873\nDisparity Reduction (1 vs 4) -0.5608971   -0.7346168  -0.42650061\nInitial Disparity   (1 vs 2) -0.2749659   -0.3378874  -0.19366549\nDisparity Remaining (1 vs 2) -0.1213246   -0.2203441  -0.05458727\nDisparity Reduction (1 vs 2) -0.1536412   -0.1896213  -0.10521328\nInitial Disparity   (1 vs 3) -0.6137425   -0.7326095  -0.47793913\nDisparity Remaining (1 vs 3) -0.3500123   -0.5151994  -0.08733348\nDisparity Reduction (1 vs 3) -0.2637302   -0.4880614  -0.11574038\n\n\n\nsensitivity analysis(Park et al. 2023)\n\n\nsensRes &lt;- sensitivity(boot.res = fit_smi, fit.m = fit.m, fit.y = fit.y, \n                       mediator = \"T\",\n                       covariates = c(\"C1\", \"C2\", \"C3\", \"C4\"), \n                       treat = \"X\",\n                       sel.lev.treat = \"4\", \n                       max.rsq = 0.3)\nplot(sensRes)"
  },
  {
    "objectID": "blog/posts/Causal_Decomposition.html#cfdecomp-1",
    "href": "blog/posts/Causal_Decomposition.html#cfdecomp-1",
    "title": "Causal Decomposition Analysis",
    "section": "cfdecomp",
    "text": "cfdecomp\n\n# cfd.mean\nset.seed(123456)\nfit_cfdecomp_b &lt;-\n  cfd.mean(\n    formula.y = 'Y ~ X + T2 + X:T2 + L1 + L2 + C1 + C2 + C3 + C4',\n    formula.m = 'T2 ~ X + C1 + C2 + C3 + C4',\n    mediator = 'T2',\n    group = 'X',\n    data = d |&gt; mutate(T2 = as.numeric(T2) - 1) |&gt; data.frame(),\n    family.y = 'gaussian',\n    family.m = 'binomial',\n    bs.size = 50,\n    mc.size = 10,\n    alpha = 0.05\n  )\nmean(fit_cfdecomp_b$out_nc_y[,\"4\"] - fit_cfdecomp_b$out_nc_y[,\"1\"])\n\n[1] -0.8981701\n\nmean(fit_cfdecomp_b$out_cf_y[,\"4\"] - fit_cfdecomp_b$out_nc_y[,\"1\"])\n\n[1] -0.5546351\n\nmean(fit_cfdecomp_b$out_nc_y[,\"4\"] - fit_cfdecomp_b$out_cf_y[,\"4\"])\n\n[1] -0.343535\n\nfit_cfdecomp_b$mediation\n\n        4         2         3 \n0.3828971 0.1298177 0.2061071 \n\nmean(fit_cfdecomp_b$out_nc_y[,\"2\"] - fit_cfdecomp_b$out_nc_y[,\"1\"])\n\n[1] -0.2774845\n\nmean(fit_cfdecomp_b$out_cf_y[,\"2\"] - fit_cfdecomp_b$out_nc_y[,\"1\"])\n\n[1] -0.2419289\n\nmean(fit_cfdecomp_b$out_nc_y[,\"2\"] - fit_cfdecomp_b$out_cf_y[,\"2\"])\n\n[1] -0.03555558\n\nmean(fit_cfdecomp_b$out_nc_y[,\"3\"] - fit_cfdecomp_b$out_nc_y[,\"1\"])\n\n[1] -0.5849521\n\nmean(fit_cfdecomp_b$out_cf_y[,\"3\"] - fit_cfdecomp_b$out_nc_y[,\"1\"])\n\n[1] -0.4655888\n\nmean(fit_cfdecomp_b$out_nc_y[,\"3\"] - fit_cfdecomp_b$out_cf_y[,\"3\"])\n\n[1] -0.1193632"
  },
  {
    "objectID": "blog/posts/Causal_Decomposition.html#causal.decomp-1",
    "href": "blog/posts/Causal_Decomposition.html#causal.decomp-1",
    "title": "Causal Decomposition Analysis",
    "section": "causal.decomp",
    "text": "causal.decomp\n\n# smi\nfit.y &lt;- lm(Y ~ X + T2 + X:T2 + L1 + L2 + C1 + C2 + C3 + C4, data = d)\nfit.m &lt;- glm(T2 ~ X + C1 + C2 + C3 + C4, data = d, family = binomial(link = \"logit\"))\n\nfit_smi_b &lt;- smi(fit.y = fit.y,\n               fit.m = fit.m,\n               treat = \"X\", \n               sims = 100, \n               conf.level = .95,\n               conditional = TRUE,\n               # covariates = 1,\n               covariates = c(\"C1\", \"C2\", \"C3\", \"C4\"),\n               seed = 123456)\nfit_smi_b\n\n\nResults:\n\n                                estimate 95% CI Lower 95% CI Upper\nInitial Disparity   (1 vs 4) -0.95667843  -1.02938841  -0.88175003\nDisparity Remaining (1 vs 4) -0.61262729  -0.77966809  -0.46844380\nDisparity Reduction (1 vs 4) -0.34405113  -0.49650368  -0.21581205\nInitial Disparity   (1 vs 2) -0.31394841  -0.38058533  -0.26000373\nDisparity Remaining (1 vs 2) -0.27995004  -0.34624791  -0.22226331\nDisparity Reduction (1 vs 2) -0.03399837  -0.05003528  -0.02022012\nInitial Disparity   (1 vs 3) -0.59968604  -0.69525994  -0.49987857\nDisparity Remaining (1 vs 3) -0.48148718  -0.61387973  -0.32105036\nDisparity Reduction (1 vs 3) -0.11819886  -0.24676548  -0.03361808\n\nsensRes &lt;- sensitivity(boot.res = fit_smi_b, \n                       fit.m = fit.m, \n                       fit.y = fit.y, \n                       mediator = \"T2\",\n                       covariates = c(\"C1\", \"C2\", \"C3\", \"C4\"), \n                       treat = \"X\",\n                       sel.lev.treat = \"4\", \n                       max.rsq = 0.3)\nplot(sensRes)"
  },
  {
    "objectID": "blog/posts/Causal_Decomposition.html#gapclosing",
    "href": "blog/posts/Causal_Decomposition.html#gapclosing",
    "title": "Causal Decomposition Analysis",
    "section": "gapclosing",
    "text": "gapclosing\n\nLundberg (2022)\n\n\n# gapclosing - regression\n# stochastic intervention\n# treatmentの割り当て確率の予測値を算出\nfit_glm &lt;- glm(T2 ~ X + C1 + C2 + C3, data = d, family = binomial(link = \"logit\"))\n\n# 全員のtreatmentが1だった時の予測値\nassing_prob &lt;- predict(fit_glm, newdata = d |&gt; mutate(X = \"1\"), type = \"response\")\n\n# 予測値をもとにrandom draw\ndraw &lt;- rbinom(n = nrow(d), size = 1, prob = assing_prob)\n\nfit_gapclosing &lt;- \n  gapclosing(\n    data = d |&gt; mutate(T2 = as.numeric(T2) - 1),\n    outcome_formula = Y ~ T2 * X + C1 + C2 + C3 + C4 + L1 + L2,\n    treatment_name = \"T2\",\n    category_name = \"X\",\n    counterfactual_assignments = draw # random draw\n  )\n\nfit_gapclosing\n\n\nFactual mean outcomes:\n# A tibble: 4 × 2\n  X     estimate\n  &lt;fct&gt;    &lt;dbl&gt;\n1 1         7.60\n2 4         6.70\n3 2         7.32\n4 3         6.98\n\nCounterfactual mean outcomes (post-intervention means):\n# A tibble: 4 × 2\n  X     estimate\n  &lt;fct&gt;    &lt;dbl&gt;\n1 1         7.60\n2 4         7.03\n3 2         7.36\n4 3         7.11\n\nFactual disparities:\n# A tibble: 12 × 2\n   X     estimate\n   &lt;chr&gt;    &lt;dbl&gt;\n 1 1 - 4    0.899\n 2 1 - 2    0.275\n 3 1 - 3    0.614\n 4 4 - 1   -0.899\n 5 4 - 2   -0.624\n 6 4 - 3   -0.286\n 7 2 - 1   -0.275\n 8 2 - 4    0.624\n 9 2 - 3    0.339\n10 3 - 1   -0.614\n11 3 - 4    0.286\n12 3 - 2   -0.339\n\nCounterfactual disparities (gap-closing estimands):\n# A tibble: 12 × 2\n   X     estimate\n   &lt;chr&gt;    &lt;dbl&gt;\n 1 1 - 4   0.566 \n 2 1 - 2   0.241 \n 3 1 - 3   0.483 \n 4 4 - 1  -0.566 \n 5 4 - 2  -0.325 \n 6 4 - 3  -0.0830\n 7 2 - 1  -0.241 \n 8 2 - 4   0.325 \n 9 2 - 3   0.242 \n10 3 - 1  -0.483 \n11 3 - 4   0.0830\n12 3 - 2  -0.242 \n\nAdditive gap closed: Counterfactual - Factual\n# A tibble: 12 × 2\n   X     estimate\n   &lt;chr&gt;    &lt;dbl&gt;\n 1 1 - 4   0.334 \n 2 1 - 2   0.0339\n 3 1 - 3   0.131 \n 4 4 - 1  -0.334 \n 5 4 - 2  -0.300 \n 6 4 - 3  -0.203 \n 7 2 - 1  -0.0339\n 8 2 - 4   0.300 \n 9 2 - 3   0.0972\n10 3 - 1  -0.131 \n11 3 - 4   0.203 \n12 3 - 2  -0.0972\n\nProportional gap closed: (Counterfactual - Factual) / Factual\n# A tibble: 12 × 2\n   X     estimate\n   &lt;chr&gt;    &lt;dbl&gt;\n 1 1 - 4    0.371\n 2 1 - 2    0.123\n 3 1 - 3    0.214\n 4 4 - 1    0.371\n 5 4 - 2    0.480\n 6 4 - 3    0.710\n 7 2 - 1    0.123\n 8 2 - 4    0.480\n 9 2 - 3    0.287\n10 3 - 1    0.214\n11 3 - 4    0.710\n12 3 - 2    0.287\n\ndisparityplot(fit_gapclosing, category_A = \"1\", category_B = \"4\")\n\n\n\n\n\n\n\ndisparityplot(fit_gapclosing, category_A = \"1\", category_B = \"2\")\n\n\n\n\n\n\n\ndisparityplot(fit_gapclosing, category_A = \"1\", category_B = \"3\")\n\n\n\n\n\n\n\n\n\n機械学習をつかったdoubly robustな方法も使える\n\n\n# gapclosing - ranger, doubly robust\nfit_gapclosing_ranger &lt;- \n  gapclosing(\n  data = d |&gt; mutate(T2 = as.numeric(T2) - 1),\n  outcome_formula = Y ~ T2 + X + C1 + C2 + C3 + C4 + L1 + L2,\n  treatment_formula = T2 ~ X + C1 + C2 + C3 + C4 + L1 + L2, \n  treatment_name = \"T2\",\n  treatment_algorithm = \"ranger\",\n  outcome_algorithm = \"ranger\",\n  category_name = \"X\",\n  counterfactual_assignments = rbinom(n = nrow(d), size = 1, prob = assing_prob) \n)\n\nfit_gapclosing\n\n\nFactual mean outcomes:\n# A tibble: 4 × 2\n  X     estimate\n  &lt;fct&gt;    &lt;dbl&gt;\n1 1         7.60\n2 4         6.70\n3 2         7.32\n4 3         6.98\n\nCounterfactual mean outcomes (post-intervention means):\n# A tibble: 4 × 2\n  X     estimate\n  &lt;fct&gt;    &lt;dbl&gt;\n1 1         7.60\n2 4         7.03\n3 2         7.36\n4 3         7.11\n\nFactual disparities:\n# A tibble: 12 × 2\n   X     estimate\n   &lt;chr&gt;    &lt;dbl&gt;\n 1 1 - 4    0.899\n 2 1 - 2    0.275\n 3 1 - 3    0.614\n 4 4 - 1   -0.899\n 5 4 - 2   -0.624\n 6 4 - 3   -0.286\n 7 2 - 1   -0.275\n 8 2 - 4    0.624\n 9 2 - 3    0.339\n10 3 - 1   -0.614\n11 3 - 4    0.286\n12 3 - 2   -0.339\n\nCounterfactual disparities (gap-closing estimands):\n# A tibble: 12 × 2\n   X     estimate\n   &lt;chr&gt;    &lt;dbl&gt;\n 1 1 - 4   0.566 \n 2 1 - 2   0.241 \n 3 1 - 3   0.483 \n 4 4 - 1  -0.566 \n 5 4 - 2  -0.325 \n 6 4 - 3  -0.0830\n 7 2 - 1  -0.241 \n 8 2 - 4   0.325 \n 9 2 - 3   0.242 \n10 3 - 1  -0.483 \n11 3 - 4   0.0830\n12 3 - 2  -0.242 \n\nAdditive gap closed: Counterfactual - Factual\n# A tibble: 12 × 2\n   X     estimate\n   &lt;chr&gt;    &lt;dbl&gt;\n 1 1 - 4   0.334 \n 2 1 - 2   0.0339\n 3 1 - 3   0.131 \n 4 4 - 1  -0.334 \n 5 4 - 2  -0.300 \n 6 4 - 3  -0.203 \n 7 2 - 1  -0.0339\n 8 2 - 4   0.300 \n 9 2 - 3   0.0972\n10 3 - 1  -0.131 \n11 3 - 4   0.203 \n12 3 - 2  -0.0972\n\nProportional gap closed: (Counterfactual - Factual) / Factual\n# A tibble: 12 × 2\n   X     estimate\n   &lt;chr&gt;    &lt;dbl&gt;\n 1 1 - 4    0.371\n 2 1 - 2    0.123\n 3 1 - 3    0.214\n 4 4 - 1    0.371\n 5 4 - 2    0.480\n 6 4 - 3    0.710\n 7 2 - 1    0.123\n 8 2 - 4    0.480\n 9 2 - 3    0.287\n10 3 - 1    0.214\n11 3 - 4    0.710\n12 3 - 2    0.287\n\ndisparityplot(fit_gapclosing_ranger, category_A = \"1\", category_B = \"4\")\n\n\n\n\n\n\n\ndisparityplot(fit_gapclosing_ranger, category_A = \"1\", category_B = \"2\")\n\n\n\n\n\n\n\ndisparityplot(fit_gapclosing_ranger, category_A = \"1\", category_B = \"3\")"
  },
  {
    "objectID": "blog/posts/Multilevel.html",
    "href": "blog/posts/Multilevel.html",
    "title": "マルチレベルモデルは点推定値に影響を及ぼすか？",
    "section": "",
    "text": "library(tidyverse)\nlibrary(lmerTest)\nlibrary(broom)\nlibrary(broom.mixed)"
  },
  {
    "objectID": "blog/posts/Multilevel.html#マルチレベルモデルにおけるよくある誤解",
    "href": "blog/posts/Multilevel.html#マルチレベルモデルにおけるよくある誤解",
    "title": "マルチレベルモデルは点推定値に影響を及ぼすか？",
    "section": "マルチレベルモデルにおけるよくある誤解",
    "text": "マルチレベルモデルにおけるよくある誤解\n「マルチレベルモデルでやんなきゃ係数にバイアスが…」←ホント？"
  },
  {
    "objectID": "blog/posts/Multilevel.html#データ生成",
    "href": "blog/posts/Multilevel.html#データ生成",
    "title": "マルチレベルモデルは点推定値に影響を及ぼすか？",
    "section": "データ生成",
    "text": "データ生成\nマルチレベルのデータを考える サンプルサイズ1000、グループ数50のデータ\n\\[\\begin{align}\ny_{ig} \\sim 0.5x_{ig} + \\mathrm{Normal}(\\theta_g, 1) \\\\\n\\theta_g \\sim \\mathrm{Normal}(0, 3) \\\\\nx_i \\sim \\mathrm{Normal}(0, 1)\n\\end{align}\\]\n\ndgp &lt;- function(samplesize = 1000) {\n  tibble(\n    id = 1:samplesize,\n    group = rep(1:50, 20),\n    x = rnorm(samplesize, mean = 0, sd = 1),\n  ) |&gt; \n    group_by(group) |&gt; \n    mutate(group_mean = rnorm(1, mean = 0, sd = 3)) |&gt; \n    ungroup() |&gt; \n    mutate(y = 0.5*x + rnorm(samplesize, mean = group_mean, sd = 1))\n}\n\ndata &lt;- dgp()\n\n\nlm(y ~ x, data = data) |&gt; \n  summary()\n\n\nCall:\nlm(formula = y ~ x, data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.6088 -2.3228 -0.1326  2.1536  9.9809 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.30026    0.10027  -2.994  0.00282 ** \nx            0.43733    0.09608   4.552 5.98e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.171 on 998 degrees of freedom\nMultiple R-squared:  0.02034,   Adjusted R-squared:  0.01935 \nF-statistic: 20.72 on 1 and 998 DF,  p-value: 5.98e-06\n\nlmer(y ~ x + (1|group), data = data) |&gt; \n  summary()\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: y ~ x + (1 | group)\n   Data: data\n\nREML criterion at convergence: 3184.6\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-2.6532 -0.6731  0.0089  0.6698  3.3401 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n group    (Intercept) 9.130    3.022   \n Residual             1.092    1.045   \nNumber of obs: 1000, groups:  group, 50\n\nFixed effects:\n             Estimate Std. Error        df t value Pr(&gt;|t|)    \n(Intercept)  -0.30028    0.42859  48.99985  -0.701    0.487    \nx             0.48207    0.03242 949.54983  14.871   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n  (Intr)\nx 0.000"
  },
  {
    "objectID": "blog/posts/Multilevel.html#シミュレーション",
    "href": "blog/posts/Multilevel.html#シミュレーション",
    "title": "マルチレベルモデルは点推定値に影響を及ぼすか？",
    "section": "シミュレーション",
    "text": "シミュレーション\nデータを1000個作成\n\ndata_list &lt;- \n  map(1:1000, \\(x) dgp(1000)) |&gt; \n  enframe()\n\nOLSと変量効果モデルで推定\n\nresult &lt;- \n  data_list |&gt; \n  mutate(\n    lm = map(value, \\(data) lm(y ~ x, data = data)),\n    lmer = map(value, \\(data) lmer(y ~ x + (1|group), data = data))\n  )\n\nxの係数のみ取り出す\n\nres2 &lt;- \n  result |&gt; \n  mutate(\n    lm_res = map(lm, \\(model) {\n      tidy(model) |&gt; \n        select(term, estimate)\n    }),\n    lmer_res = map(lmer, \\(model) {\n      tidy(model) |&gt; \n        select(term, estimate)\n    })\n  ) |&gt; \n  select(name, lm_res, lmer_res) |&gt; \n  pivot_longer(!name, names_to = 'model', values_to = 'value') |&gt; \n  unnest(value) |&gt; \n  filter(term == 'x')\n\n結果を図示\n\nどちらの点推定値も真の値の0.5を中心に分布＝バイアスはない\nOLSによる点推定値はバリアンスが大きい\n変量効果（マルチレベル）モデルによる点推定値はバリアンスが小さい\n\n\nres2 |&gt; \n  ggplot(aes(estimate, fill = model))+\n  geom_vline(xintercept = 0.5, linetype = 'dashed', alpha = 0.5)+\n  geom_histogram(alpha = 0.3, color = 'black', binwidth = 0.02, position = 'identity')+\n  scale_x_continuous(breaks = seq(0, 1, 0.2))"
  },
  {
    "objectID": "blog/posts/rlang学習.html",
    "href": "blog/posts/rlang学習.html",
    "title": "rlang学習",
    "section": "",
    "text": "Advanced R"
  },
  {
    "objectID": "blog/posts/rlang学習.html#参考",
    "href": "blog/posts/rlang学習.html#参考",
    "title": "rlang学習",
    "section": "",
    "text": "Advanced R"
  },
  {
    "objectID": "blog/posts/rlang学習.html#下準備",
    "href": "blog/posts/rlang学習.html#下準備",
    "title": "rlang学習",
    "section": "下準備",
    "text": "下準備\n\nlibrary(rlang)\nlibrary(lobstr)"
  },
  {
    "objectID": "blog/posts/rlang学習.html#expression",
    "href": "blog/posts/rlang学習.html#expression",
    "title": "rlang学習",
    "section": "expression",
    "text": "expression\n\nexpr：コードをそのまま捕まえる\n\n作成されたオブジェクト：expression\ncall, symbol, constant, pairlistの4つからなる\n\n\n\nexpr(mean(x, na.rm = TRUE))\n\nmean(x, na.rm = TRUE)\n\nexpr(10 + 100 + 1000)\n\n10 + 100 + 1000\n\n\n\nexprは関数の引数なども書いてあるまま保持する\n\n\n# これはxになる\ncapture_it &lt;- function(x) {\n  expr(x)\n}\n\ncapture_it(a + b + c)\n\nx\n\n\n\n引数は評価した上でコードを保持したい場合はenexprを使う\n\n\ncapture_it &lt;- function(x) {\n  enexpr(x)\n}\ncapture_it(a + b + c)\n\na + b + c"
  },
  {
    "objectID": "blog/posts/rlang学習.html#abstract-syntax-tree-ast-による表現",
    "href": "blog/posts/rlang学習.html#abstract-syntax-tree-ast-による表現",
    "title": "rlang学習",
    "section": "abstract syntax tree (AST) による表現",
    "text": "abstract syntax tree (AST) による表現\n\ncodeはtree形式で表現できる\nfがcall, aがsymbol, “b”がconstantらしい\n\n\nlobstr::ast(f(a, \"b\"))\n\n█─f \n├─a \n└─\"b\" \n\n\n\n+や*も関数であることがわかる\n\n\nlobstr::ast(1 + 2 * 3)\n\n█─`+` \n├─1 \n└─█─`*` \n  ├─2 \n  └─3"
  },
  {
    "objectID": "blog/posts/rlang学習.html#codeを生成するcode",
    "href": "blog/posts/rlang学習.html#codeを生成するcode",
    "title": "rlang学習",
    "section": "codeを生成するcode",
    "text": "codeを生成するcode\n\ncall2\n\n\ncall2('f', 1, 2)\n\nf(1, 2)\n\ncall2('+', 1, call2('*', 2, 3))\n\n1 + 2 * 3\n\n\n\n!!を使ったcodeの生成\n\n捕まえておいたコードを評価したいところで評価する\n\n\n\nxx &lt;- expr(x + x)\nyy &lt;- expr(y + y)\n\nexpr(!!xx / !!yy)\n\n(x + x)/(y + y)"
  },
  {
    "objectID": "blog/posts/rlang学習.html#codeの評価",
    "href": "blog/posts/rlang学習.html#codeの評価",
    "title": "rlang学習",
    "section": "codeの評価",
    "text": "codeの評価\n\neval(expr, env)：exprをenvで評価\n\n\neval(expr(x + y), env(x = 1, y = 10))\n\n[1] 11\n\neval(expr(x + y), env(x = 2, y = 100))\n\n[1] 102\n\n\n\nenvを指定しないときは現在のenvironmentで評価\n\n\nx &lt;- 10\ny &lt;- 100\n\neval(expr(x + y))\n\n[1] 110\n\n\n\nデータを環境として扱う\neval_tidy(expr, data)：exprをdata内部で評価\n\ndataでcurrent environmentを上書きするのでdata-mask呼ばれる\n\nevalでもできないことはないが、落とし穴があるらしい\n\n\ndf &lt;- data.frame(x = 1:5, y = sample(5))\neval_tidy(expr(x + y), df)\n\n[1]  4  6  4  6 10\n\n\n\ndplyrっぽい関数\n\n\nwith2 &lt;- function(df, expr) {\n  eval_tidy(enexpr(expr), df)\n}\n\nwith2(df, x + y)\n\n[1]  4  6  4  6 10"
  },
  {
    "objectID": "blog/posts/rlang学習.html#quosure",
    "href": "blog/posts/rlang学習.html#quosure",
    "title": "rlang学習",
    "section": "Quosure",
    "text": "Quosure\n\nexprだけを使う方法は問題がでてくる\n関数の内部でaを定義してみる\n\n\nwith2 &lt;- function(df, expr) {\n  a &lt;- 1000\n  eval_tidy(enexpr(expr), df)\n}\n\n\nさらに関数の外部（global environment）でもaを定義する\n\n\ndf &lt;- data.frame(x = 1:3)\na &lt;- 10\n\n\nこの状態でwith2を使うと問題が発生\n\nglobal environmentのaではなく、関数内部で定義したaが優先的に使われる\n\n\n\nwith2(df, x + a)\n\n[1] 1001 1002 1003\n\n\n\nquosure：expressionとenvironmentをセットで保持する\n\ndata-maskでは、eval_tidyのdata -&gt; quosureのenv -&gt; global envの順で評価される\n\n\n\nwith2 &lt;- function(df, expr) {\n  a &lt;- 1000\n  eval_tidy(enquo(expr), df)\n}\n\nx &lt;- 10\n\nwith2(df, x + a)\n\n[1] 11 12 13"
  },
  {
    "objectID": "blog/posts/Sudharsanan_Bijlsma_2021.html",
    "href": "blog/posts/Sudharsanan_Bijlsma_2021.html",
    "title": "Sudharsanan & Bijlsma (2021)",
    "section": "",
    "text": "Sudharsanan, Nikkil & Maarten J. Bijlsma, 2021, “Educational Note: Causal Decomposition of Population Health Differences Using Monte Carlo Integration and the G-Formula,” International Journal of Epidemiology, 50(6): 2098–2107 (10.1093/ije/dyab090)."
  },
  {
    "objectID": "blog/posts/Sudharsanan_Bijlsma_2021.html#文献",
    "href": "blog/posts/Sudharsanan_Bijlsma_2021.html#文献",
    "title": "Sudharsanan & Bijlsma (2021)",
    "section": "",
    "text": "Sudharsanan, Nikkil & Maarten J. Bijlsma, 2021, “Educational Note: Causal Decomposition of Population Health Differences Using Monte Carlo Integration and the G-Formula,” International Journal of Epidemiology, 50(6): 2098–2107 (10.1093/ije/dyab090)."
  },
  {
    "objectID": "blog/posts/Sudharsanan_Bijlsma_2021.html#下準備",
    "href": "blog/posts/Sudharsanan_Bijlsma_2021.html#下準備",
    "title": "Sudharsanan & Bijlsma (2021)",
    "section": "下準備",
    "text": "下準備\n\nパッケージ\n\n\nlibrary(tidyverse)\nlibrary(cfdecomp)\n\nkable &lt;- partial(\n  knitr::kable,\n  digits = 3\n)\n\n\nデータ\n\n\n# the decomposition functions in our package are computationally intensive\n# to make the example run quick, I perform it on a subsample (n=125) of the data:\nset.seed(100)\ndata &lt;- \n  cfd.example.data[sample(1000),] |&gt; \n  select(SES, age, med.gauss, out.gauss, id) |&gt; \n  as_tibble()"
  },
  {
    "objectID": "blog/posts/Sudharsanan_Bijlsma_2021.html#パッケージによる推定",
    "href": "blog/posts/Sudharsanan_Bijlsma_2021.html#パッケージによる推定",
    "title": "Sudharsanan & Bijlsma (2021)",
    "section": "パッケージによる推定",
    "text": "パッケージによる推定\n\ncfdecompパッケージで推定\n\n\n# cfd.mean \nmean.results.1 &lt;- \n  cfd.mean(\n    formula.y = out.gauss ~ SES * med.gauss * age,\n    formula.m = med.gauss ~ SES * age,\n    mediator = 'med.gauss',\n    group = 'SES',\n    data = as.data.frame(data),\n    family.y = 'gaussian',\n    family.m = 'gaussian',\n    bs.size=250,\n    mc.size=10,\n    alpha=0.05,\n    # cluster.sample=FALSE,\n    # cluster.name='id'\n  )\n\n\nSES2とSES3のmediatorの分布をSES1のmediatorの分布に揃える\n\n\ntibble(\n  category = c(\"SES1\", \"SES2\", \"SES3\"),\n  factual_mean = c(\n    mean(mean.results.1$out_nc_y[, 1]),\n    mean(mean.results.1$out_nc_y[, 2]),\n    mean(mean.results.1$out_nc_y[, 3])\n  ),\n  # and after giving the gaussian mediator of SES group 2 the distribution of the one in group 1\n  # the difference becomes:\n  counterfactual_mean = c(\n    mean(mean.results.1$out_cf_y[, 1]),\n    mean(mean.results.1$out_cf_y[, 2]),\n    mean(mean.results.1$out_cf_y[, 3])\n  )) |&gt; \n  kable()\n\n\n\n\ncategory\nfactual_mean\ncounterfactual_mean\n\n\n\n\nSES1\n4.306\n4.306\n\n\nSES2\n3.239\n3.511\n\n\nSES3\n2.212\n2.946"
  },
  {
    "objectID": "blog/posts/Sudharsanan_Bijlsma_2021.html#自分でコードを書いてみる",
    "href": "blog/posts/Sudharsanan_Bijlsma_2021.html#自分でコードを書いてみる",
    "title": "Sudharsanan & Bijlsma (2021)",
    "section": "自分でコードを書いてみる",
    "text": "自分でコードを書いてみる"
  },
  {
    "objectID": "blog/posts/Sudharsanan_Bijlsma_2021.html#step-1-regression-estimates",
    "href": "blog/posts/Sudharsanan_Bijlsma_2021.html#step-1-regression-estimates",
    "title": "Sudharsanan & Bijlsma (2021)",
    "section": "step 1: regression estimates",
    "text": "step 1: regression estimates\n\nmediatorとoutcomeのモデルをデータから推定\n\n\nmediator_model &lt;- lm(med.gauss ~ SES * age, data = data)\noutcome_model &lt;- lm(out.gauss ~ SES * age * med.gauss, data = data)"
  },
  {
    "objectID": "blog/posts/Sudharsanan_Bijlsma_2021.html#step-2-simulate-the-natural-course-pseudo-population",
    "href": "blog/posts/Sudharsanan_Bijlsma_2021.html#step-2-simulate-the-natural-course-pseudo-population",
    "title": "Sudharsanan & Bijlsma (2021)",
    "section": "step 2: simulate the natural-course pseudo-population",
    "text": "step 2: simulate the natural-course pseudo-population\n\n推定したmediatorのモデルから、mediatorの「分布」を再現\n\n\\[\\begin{align*}\nMed_i = \\mathrm{E}[Med | X] + e_i \\\\\ne_i \\sim \\mathrm{N}(0, \\sigma)\n\\end{align*}\\]\n\nmediatorの分布のパラメータを取得\n\n\n# predict mediator\n# mediatorの「分布」のパラメータを取得\npred_mean_m &lt;- predict(mediator_model, newdata = data, type = \"response\")\nresidual_ref_m &lt;- mediator_model$residuals\nsd_ref_m &lt;- sd(residual_ref_m)\n\n\n推定したパラメータをもとに、mediatorの値をシミュレート\n\n\ndf_nc_med &lt;- \n  data |&gt; \n  mutate(\n    # ランダム性なし\n    pred_med = pred_mean_m,\n    # ランダム性をもたせる1（推定した標準偏差のパラメータを使用）\n    pred_med_draw_1 = rnorm(n(), mean = pred_mean_m, sd = sd_ref_m),\n    # ランダム性をもたせる2（残差からランダムにサンプリング）\n    pred_med_draw_2 = pred_mean_m + sample(residual_ref_m, n(), replace = TRUE)\n  )\n\ndf_nc_med |&gt; \n  summarise(\n    across(c(med.gauss, pred_med:pred_med_draw_2), mean),\n    .by = SES\n  ) |&gt; \n  arrange(SES) |&gt; \n  kable()\n\n\n\n\nSES\nmed.gauss\npred_med\npred_med_draw_1\npred_med_draw_2\n\n\n\n\n1\n8.428\n8.428\n8.418\n8.286\n\n\n2\n7.223\n7.223\n7.276\n7.454\n\n\n3\n5.410\n5.410\n5.579\n5.374\n\n\n\n\n\n\nシミュレートした値をoutcomeモデルに代入して予測値を計算・集計\n\n\ndf_nc_med |&gt; \n  mutate(\n    # ランダム性なし\n    pred_out = predict(\n      outcome_model, newdata = df_nc_med |&gt; mutate(med.gauss = pred_med)\n    ),\n    # ランダム性をもたせる1（推定した標準偏差のパラメータを使用）\n    pred_out_draw_1 = predict(\n      outcome_model, newdata = df_nc_med |&gt; mutate(med.gauss = pred_med_draw_1)\n    ),\n    # ランダム性をもたせる2（残差からランダムにサンプリング）\n    pred_out_draw_2 = predict(\n      outcome_model, newdata = df_nc_med |&gt; mutate(med.gauss = pred_med_draw_2)\n    )\n  ) |&gt; \n  summarise(\n    across(c(out.gauss, pred_out:pred_out_draw_2), mean),\n    .by = SES\n  ) |&gt; \n  arrange(SES) |&gt; \n  kable()\n\n\n\n\nSES\nout.gauss\npred_out\npred_out_draw_1\npred_out_draw_2\n\n\n\n\n1\n4.309\n4.309\n4.312\n4.286\n\n\n2\n3.237\n3.237\n3.249\n3.292\n\n\n3\n2.217\n2.217\n2.254\n2.214"
  },
  {
    "objectID": "blog/posts/Sudharsanan_Bijlsma_2021.html#step-3-simulate-the-counterfactual-pseudo-population",
    "href": "blog/posts/Sudharsanan_Bijlsma_2021.html#step-3-simulate-the-counterfactual-pseudo-population",
    "title": "Sudharsanan & Bijlsma (2021)",
    "section": "step 3: simulate the counterfactual pseudo-population",
    "text": "step 3: simulate the counterfactual pseudo-population\n\n推定したmediatorのモデルにおいて、全員のSESが1だった場合のmediatorの分布を再現\n全員のSESを1にしてmediatorのパラメータを取得\n回帰モデルでは残差の部分は共変量に依存しない（SES間で分布が同じ、平均0・共通の標準偏差の正規分布）\n\nならばSESが1のグループの標準偏差を使わなくても良いのでは？（全体の標準偏差でもよい）\n標準偏差もグループによって異なる、といったモデルの場合にはどうなるか？\n\n\n\n# 平均\npred_mean_m_SES1 &lt;- predict(mediator_model, newdata = data |&gt; mutate(SES = '1'))\n# SES = 1のグループの残差\nresidual_ref_m_SES1 &lt;-\n  broom::augment(mediator_model)  |&gt;\n  filter(SES == '1') |&gt; \n  pull(.resid)\n# 標準偏差\nsd_ref_m_SES1 &lt;- sd(residual_ref_m_SES1)\n\n\n推定したパラメータをもとに、mediatorの値をシミュレート\n\n\ndf_cf_med &lt;- \n  data |&gt; \n  mutate(\n    # ランダム性なし\n    pred_med_SES1 = pred_mean_m_SES1,\n    # ランダム性をもたせる1（推定した標準偏差のパラメータを使用）\n    pred_med_draw_1_SES1 = rnorm(n(), mean = pred_mean_m_SES1, sd = sd_ref_m_SES1),\n    # ランダム性をもたせる2（残差からランダムにサンプリング）\n    pred_med_draw_2_SES1 = pred_mean_m_SES1 + sample(residual_ref_m_SES1, n(), replace = TRUE)\n  )\n\ndf_cf_med |&gt; \n  summarise(\n    across(c(med.gauss, pred_med_SES1:pred_med_draw_2_SES1), mean),\n    .by = SES\n  ) |&gt; \n  arrange(SES) |&gt; \n  kable()\n\n\n\n\n\n\n\n\n\n\n\nSES\nmed.gauss\npred_med_SES1\npred_med_draw_1_SES1\npred_med_draw_2_SES1\n\n\n\n\n1\n8.428\n8.428\n8.386\n8.632\n\n\n2\n7.223\n8.337\n8.258\n8.292\n\n\n3\n5.410\n8.442\n8.292\n8.593\n\n\n\n\n\n\nシミュレートした値をoutcomeモデルに代入して予測値を計算・集計\n\n\ndf_cf_med |&gt; \n  mutate(\n    # ランダム性なし\n    pred_out_SES1 = predict(\n      outcome_model, newdata = df_cf_med |&gt; mutate(med.gauss = pred_med_SES1)\n    ),\n    # ランダム性をもたせる1（推定した標準偏差のパラメータを使用）\n    pred_out_draw_1_SES1 = predict(\n      outcome_model, newdata = df_cf_med |&gt; mutate(med.gauss = pred_med_draw_1_SES1)\n    ),\n    # ランダム性をもたせる2（残差からランダムにサンプリング）\n    pred_out_draw_2_SES1 = predict(\n      outcome_model, newdata = df_cf_med |&gt; mutate(med.gauss = pred_med_draw_2_SES1)\n    )\n  ) |&gt; \n  summarise(\n    across(c(out.gauss, pred_out_SES1:pred_out_draw_2_SES1), mean),\n    .by = SES\n  ) |&gt; \n  arrange(SES) |&gt; \n  kable()\n\n\n\n\n\n\n\n\n\n\n\nSES\nout.gauss\npred_out_SES1\npred_out_draw_1_SES1\npred_out_draw_2_SES1\n\n\n\n\n1\n4.309\n4.309\n4.303\n4.347\n\n\n2\n3.237\n3.507\n3.485\n3.498\n\n\n3\n2.217\n2.949\n2.920\n2.979"
  },
  {
    "objectID": "blog/posts/Sudharsanan_Bijlsma_2021.html#monte-carloとbootstrapの実装",
    "href": "blog/posts/Sudharsanan_Bijlsma_2021.html#monte-carloとbootstrapの実装",
    "title": "Sudharsanan & Bijlsma (2021)",
    "section": "monte carloとbootstrapの実装",
    "text": "monte carloとbootstrapの実装\n\n実際にはrandom drawは一回ではなく何回か行うことで不確実性を表現する\n標準誤差の推定のためにbootstrap法も必要\nまずはtreatmentとoutcomeのモデルを推定し、パラメータを取得\n\n\n# パラメータ推定\nestimate_model &lt;- function(data) {\n  \n  mediator_model &lt;- lm(med.gauss ~ SES * age, data = data)\n  outcome_model &lt;- lm(out.gauss ~ SES * age * med.gauss, data = data)\n  \n  pred_mean_m &lt;- predict(mediator_model, newdata = data, type = \"response\")\n  residual_ref_m &lt;- mediator_model$residuals\n  sd_ref_m &lt;- sd(residual_ref_m)\n\n  pred_mean_m_SES1 &lt;- predict(mediator_model, newdata = data |&gt; mutate(SES = '1'))\n  residual_ref_m_SES1 &lt;-\n    broom::augment(mediator_model)  |&gt;\n    filter(SES == '1') |&gt; \n    pull(.resid)\n  sd_ref_m_SES1 &lt;- sd(residual_ref_m_SES1)\n\n}\n\n\nパラメータをもとにmediatorをシミュレートするのを何回か繰り返す\n\n\nmontecarlo_sampling &lt;- function(data, mc = 10) {\n  # パラメータ推定\n  estimate_model(data)\n  # モンテカルロシミュレーション\n  map(1:mc, \\(mc) {\n    # mediatorサンプリング\n    boot_sample &lt;- \n      data |&gt; \n      mutate(\n        pred_med_draw_1 = rnorm(n(), mean = pred_mean_m, sd = sd_ref_m),\n        pred_med_draw_2 = pred_mean_m + sample(residual_ref_m, n(), replace = TRUE),\n        pred_med_draw_1_SES1 = rnorm(n(), mean = pred_mean_m_SES1, sd = sd_ref_m_SES1),\n        pred_med_draw_2_SES1 = pred_mean_m_SES1 + sample(residual_ref_m_SES1, n(), replace = TRUE),\n      )\n    # サンプリングしたものからoutcome予測\n    boot_sample |&gt; \n      mutate(\n        pred_out_draw_1 = predict(\n          outcome_model, newdata = boot_sample |&gt; mutate(med.gauss = pred_med_draw_1)\n        ),\n        pred_out_draw_2 = predict(\n          outcome_model, newdata = boot_sample |&gt; mutate(med.gauss = pred_med_draw_2)\n        ),\n        pred_out_draw_1_SES1 = predict(\n          outcome_model, newdata = boot_sample |&gt; mutate(med.gauss = pred_med_draw_1_SES1)\n        ),\n        pred_out_draw_2_SES1 = predict(\n          outcome_model, newdata = boot_sample |&gt; mutate(med.gauss = pred_med_draw_2_SES1)\n        )\n      ) |&gt; \n      group_by(SES) |&gt; \n      summarise(across(c(pred_out_draw_1:pred_out_draw_2_SES1), mean))\n    \n  }) |&gt; \n    list_rbind(names_to = 'mc') |&gt; \n    # シミュレーション結果を集計\n    group_by(SES) |&gt; \n    summarise(across(c(pred_out_draw_1:pred_out_draw_2_SES1), mean))\n  \n}\n\n\nこれをbootstrapで繰り返す\n\n\nresult &lt;- \n  map(1:250, \\(index) {\n    # bootstrapサンプル発生\n    bootsample &lt;- slice_sample(data, prop = 1, replace = TRUE)\n    montecarlo_sampling(bootsample, mc = 10) \n  }) |&gt; \n  list_rbind(names_to = 'index')\n\n\n結果を集計\n\n\nresult |&gt; \n  pivot_longer(\n    cols = c(pred_out_draw_1:pred_out_draw_2_SES1), \n    names_to = 'type', \n    values_to = 'value'\n  ) |&gt; \n  summarise(\n    mean = mean(value), \n    conf.low = quantile(value, 0.025),\n    conf.high = quantile(value, 0.975),\n    .by = c(SES, type)\n  ) |&gt; \n  arrange(type) |&gt; \n  kable()\n\n\n\n\nSES\ntype\nmean\nconf.low\nconf.high\n\n\n\n\n1\npred_out_draw_1\n4.015\n3.937\n4.091\n\n\n2\npred_out_draw_1\n3.183\n3.124\n3.251\n\n\n3\npred_out_draw_1\n2.645\n2.599\n2.688\n\n\n1\npred_out_draw_1_SES1\n4.276\n4.193\n4.351\n\n\n2\npred_out_draw_1_SES1\n3.513\n3.446\n3.582\n\n\n3\npred_out_draw_1_SES1\n2.970\n2.934\n3.000\n\n\n1\npred_out_draw_2\n4.015\n3.938\n4.094\n\n\n2\npred_out_draw_2\n3.182\n3.123\n3.253\n\n\n3\npred_out_draw_2\n2.646\n2.600\n2.687\n\n\n1\npred_out_draw_2_SES1\n4.276\n4.183\n4.358\n\n\n2\npred_out_draw_2_SES1\n3.515\n3.457\n3.584\n\n\n3\npred_out_draw_2_SES1\n2.970\n2.941\n2.998"
  },
  {
    "objectID": "blog/posts/Rでロジットモデル.html",
    "href": "blog/posts/Rでロジットモデル.html",
    "title": "Rでロジットモデル",
    "section": "",
    "text": "これは簡単で，2値のfactorを従属変数，リンク関数をglm(family = binomial('logit'))のように指定してやればOK\nちなみにbinomial('probit')にしてやればプロビット回帰になる"
  },
  {
    "objectID": "blog/posts/Rでロジットモデル.html#二項ロジット",
    "href": "blog/posts/Rでロジットモデル.html#二項ロジット",
    "title": "Rでロジットモデル",
    "section": "",
    "text": "これは簡単で，2値のfactorを従属変数，リンク関数をglm(family = binomial('logit'))のように指定してやればOK\nちなみにbinomial('probit')にしてやればプロビット回帰になる"
  },
  {
    "objectID": "blog/posts/Rでロジットモデル.html#多項ロジット",
    "href": "blog/posts/Rでロジットモデル.html#多項ロジット",
    "title": "Rでロジットモデル",
    "section": "多項ロジット",
    "text": "多項ロジット\nnnet::multinomは検定とかやってくれないし，かといってmlogit::mlogitは独自のデータ形式を要求してくるので面倒くさい．ただ，nnetはbroomに対応しているので，検定部分はbroomに任せることでこの問題は解決．\nということでこれからはnnetで推定→broomに放り込んで検定というのを使っていきたい"
  },
  {
    "objectID": "blog/posts/Rでロジットモデル.html#順序ロジット",
    "href": "blog/posts/Rでロジットモデル.html#順序ロジット",
    "title": "Rでロジットモデル",
    "section": "順序ロジット",
    "text": "順序ロジット\nMASS::polrが最有力か？ただし，data.frame形式しか受け付けていないのでtibbleのままデータを入れるとエラーになるのが難点．以前LMestで潜在移行モデルを動かした時にも似たようなことがあった．なんとかならんものか…\nちなみにpolrってなんだろうと思ったら，proportional odds logistic regression（比例オッズロジスティック回帰）の略でした．"
  },
  {
    "objectID": "blog/posts/Rでロジットモデル.html#一般化順序ロジット",
    "href": "blog/posts/Rでロジットモデル.html#一般化順序ロジット",
    "title": "Rでロジットモデル",
    "section": "一般化順序ロジット",
    "text": "一般化順序ロジット\n平行性の仮定を一部緩めた部分比例オッズモデル（partial proportional odds model）1とかをやりたいときは，VGAM::vglmが使える．\nx1，x2，x3のうち，x1だけ平行性の仮定を緩めたいときは，以下のように書く\nVGAM::vglm(formula = y ~ x1 + x2 + x3, \n           family = VGAM::cumulative(parallel = FALSE ~ 1 + x1, reverse = TRUE),\n           data = data) \nparallelの書き方がミソで，parallel = FALSEのあとに等値制約を緩めたいパラメタをformulaで記述する．今回の場合は切片（閾値）とx1の係数だけは各段階で異なるので~ 1 + x1と記述する．また，parallel = TRUEとすると通常の順序ロジットモデルになる．\nreverseは係数の符号を反転させるオプションで，reverse = TRUEを入れるとStataのgologit2を用いた時の結果と同じになる．"
  },
  {
    "objectID": "blog/posts/Rでロジットモデル.html#footnotes",
    "href": "blog/posts/Rでロジットモデル.html#footnotes",
    "title": "Rでロジットモデル",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n教育社会学での使用例は近藤・古田（2009）とか．↩︎"
  },
  {
    "objectID": "blog/posts/学校外教育参加率.html",
    "href": "blog/posts/学校外教育参加率.html",
    "title": "学校外教育の参加率",
    "section": "",
    "text": "library(tidyverse)\nlibrary(estatapi)"
  },
  {
    "objectID": "blog/posts/学校外教育参加率.html#パッケージ読み込み",
    "href": "blog/posts/学校外教育参加率.html#パッケージ読み込み",
    "title": "学校外教育の参加率",
    "section": "",
    "text": "library(tidyverse)\nlibrary(estatapi)"
  },
  {
    "objectID": "blog/posts/学校外教育参加率.html#準備",
    "href": "blog/posts/学校外教育参加率.html#準備",
    "title": "学校外教育の参加率",
    "section": "準備",
    "text": "準備\n\nあらかじめE-Stat APIの申請を行い、アプリケーションIDを取得しておく必要がある\n.Renvironや.Rprofileあたりで、環境変数ESTAT_APPIDにアプリケーションIDを設定しておくと便利\n\n\n# E-Stat APIのアプリケーションIDを環境変数から取得\nappid &lt;- Sys.getenv('ESTAT_APPID')"
  },
  {
    "objectID": "blog/posts/学校外教育参加率.html#メタデータ取得",
    "href": "blog/posts/学校外教育参加率.html#メタデータ取得",
    "title": "学校外教育の参加率",
    "section": "メタデータ取得",
    "text": "メタデータ取得\n使用するデータは以下の3つ\n\n「子供の学習費調査、3　学年（年齢）別，所在市町村の人口規模（学科）別の学習費支出状況 小学校」https://e-stat.go.jp/stat-search/database?page=1&statdisp_id=0003368820\n「子供の学習費調査、3　学年（年齢）別，所在市町村の人口規模（学科）別の学習費支出状況 中学校」https://e-stat.go.jp/stat-search/database?page=1&statdisp_id=0003368821\n「子供の学習費調査、3　学年（年齢）別，所在市町村の人口規模（学科）別の学習費支出状況 高等学校（全日制）」https://e-stat.go.jp/stat-search/database?page=1&statdisp_id=0003368822\n\n\n# 以下のコードでメタデータが取得できる\nestat_getMetaInfo(appid, '0003368820')\nestat_getMetaInfo(appid, '0003368821')\nestat_getMetaInfo(appid, '0003368822')"
  },
  {
    "objectID": "blog/posts/学校外教育参加率.html#データ取得",
    "href": "blog/posts/学校外教育参加率.html#データ取得",
    "title": "学校外教育の参加率",
    "section": "データ取得",
    "text": "データ取得\n\nメタデータを参照しつつ必要な情報のみを取得\n\n\nelementary &lt;- \n  estat_getStatsData(\n    appid, \n    '0003368820',\n    cdTab = '110', \n    cdCat01 = '160,170,180,190,200,210', \n    cdCat02 = '320'\n  )\n\njhigh &lt;- \n  estat_getStatsData(\n    appid, \n    '0003368821',\n    cdTab = '110', \n    cdCat01 = '160,170,180', \n    cdCat02 = '320'\n  )\n\nhigh &lt;- \n  estat_getStatsData(\n    appid, \n    '0003368822',\n    cdTab = '110', \n    cdCat01 = '160,170,180', \n    cdCat02 = '320'\n  )"
  },
  {
    "objectID": "blog/posts/学校外教育参加率.html#データ整形プロット",
    "href": "blog/posts/学校外教育参加率.html#データ整形プロット",
    "title": "学校外教育の参加率",
    "section": "データ整形＆プロット",
    "text": "データ整形＆プロット\n\nbind_rows(\n  小学校 = elementary,\n  中学校 = jhigh,\n  高校 = high,\n  .id = '学校段階'\n) |&gt; \n  filter(`時間軸（年度次）` == '2016年度') |&gt; \n  filter(!(学校段階 == '小学校' & `学年(年齢)・所在市町村人口規模(学科)` %in% c('第1学年', '第2学年', '第3学年'))) |&gt; \n  \n  ggplot(aes(`学年(年齢)・所在市町村人口規模(学科)`, value))+\n  geom_bar(stat = 'identity')+\n  geom_label(aes(label = value), size = 3)+\n  facet_grid(rows = vars(公私立区分), cols = vars(学校段階), scales = 'free_x')+\n  labs(x = '学年', y = '学習塾利用率')+\n  theme_bw(base_family = 'Noto Sans JP')"
  },
  {
    "objectID": "blog/posts/学生生活実態調査の文字コード.html",
    "href": "blog/posts/学生生活実態調査の文字コード.html",
    "title": "学生生活実態調査の文字コード",
    "section": "",
    "text": "文字コードがカオスで調べるのが大変だったのでメモ\n\n\n\n\n\n\n\n\n\n年次\n当該データ\n文字コード\nURL\n\n\n\n\n1991\n0078.sav\ncp932\nhttps://ssjda.iss.u-tokyo.ac.jp/Direct/gaiyo.php?eid=0078\n\n\n1992\n0079.sav\ncp932\nhttps://ssjda.iss.u-tokyo.ac.jp/Direct/gaiyo.php?eid=0079\n\n\n1993\n0053.sav\ncp932\nhttps://ssjda.iss.u-tokyo.ac.jp/Direct/gaiyo.php?eid=0053\n\n\n1994\n0080.sav\ncp932\nhttps://ssjda.iss.u-tokyo.ac.jp/Direct/gaiyo.php?eid=0080\n\n\n1995\n0125.sav\ncp932\nhttps://ssjda.iss.u-tokyo.ac.jp/Direct/gaiyo.php?eid=0125\n\n\n1996\n0126.sav\ncp932\nhttps://ssjda.iss.u-tokyo.ac.jp/Direct/gaiyo.php?eid=0126\n\n\n1997\n0127.sav\ncp932\nhttps://ssjda.iss.u-tokyo.ac.jp/Direct/gaiyo.php?eid=0127\n\n\n1998\n0128.sav\ncp932\nhttps://ssjda.iss.u-tokyo.ac.jp/Direct/gaiyo.php?eid=0128\n\n\n1999\n0157.sav\ncp932\nhttps://ssjda.iss.u-tokyo.ac.jp/Direct/gaiyo.php?eid=0157\n\n\n2000\n0201.sav\ncp932\nhttps://ssjda.iss.u-tokyo.ac.jp/Direct/gaiyo.php?eid=0201\n\n\n2001\n0267.sav\ncp932\nhttps://ssjda.iss.u-tokyo.ac.jp/Direct/gaiyo.php?eid=0267\n\n\n2002\n0292.sav\ncp932\nhttps://ssjda.iss.u-tokyo.ac.jp/Direct/gaiyo.php?eid=0292\n\n\n2003\n0345.sav\ncp932\nhttps://ssjda.iss.u-tokyo.ac.jp/Direct/gaiyo.php?eid=0345\n\n\n2004\n0399.sav\ncp932\nhttps://ssjda.iss.u-tokyo.ac.jp/Direct/gaiyo.php?eid=0399\n\n\n2005\n0519.sav\ncp932\nhttps://ssjda.iss.u-tokyo.ac.jp/Direct/gaiyo.php?eid=0519\n\n\n2006\n0562.sav\ncp932\nhttps://ssjda.iss.u-tokyo.ac.jp/Direct/gaiyo.php?eid=0562\n\n\n2007\n0605.sav\ncp932\nhttps://ssjda.iss.u-tokyo.ac.jp/Direct/gaiyo.php?eid=0605\n\n\n2008\n0664.sav\ncp932\nhttps://ssjda.iss.u-tokyo.ac.jp/Direct/gaiyo.php?eid=0664\n\n\n2009\n0753.sav\ncp932\nhttps://ssjda.iss.u-tokyo.ac.jp/Direct/gaiyo.php?eid=0753\n\n\n2010\n0812.sav\ncp932\nhttps://ssjda.iss.u-tokyo.ac.jp/Direct/gaiyo.php?eid=0812\n\n\n2011\n0841.sav\ncp932\nhttps://ssjda.iss.u-tokyo.ac.jp/Direct/gaiyo.php?eid=0841\n\n\n2012\n0879.sav\ncp932\nhttps://ssjda.iss.u-tokyo.ac.jp/Direct/gaiyo.php?eid=0879\n\n\n2013\n0955.sav\nutf8\nhttps://ssjda.iss.u-tokyo.ac.jp/Direct/gaiyo.php?eid=0955\n\n\n2014\n1057.sav\nutf8\nhttps://ssjda.iss.u-tokyo.ac.jp/Direct/gaiyo.php?eid=1057\n\n\n2015\n1099.sav\ncp932\nhttps://ssjda.iss.u-tokyo.ac.jp/Direct/gaiyo.php?eid=1099\n\n\n2016\n1163.sav\ncp932\nhttps://ssjda.iss.u-tokyo.ac.jp/Direct/gaiyo.php?eid=1163\n\n\n2017\n1232.sav\ncp932\nhttps://ssjda.iss.u-tokyo.ac.jp/Direct/gaiyo.php?eid=1232\n\n\n2018\n1295.sav\nutf8\nhttps://ssjda.iss.u-tokyo.ac.jp/Direct/gaiyo.php?eid=1295\n\n\n2019\n1384.sav\ncp932\nhttps://ssjda.iss.u-tokyo.ac.jp/Direct/gaiyo.php?eid=1384\n\n\n2020\n1442.sav\ncp932\nhttps://ssjda.iss.u-tokyo.ac.jp/Direct/gaiyo.php?eid=1442"
  },
  {
    "objectID": "blog/posts/L-TMLE.html",
    "href": "blog/posts/L-TMLE.html",
    "title": "Longitudinal TMLE",
    "section": "",
    "text": "Hernán and Robins (2020) では、TMLEは時間依存交絡などに対処するG-Methodの一般化として提示されている。ここではltmleパッケージを用いた時間依存交絡への対処を中心に記述する。\n\n\n\n\n\nlibrary(tidyverse)\nlibrary(broom)\nlibrary(SuperLearner)\nlibrary(ltmle)\nlibrary(randomForest)\nlibrary(ggdag)\nlibrary(future)\n\nkable &lt;- partial(\n  knitr::kable,\n  digits = 3\n)\n\noptions(mc.cores = 8)\nplan(strategy = 'multisession')\n\nset.seed(95)\n\n\n\n\n以下のDAGを考える\n\ndagify(\n  L1 ~ A0 + U1,\n  A1 ~ A0 + L1,\n  Y ~ U1,\n  exposure = c('A0', 'A1'), \n  outcome = 'Y', \n  coords = list(x = c(A0 = 0, L1 = 1, A1 = 2, U1 = 1, Y = 3),\n                y = c(A0 = 0, L1 = 0, A1 = 0, U1 = -1, Y = 0))\n) |&gt; \n  ggdag()+\n  theme_dag()\n\n\n\n\n\n\n\n\nTreatmentはA0とA1であるが、ここからYへのpathはないので、因果効果は全ての組み合わせについて0になる\n具体的には、potential outcomeのすべての組み合わせについて、差分を取った値が0になる\n\\[\\begin{align}\n\\mathrm{E}[Y^{0, 0} - Y^{1, 0}] &= 0\\\\\n\\mathrm{E}[Y^{0, 0} - Y^{0, 1}] &= 0\\\\\n\\mathrm{E}[Y^{0, 0} - Y^{1, 1}] &= 0\\\\\n\\mathrm{E}[Y^{1, 0} - Y^{0, 1}] &= 0\\\\\n\\mathrm{E}[Y^{1, 0} - Y^{1, 1}] &= 0\\\\\n\\mathrm{E}[Y^{0, 1} - Y^{1, 1}] &= 0\n\\end{align}\\]\n\ngenerate_data &lt;- function(n){ \n    A0 &lt;- rbinom(n, size=1, prob=0.3) # binary treatment\n    U1 &lt;- rnorm(n, mean = 0, sd = 1) # latent continuous confounder\n    L1 &lt;- 0.1 + 0.3*A0 + 0.3*U1 + rnorm(n, sd = 0.1)\n    A1 &lt;- rbinom(n, size=1, prob = plogis(-0.2 + 0.3*L1 + 0.02*L1^2)) # binary treatment\n    Y &lt;- -0.2 + 0.5*U1 + rnorm(n, sd = 0.1) # continuous outcome depends on confounders\n    return(tibble(Y, A0, A1, L1, U1))\n}\n\ndata_obs &lt;- generate_data(1000)\n\n\nA0の因果効果\n\n\ndata_obs |&gt; \n  lm(Y ~ A0, data = _) |&gt; \n  tidy() |&gt; \n  kable()\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-0.207\n0.019\n-10.925\n0.000\n\n\nA0\n0.029\n0.036\n0.804\n0.422\n\n\n\n\n\n\nA1の因果効果\n\n\ndata_obs |&gt; \n  lm(Y ~ A1 + L1, data = _) |&gt; \n  tidy() |&gt; \n  kable()\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-0.427\n0.012\n-34.532\n0.000\n\n\nA1\n-0.017\n0.017\n-1.020\n0.308\n\n\nL1\n1.275\n0.025\n51.719\n0.000\n\n\n\n\n\n\ncollider bias\n\n\\(L_1\\)を条件づけると、\\(A_0 \\to L_1 \\gets U_1 \\to Y\\)というパスが開いて、バイアスをもたらす\n\ndata_obs |&gt; \n  lm(Y ~ A0 + L1, data = _) |&gt; \n  tidy() |&gt; \n  kable()\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-0.352\n0.007\n-49.626\n0\n\n\nA0\n-0.469\n0.014\n-32.796\n0\n\n\nL1\n1.540\n0.019\n81.391\n0\n\n\n\n\n\n\ndata_obs |&gt; \n  lm(Y ~ A0*A1, data = _) |&gt; \n  tidy() |&gt; \n  kable()\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-0.217\n0.026\n-8.364\n0.000\n\n\nA0\n0.043\n0.051\n0.859\n0.390\n\n\nA1\n0.021\n0.038\n0.559\n0.576\n\n\nA0:A1\n-0.031\n0.072\n-0.428\n0.669\n\n\n\n\n\n\n\n\n\nHernán and Robins (2020) にて紹介されている方法\n以下のウェイトを作成する\n\\[\nW^{A_0, A_1} = \\frac{1}{f(A_0 | L_0)} \\times \\frac{1}{f(A_1 | A_0, L_0, L_1)}\n\\]\n今回はベースライン共変量がない（\\(L_0 = \\varnothing\\)）ので\n\\[\nW^{A_0, A_1} = \\frac{1}{f(A_0)} \\times \\frac{1}{f(A_1 | A_0, L_1)}\n\\]\n\nres_ipw &lt;-\n  broom::augment_columns(\n    glm(A0 ~ 1, data = data_obs, family = 'binomial'), \n    data = data_obs, \n    type.predict = 'response'\n  ) |&gt; \n  # A0に対するウェイト\n  mutate(\n    ipw_A0 = case_when(\n      A0 == 1 ~ 1 / .fitted,\n      A0 == 0 ~ 1 / (1 - .fitted)\n    )\n  ) |&gt; \n  select(Y:U1, ipw_A0) |&gt; \n  broom::augment_columns(\n    glm(A1 ~ A0 + L1 + I(L1^2), data = data_obs, family = 'binomial'),\n    data = _,\n    type.predict = 'response'\n  ) |&gt; \n  mutate(\n    # A1に対するウェイト\n    ipw_A1 = case_when(\n      A1 == 1 ~ 1 / .fitted,\n      A1 == 0 ~ 1 / (1 - .fitted)\n    )\n  ) |&gt; \n  select(Y:U1, ipw_A0, ipw_A1) |&gt; \n  # A0に対するウェイトとA1に対するウェイトをかけ算\n  mutate(ipw = ipw_A0*ipw_A1) |&gt; \n  # 重み付け推定\n  summarise(CFmean = weighted.mean(Y, ipw), .by = c(A0, A1)) |&gt; \n  arrange(A0, A1)\n\nipwによる各treatment strategyにおけるcounterfactual mean\n\nres_ipw\n\n# A tibble: 4 × 3\n     A0    A1 CFmean\n  &lt;int&gt; &lt;int&gt;  &lt;dbl&gt;\n1     0     0 -0.208\n2     0     1 -0.207\n3     1     0 -0.166\n4     1     1 -0.189\n\n\ncounterfactual meanの差分\n\ncross_join(\n  res_ipw |&gt; \n    mutate(A0_A1 = str_c(A0, ',', A1)) |&gt; \n    select(A0_A1, CFmean),\n  res_ipw |&gt; \n    mutate(A0_A1 = str_c(A0, ',', A1)) |&gt; \n    select(A0_A1, CFmean)\n  ) |&gt; \n  filter(A0_A1.x != A0_A1.y) |&gt; \n  mutate(ATE = CFmean.y - CFmean.x)\n\n# A tibble: 12 × 5\n   A0_A1.x CFmean.x A0_A1.y CFmean.y       ATE\n   &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;      &lt;dbl&gt;     &lt;dbl&gt;\n 1 0,0       -0.208 0,1       -0.207  0.000654\n 2 0,0       -0.208 1,0       -0.166  0.0419  \n 3 0,0       -0.208 1,1       -0.189  0.0190  \n 4 0,1       -0.207 0,0       -0.208 -0.000654\n 5 0,1       -0.207 1,0       -0.166  0.0412  \n 6 0,1       -0.207 1,1       -0.189  0.0184  \n 7 1,0       -0.166 0,0       -0.208 -0.0419  \n 8 1,0       -0.166 0,1       -0.207 -0.0412  \n 9 1,0       -0.166 1,1       -0.189 -0.0229  \n10 1,1       -0.189 0,0       -0.208 -0.0190  \n11 1,1       -0.189 0,1       -0.207 -0.0184  \n12 1,1       -0.189 1,0       -0.166  0.0229  \n\n\n\n\n\n\nltmleによる実装\n\n\nres_tmle &lt;- \n  ltmle::ltmle(\n    # データセットの列の順番が大切\n    data = data_obs |&gt; select(A0, L1, A1, Y), \n    Anodes = c('A0', 'A1'), \n    Lnodes = 'L1',\n    Ynodes = 'Y',\n    abar = list(treatment = c(1, 1), control = c(0, 0)), \n    SL.library = c('SL.glm', 'SL.gam', 'SL.randomForest')\n  )\n\n\nsummary(res_tmle)\n\nEstimator:  tmle \nCall:\nltmle::ltmle(data = select(data_obs, A0, L1, A1, Y), Anodes = c(\"A0\", \n    \"A1\"), Lnodes = \"L1\", Ynodes = \"Y\", abar = list(treatment = c(1, \n    1), control = c(0, 0)), SL.library = c(\"SL.glm\", \"SL.gam\", \n    \"SL.randomForest\"))\n\nTreatment Estimate:\n   Parameter Estimate:  -0.17554 \n    Estimated Std Err:  0.033541 \n              p-value:  &lt;2e-16 \n    95% Conf Interval: (-0.24128, -0.1098) \n\nControl Estimate:\n   Parameter Estimate:  -0.20361 \n    Estimated Std Err:  0.019777 \n              p-value:  &lt;2e-16 \n    95% Conf Interval: (-0.24237, -0.16485) \n\nAdditive Treatment Effect:\n   Parameter Estimate:  0.028069 \n    Estimated Std Err:  0.038937 \n              p-value:  0.47099 \n    95% Conf Interval: (-0.048247, 0.10438) \n\n\n\n\n\nltmleはメジャー（？）だが、あんまり汎用性が高いと言えず、開発も盛んなようには見えない。ほかにlongitudinal TMLEを実装しているパッケージとしてlmtpやstremrがある。lmtpはCRANにも登録してあり、試してみる価値あり。stremrはさまざまなアプローチの統合を目指す非常に意欲的なパッケージだが、開発は止まっている？っぽい。"
  },
  {
    "objectID": "blog/posts/L-TMLE.html#下準備",
    "href": "blog/posts/L-TMLE.html#下準備",
    "title": "Longitudinal TMLE",
    "section": "",
    "text": "library(tidyverse)\nlibrary(broom)\nlibrary(SuperLearner)\nlibrary(ltmle)\nlibrary(randomForest)\nlibrary(ggdag)\nlibrary(future)\n\nkable &lt;- partial(\n  knitr::kable,\n  digits = 3\n)\n\noptions(mc.cores = 8)\nplan(strategy = 'multisession')\n\nset.seed(95)\n\n\n\n\n以下のDAGを考える\n\ndagify(\n  L1 ~ A0 + U1,\n  A1 ~ A0 + L1,\n  Y ~ U1,\n  exposure = c('A0', 'A1'), \n  outcome = 'Y', \n  coords = list(x = c(A0 = 0, L1 = 1, A1 = 2, U1 = 1, Y = 3),\n                y = c(A0 = 0, L1 = 0, A1 = 0, U1 = -1, Y = 0))\n) |&gt; \n  ggdag()+\n  theme_dag()\n\n\n\n\n\n\n\n\nTreatmentはA0とA1であるが、ここからYへのpathはないので、因果効果は全ての組み合わせについて0になる\n具体的には、potential outcomeのすべての組み合わせについて、差分を取った値が0になる\n\\[\\begin{align}\n\\mathrm{E}[Y^{0, 0} - Y^{1, 0}] &= 0\\\\\n\\mathrm{E}[Y^{0, 0} - Y^{0, 1}] &= 0\\\\\n\\mathrm{E}[Y^{0, 0} - Y^{1, 1}] &= 0\\\\\n\\mathrm{E}[Y^{1, 0} - Y^{0, 1}] &= 0\\\\\n\\mathrm{E}[Y^{1, 0} - Y^{1, 1}] &= 0\\\\\n\\mathrm{E}[Y^{0, 1} - Y^{1, 1}] &= 0\n\\end{align}\\]\n\ngenerate_data &lt;- function(n){ \n    A0 &lt;- rbinom(n, size=1, prob=0.3) # binary treatment\n    U1 &lt;- rnorm(n, mean = 0, sd = 1) # latent continuous confounder\n    L1 &lt;- 0.1 + 0.3*A0 + 0.3*U1 + rnorm(n, sd = 0.1)\n    A1 &lt;- rbinom(n, size=1, prob = plogis(-0.2 + 0.3*L1 + 0.02*L1^2)) # binary treatment\n    Y &lt;- -0.2 + 0.5*U1 + rnorm(n, sd = 0.1) # continuous outcome depends on confounders\n    return(tibble(Y, A0, A1, L1, U1))\n}\n\ndata_obs &lt;- generate_data(1000)\n\n\nA0の因果効果\n\n\ndata_obs |&gt; \n  lm(Y ~ A0, data = _) |&gt; \n  tidy() |&gt; \n  kable()\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-0.207\n0.019\n-10.925\n0.000\n\n\nA0\n0.029\n0.036\n0.804\n0.422\n\n\n\n\n\n\nA1の因果効果\n\n\ndata_obs |&gt; \n  lm(Y ~ A1 + L1, data = _) |&gt; \n  tidy() |&gt; \n  kable()\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-0.427\n0.012\n-34.532\n0.000\n\n\nA1\n-0.017\n0.017\n-1.020\n0.308\n\n\nL1\n1.275\n0.025\n51.719\n0.000\n\n\n\n\n\n\ncollider bias\n\n\\(L_1\\)を条件づけると、\\(A_0 \\to L_1 \\gets U_1 \\to Y\\)というパスが開いて、バイアスをもたらす\n\ndata_obs |&gt; \n  lm(Y ~ A0 + L1, data = _) |&gt; \n  tidy() |&gt; \n  kable()\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-0.352\n0.007\n-49.626\n0\n\n\nA0\n-0.469\n0.014\n-32.796\n0\n\n\nL1\n1.540\n0.019\n81.391\n0\n\n\n\n\n\n\ndata_obs |&gt; \n  lm(Y ~ A0*A1, data = _) |&gt; \n  tidy() |&gt; \n  kable()\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-0.217\n0.026\n-8.364\n0.000\n\n\nA0\n0.043\n0.051\n0.859\n0.390\n\n\nA1\n0.021\n0.038\n0.559\n0.576\n\n\nA0:A1\n-0.031\n0.072\n-0.428\n0.669"
  },
  {
    "objectID": "blog/posts/L-TMLE.html#ipw",
    "href": "blog/posts/L-TMLE.html#ipw",
    "title": "Longitudinal TMLE",
    "section": "",
    "text": "Hernán and Robins (2020) にて紹介されている方法\n以下のウェイトを作成する\n\\[\nW^{A_0, A_1} = \\frac{1}{f(A_0 | L_0)} \\times \\frac{1}{f(A_1 | A_0, L_0, L_1)}\n\\]\n今回はベースライン共変量がない（\\(L_0 = \\varnothing\\)）ので\n\\[\nW^{A_0, A_1} = \\frac{1}{f(A_0)} \\times \\frac{1}{f(A_1 | A_0, L_1)}\n\\]\n\nres_ipw &lt;-\n  broom::augment_columns(\n    glm(A0 ~ 1, data = data_obs, family = 'binomial'), \n    data = data_obs, \n    type.predict = 'response'\n  ) |&gt; \n  # A0に対するウェイト\n  mutate(\n    ipw_A0 = case_when(\n      A0 == 1 ~ 1 / .fitted,\n      A0 == 0 ~ 1 / (1 - .fitted)\n    )\n  ) |&gt; \n  select(Y:U1, ipw_A0) |&gt; \n  broom::augment_columns(\n    glm(A1 ~ A0 + L1 + I(L1^2), data = data_obs, family = 'binomial'),\n    data = _,\n    type.predict = 'response'\n  ) |&gt; \n  mutate(\n    # A1に対するウェイト\n    ipw_A1 = case_when(\n      A1 == 1 ~ 1 / .fitted,\n      A1 == 0 ~ 1 / (1 - .fitted)\n    )\n  ) |&gt; \n  select(Y:U1, ipw_A0, ipw_A1) |&gt; \n  # A0に対するウェイトとA1に対するウェイトをかけ算\n  mutate(ipw = ipw_A0*ipw_A1) |&gt; \n  # 重み付け推定\n  summarise(CFmean = weighted.mean(Y, ipw), .by = c(A0, A1)) |&gt; \n  arrange(A0, A1)\n\nipwによる各treatment strategyにおけるcounterfactual mean\n\nres_ipw\n\n# A tibble: 4 × 3\n     A0    A1 CFmean\n  &lt;int&gt; &lt;int&gt;  &lt;dbl&gt;\n1     0     0 -0.208\n2     0     1 -0.207\n3     1     0 -0.166\n4     1     1 -0.189\n\n\ncounterfactual meanの差分\n\ncross_join(\n  res_ipw |&gt; \n    mutate(A0_A1 = str_c(A0, ',', A1)) |&gt; \n    select(A0_A1, CFmean),\n  res_ipw |&gt; \n    mutate(A0_A1 = str_c(A0, ',', A1)) |&gt; \n    select(A0_A1, CFmean)\n  ) |&gt; \n  filter(A0_A1.x != A0_A1.y) |&gt; \n  mutate(ATE = CFmean.y - CFmean.x)\n\n# A tibble: 12 × 5\n   A0_A1.x CFmean.x A0_A1.y CFmean.y       ATE\n   &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;      &lt;dbl&gt;     &lt;dbl&gt;\n 1 0,0       -0.208 0,1       -0.207  0.000654\n 2 0,0       -0.208 1,0       -0.166  0.0419  \n 3 0,0       -0.208 1,1       -0.189  0.0190  \n 4 0,1       -0.207 0,0       -0.208 -0.000654\n 5 0,1       -0.207 1,0       -0.166  0.0412  \n 6 0,1       -0.207 1,1       -0.189  0.0184  \n 7 1,0       -0.166 0,0       -0.208 -0.0419  \n 8 1,0       -0.166 0,1       -0.207 -0.0412  \n 9 1,0       -0.166 1,1       -0.189 -0.0229  \n10 1,1       -0.189 0,0       -0.208 -0.0190  \n11 1,1       -0.189 0,1       -0.207 -0.0184  \n12 1,1       -0.189 1,0       -0.166  0.0229"
  },
  {
    "objectID": "blog/posts/L-TMLE.html#longitudinal-tmle",
    "href": "blog/posts/L-TMLE.html#longitudinal-tmle",
    "title": "Longitudinal TMLE",
    "section": "",
    "text": "ltmleによる実装\n\n\nres_tmle &lt;- \n  ltmle::ltmle(\n    # データセットの列の順番が大切\n    data = data_obs |&gt; select(A0, L1, A1, Y), \n    Anodes = c('A0', 'A1'), \n    Lnodes = 'L1',\n    Ynodes = 'Y',\n    abar = list(treatment = c(1, 1), control = c(0, 0)), \n    SL.library = c('SL.glm', 'SL.gam', 'SL.randomForest')\n  )\n\n\nsummary(res_tmle)\n\nEstimator:  tmle \nCall:\nltmle::ltmle(data = select(data_obs, A0, L1, A1, Y), Anodes = c(\"A0\", \n    \"A1\"), Lnodes = \"L1\", Ynodes = \"Y\", abar = list(treatment = c(1, \n    1), control = c(0, 0)), SL.library = c(\"SL.glm\", \"SL.gam\", \n    \"SL.randomForest\"))\n\nTreatment Estimate:\n   Parameter Estimate:  -0.17554 \n    Estimated Std Err:  0.033541 \n              p-value:  &lt;2e-16 \n    95% Conf Interval: (-0.24128, -0.1098) \n\nControl Estimate:\n   Parameter Estimate:  -0.20361 \n    Estimated Std Err:  0.019777 \n              p-value:  &lt;2e-16 \n    95% Conf Interval: (-0.24237, -0.16485) \n\nAdditive Treatment Effect:\n   Parameter Estimate:  0.028069 \n    Estimated Std Err:  0.038937 \n              p-value:  0.47099 \n    95% Conf Interval: (-0.048247, 0.10438)"
  },
  {
    "objectID": "blog/posts/L-TMLE.html#他のパッケージ",
    "href": "blog/posts/L-TMLE.html#他のパッケージ",
    "title": "Longitudinal TMLE",
    "section": "",
    "text": "ltmleはメジャー（？）だが、あんまり汎用性が高いと言えず、開発も盛んなようには見えない。ほかにlongitudinal TMLEを実装しているパッケージとしてlmtpやstremrがある。lmtpはCRANにも登録してあり、試してみる価値あり。stremrはさまざまなアプローチの統合を目指す非常に意欲的なパッケージだが、開発は止まっている？っぽい。"
  },
  {
    "objectID": "blog/posts/multi_answer.html",
    "href": "blog/posts/multi_answer.html",
    "title": "マルチアンサーの処理",
    "section": "",
    "text": "library(tidyverse)"
  },
  {
    "objectID": "blog/posts/multi_answer.html#パッケージ",
    "href": "blog/posts/multi_answer.html#パッケージ",
    "title": "マルチアンサーの処理",
    "section": "",
    "text": "library(tidyverse)"
  },
  {
    "objectID": "blog/posts/multi_answer.html#googleフォームなどのマルチアンサーの処理",
    "href": "blog/posts/multi_answer.html#googleフォームなどのマルチアンサーの処理",
    "title": "マルチアンサーの処理",
    "section": "Googleフォームなどのマルチアンサーの処理",
    "text": "Googleフォームなどのマルチアンサーの処理\ngoogleフォームでマルチアンサー形式の設問を作ると、以下のようにカンマで区切られた形式でデータが入力される。\n\ndata &lt;- \n  tibble(\n    q1 = c('A,B,C', 'A,B', 'A,C', 'B,C', 'A', 'B', 'C'),\n  ) |&gt; \n  mutate(id = row_number()) |&gt; \n  relocate(id, .before = q1) \n\ndata\n\n# A tibble: 7 × 2\n     id q1   \n  &lt;int&gt; &lt;chr&gt;\n1     1 A,B,C\n2     2 A,B  \n3     3 A,C  \n4     4 B,C  \n5     5 A    \n6     6 B    \n7     7 C    \n\n\nまずはカンマで区切られているやつを分割する。\n\ndata |&gt; \n  separate_longer_delim(cols = q1, delim = ',') \n\n# A tibble: 12 × 2\n      id q1   \n   &lt;int&gt; &lt;chr&gt;\n 1     1 A    \n 2     1 B    \n 3     1 C    \n 4     2 A    \n 5     2 B    \n 6     3 A    \n 7     3 C    \n 8     4 B    \n 9     4 C    \n10     5 A    \n11     6 B    \n12     7 C    \n\n\nこのデータの意味自体は、id:1はA,B,Cを選択した、というものである。\nなのでフラグとして1を立てておく。\n\ndata |&gt; \n  separate_longer_delim(cols = q1, delim = ',') |&gt;\n  mutate(flag = 1)\n\n# A tibble: 12 × 3\n      id q1     flag\n   &lt;int&gt; &lt;chr&gt; &lt;dbl&gt;\n 1     1 A         1\n 2     1 B         1\n 3     1 C         1\n 4     2 A         1\n 5     2 B         1\n 6     3 A         1\n 7     3 C         1\n 8     4 B         1\n 9     4 C         1\n10     5 A         1\n11     6 B         1\n12     7 C         1\n\n\nつぎにこれを横持ちに展開する。\n\ndata |&gt; \n  separate_longer_delim(cols = q1, delim = ',') |&gt;\n  mutate(flag = 1) |&gt; \n  pivot_wider(names_from = q1, values_from = flag)\n\n# A tibble: 7 × 4\n     id     A     B     C\n  &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     1     1     1     1\n2     2     1     1    NA\n3     3     1    NA     1\n4     4    NA     1     1\n5     5     1    NA    NA\n6     6    NA     1    NA\n7     7    NA    NA     1\n\n\nこれで、A,B,Cの選択状況がわかるようになった。\n非選択がNAになっているのは少々アレなので、0で埋めるようにする。\n\ndata |&gt; \n  separate_longer_delim(cols = q1, delim = ',') |&gt;\n  mutate(flag = 1) |&gt; \n  pivot_wider(names_from = q1, values_from = flag, values_fill = 0)\n\n# A tibble: 7 × 4\n     id     A     B     C\n  &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     1     1     1     1\n2     2     1     1     0\n3     3     1     0     1\n4     4     0     1     1\n5     5     1     0     0\n6     6     0     1     0\n7     7     0     0     1\n\n\nこれでダミー変数に変換できた。\nもともとの変数名であるq1が消えてしまっているので、それを残すにはnames_prefixを指定する。\n\ndata |&gt; \n  separate_longer_delim(cols = q1, delim = ',') |&gt;\n  mutate(flag = 1) |&gt; \n  pivot_wider(\n    names_from = q1, \n    names_prefix = 'q1_', \n    values_from = flag, \n    values_fill = 0\n  ) \n\n# A tibble: 7 × 4\n     id  q1_A  q1_B  q1_C\n  &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     1     1     1     1\n2     2     1     1     0\n3     3     1     0     1\n4     4     0     1     1\n5     5     1     0     0\n6     6     0     1     0\n7     7     0     0     1"
  },
  {
    "objectID": "blog/posts/Manual-Superlearner.html",
    "href": "blog/posts/Manual-Superlearner.html",
    "title": "手動でSuperlearner",
    "section": "",
    "text": "Naimi, Ashley I. & Laura B. Balzer, 2018, “Stacked Generalization: An Introduction to Super Learning,” European Journal of Epidemiology, 33(5): 459–64, (https://doi.org/10.1007/s10654-018-0390-z)."
  },
  {
    "objectID": "blog/posts/Manual-Superlearner.html#文献",
    "href": "blog/posts/Manual-Superlearner.html#文献",
    "title": "手動でSuperlearner",
    "section": "",
    "text": "Naimi, Ashley I. & Laura B. Balzer, 2018, “Stacked Generalization: An Introduction to Super Learning,” European Journal of Epidemiology, 33(5): 459–64, (https://doi.org/10.1007/s10654-018-0390-z)."
  },
  {
    "objectID": "blog/posts/Manual-Superlearner.html#パッケージ読み込み",
    "href": "blog/posts/Manual-Superlearner.html#パッケージ読み込み",
    "title": "手動でSuperlearner",
    "section": "パッケージ読み込み",
    "text": "パッケージ読み込み\n\nlibrary(tidyverse)\nlibrary(rsample)\nlibrary(sl3)\nlibrary(future)\nlibrary(earth)\n\ntheme_set(theme_bw(base_family = 'Noto Sans JP'))\n\n# 並列化\nplan(multisession(workers = 5))"
  },
  {
    "objectID": "blog/posts/Manual-Superlearner.html#データ生成",
    "href": "blog/posts/Manual-Superlearner.html#データ生成",
    "title": "手動でSuperlearner",
    "section": "データ生成",
    "text": "データ生成\n\\[\\begin{align}\nY = 5 + 4\\sqrt{9x} \\times I(x&lt;2) + I(x\\geq2) \\times (|x-6|^{2}) + \\epsilon\n\\end{align}\\]\n\n\\(I()\\)：Indicator function（TRUEなら1、FALSEなら0となる）\n\\(\\epsilon \\sim \\mathrm{Labpace}(0, 1)\\)\n\n\nset.seed(12345)\n\ndata &lt;- tibble(\n  x = runif(1000, 0, 8),\n  epsilon = rmutil::rlaplace(1000, 0, 1)\n) |&gt; \n  mutate(\n    y_truth = 5 + 4*sqrt(9 * x)*as.numeric(x&lt;2) + as.numeric(x&gt;=2)*(abs(x-6)^(2)),\n    y = y_truth + epsilon\n  )\n\n\nデータの確認（gamが優秀）\n\n\ndata |&gt; \n  ggplot(aes(x, y)) +\n  geom_point(alpha = 0.3)+\n  geom_line(\n    data = tibble(\n      x = seq(0, 8, 0.1), \n      y = 5 + 4*sqrt(9 * x)*as.numeric(x&lt;2) + as.numeric(x&gt;=2)*(abs(x-6)^(2))\n    ), \n    aes(x, y), \n    color = \"black\"\n  )+\n  geom_smooth(method = 'gam', formula = y ~ s(x, bs = 'cs'), se = FALSE)\n\n\n\n\n\n\n\n\n\ncross-fitの準備\nデータを5分割\n\n\ndf &lt;- \n  vfold_cv(data, v = 5) |&gt; \n  mutate(\n    train = map(splits, analysis),\n    test = map(splits, assessment)\n  )\n\ndf\n\n#  5-fold cross-validation \n# A tibble: 5 × 4\n  splits            id    train              test              \n  &lt;list&gt;            &lt;chr&gt; &lt;list&gt;             &lt;list&gt;            \n1 &lt;split [800/200]&gt; Fold1 &lt;tibble [800 × 4]&gt; &lt;tibble [200 × 4]&gt;\n2 &lt;split [800/200]&gt; Fold2 &lt;tibble [800 × 4]&gt; &lt;tibble [200 × 4]&gt;\n3 &lt;split [800/200]&gt; Fold3 &lt;tibble [800 × 4]&gt; &lt;tibble [200 × 4]&gt;\n4 &lt;split [800/200]&gt; Fold4 &lt;tibble [800 × 4]&gt; &lt;tibble [200 × 4]&gt;\n5 &lt;split [800/200]&gt; Fold5 &lt;tibble [800 × 4]&gt; &lt;tibble [200 × 4]&gt;"
  },
  {
    "objectID": "blog/posts/Manual-Superlearner.html#手動でsuper-learner",
    "href": "blog/posts/Manual-Superlearner.html#手動でsuper-learner",
    "title": "手動でSuperlearner",
    "section": "手動でSuper learner",
    "text": "手動でSuper learner\n\nCross-fitで各モデルの予測値を計算\n\n\nres &lt;- \n  df |&gt; \n  mutate(\n    model_earth = map(\n      train, \\(data) \n      earth(y ~ x, degree = 2, penalty = 3, nk = 21, pmethod = \"backward\", data = data)\n    ),\n    model_lm = map(\n      train, \\(data) \n      lm(y ~ poly(x, degree = 4), data = data)\n    ),\n    pred_earth = map2(model_earth, test, \\(x, y) predict(x, newdata = y)[,1]),\n    pred_lm = map2(model_lm, test, \\(x, y) predict(x, newdata = y))\n  )\n\n\n各モデルの予測値を独立変数、アウトカムを目的変数とした回帰モデルを、Non-negative least squaresにより推定\nパフォーマンスの良いモデルにより大きい重みがつくように、重みを推定\n\n\nweight &lt;-\n  nnls::nnls(\n    A = \n      res |&gt; \n      select(pred_earth, pred_lm) |&gt; \n      unnest(cols = c(pred_earth, pred_lm)) |&gt; \n      as.matrix(), \n    b = data$y\n  ) |&gt; \n  pluck('x')\n\nweight\n\n[1] 0.2467513 0.5523356\n\n# weightを、足して1になるように基準化\nweight_normalized &lt;- weight / sum(weight)\nweight_normalized\n\n[1] 0.3087915 0.6912085"
  },
  {
    "objectID": "blog/posts/Manual-Superlearner.html#予測値の計算",
    "href": "blog/posts/Manual-Superlearner.html#予測値の計算",
    "title": "手動でSuperlearner",
    "section": "予測値の計算",
    "text": "予測値の計算\n\n全データを用いて、各モデルの予測値を計算\n先ほど推定した重みを用いて、各モデルの予測値を組み合わせた予測値を計算\n\n\n# サンプル全体での予測値を計算\nmodel1 &lt;- earth(y ~ x, degree = 2, penalty = 3, nk = 21, pmethod = \"backward\", data = data)\nmodel2 &lt;- lm(y ~ poly(x, degree = 4), data = data)\n\nresult &lt;- \n  data |&gt; \n  mutate(\n    pred1 = predict(model1)[,1],\n    pred2 = predict(model2)\n  ) |&gt; \n  mutate(\n    pred_sl = weight_normalized[1]*pred1 + weight_normalized[2]*pred2\n  )\n\nresult\n\n# A tibble: 1,000 × 7\n       x epsilon y_truth      y pred1 pred2 pred_sl\n   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n 1  5.77  -1.86     5.05  3.19   5.21  4.84    4.96\n 2  7.01   1.16     6.01  7.18   6.36  6.57    6.51\n 3  6.09  -4.30     5.01  0.712  5.16  5.00    5.05\n 4  7.09  -0.353    6.19  5.83   6.56  6.75    6.69\n 5  3.65   0.465   10.5  11.0    9.98 11.0    10.7 \n 6  1.33  -0.677   18.8  18.2   20.0  19.5    19.7 \n 7  2.60  -1.05    16.6  15.5   17.5  16.8    17.0 \n 8  4.07   0.535    8.71  9.25   8.73  8.90    8.85\n 9  5.82   7.05     5.03 12.1    5.20  4.85    4.96\n10  7.92   2.30     8.68 11.0    8.52  8.20    8.30\n# ℹ 990 more rows\n\n\n\n結果のプロット\n\n\nresult |&gt; \n  pivot_longer(\n    cols = c(pred1, pred2, pred_sl), \n    names_to = 'model', \n    values_to = 'prediction'\n  ) |&gt; \n  ggplot(aes(x, y)) +\n  geom_point(alpha = 0.05)+\n  geom_line(aes(x, prediction, color = model))+\n  geom_line(\n    data = tibble(\n      x = seq(0, 8, 0.1), \n      y = 5 + 4*sqrt(9 * x)*as.numeric(x&lt;2) + as.numeric(x&gt;=2)*(abs(x-6)^(2))\n    ), \n    aes(x, y), \n    color = \"black\"\n  )"
  },
  {
    "objectID": "blog/posts/Manual-Superlearner.html#sl3でsuper-learner",
    "href": "blog/posts/Manual-Superlearner.html#sl3でsuper-learner",
    "title": "手動でSuperlearner",
    "section": "sl3でSuper learner",
    "text": "sl3でSuper learner\n\ntask &lt;- sl3_Task$new(\n  data = data, outcome = \"y\", covariates = \"x\", outcome_type = 'continuous', folds = 5\n)\n\nsl_lib &lt;- \n  Lrnr_sl$new(\n    learners = Stack$new(\n      Lrnr_earth$new(degree = 4), \n      Lrnr_gam$new(),\n      Lrnr_mean$new(),\n      Lrnr_xgboost$new(nrounds = 100, max_depth = 3, eta = 0.3),\n      Lrnr_bartMachine$new(serialize = TRUE)\n    ),\n    metalearner = Lrnr_nnls$new(convex = TRUE)\n  )\n\nfit &lt;- sl_lib$train(task)\n\nserializing in order to be saved for future R sessions...done\nserializing in order to be saved for future R sessions...done\nserializing in order to be saved for future R sessions...done\nserializing in order to be saved for future R sessions...done\nserializing in order to be saved for future R sessions...done\nserializing in order to be saved for future R sessions...done\n\nfit\n\n[1] \"Cross-validated risk:\"\nKey: &lt;learner&gt;\n                           learner coefficients       MSE        se   fold_sd\n                            &lt;fctr&gt;        &lt;num&gt;     &lt;num&gt;     &lt;num&gt;     &lt;num&gt;\n1: Lrnr_earth_4_3_backward_0_1_0_0  0.000000000  2.439980 0.1491178 0.5590197\n2:       Lrnr_gam_NULL_NULL_GCV.Cp  0.655902673  2.156928 0.1368631 0.5163999\n3:                       Lrnr_mean  0.000000000 30.590277 1.0519860 2.2451721\n4:        Lrnr_xgboost_100_1_3_0.3  0.004810931  2.437071 0.1515221 0.5109703\n5:           Lrnr_bartMachine_TRUE  0.339286396  2.220592 0.1429715 0.4626009\n   fold_min_MSE fold_max_MSE\n          &lt;num&gt;        &lt;num&gt;\n1:     1.969880     3.393874\n2:     1.767867     3.055205\n3:    27.834174    33.991665\n4:     2.098941     3.308421\n5:     1.856710     3.012823\n\n\n\nresult |&gt; \n  mutate(\n    pred_sl3 = fit$predict(task = task)\n  ) |&gt; \n  pivot_longer(\n    cols = c(pred1, pred2, pred_sl, pred_sl3), \n    names_to = 'model', \n    values_to = 'prediction'\n  ) |&gt; \n  ggplot(aes(x, y)) +\n  geom_point(alpha = 0.05)+\n  geom_line(\n    data = tibble(\n      x = seq(0, 8, 0.1), \n      y = 5 + 4*sqrt(9 * x)*as.numeric(x&lt;2) + as.numeric(x&gt;=2)*(abs(x-6)^(2))\n    ), \n    aes(x, y), \n    color = \"black\"\n  )+\n  geom_line(aes(x, prediction, color = model))"
  },
  {
    "objectID": "blog/posts/r_installation_with_rig.html",
    "href": "blog/posts/r_installation_with_rig.html",
    "title": "バージョンを指定したRのインストール",
    "section": "",
    "text": "わたし：Macユーザーで、MacのソフトウェアはすべてHomebrewで管理\nある日：Rのバージョンが4.4.0になっていたのに気づかずにアップデートしてしまった\nやりたいこと：Rのバージョンを4.3.3に戻したい"
  },
  {
    "objectID": "blog/posts/r_installation_with_rig.html#背景",
    "href": "blog/posts/r_installation_with_rig.html#背景",
    "title": "バージョンを指定したRのインストール",
    "section": "",
    "text": "わたし：Macユーザーで、MacのソフトウェアはすべてHomebrewで管理\nある日：Rのバージョンが4.4.0になっていたのに気づかずにアップデートしてしまった\nやりたいこと：Rのバージョンを4.3.3に戻したい"
  },
  {
    "objectID": "blog/posts/r_installation_with_rig.html#こんなのを見つけた",
    "href": "blog/posts/r_installation_with_rig.html#こんなのを見つけた",
    "title": "バージョンを指定したRのインストール",
    "section": "こんなのを見つけた",
    "text": "こんなのを見つけた\nイマドキRのインストール事情？ ～rig、pak、p3m～\nどうやらrigというものを使えば、Rのバージョンを指定してインストールできるらしい。"
  },
  {
    "objectID": "blog/posts/r_installation_with_rig.html#使ってみる",
    "href": "blog/posts/r_installation_with_rig.html#使ってみる",
    "title": "バージョンを指定したRのインストール",
    "section": "使ってみる",
    "text": "使ってみる\ngithubを見ながらインストール。\nまずはrigをhomebrewからインストール\nbrew tap r-lib/rig\nbrew install --cask rig \n次に、rigを使ってインストールしたいRのバージョンを指定してインストール\nrig add 4.3.3\nアンインストールしたい時はこう\nrig rm 4.3.3\nかんたん！"
  },
  {
    "objectID": "profile.html",
    "href": "profile.html",
    "title": "Profile",
    "section": "",
    "text": "2023年4月 - 現在：独立行政法人日本学術振興会 特別研究員（DC1）\n2021年4月 - 現在：東京大学社会科学研究所 附属社会調査・データアーカイブ研究センター 学術専門職員"
  },
  {
    "objectID": "profile.html#職歴",
    "href": "profile.html#職歴",
    "title": "Profile",
    "section": "",
    "text": "2023年4月 - 現在：独立行政法人日本学術振興会 特別研究員（DC1）\n2021年4月 - 現在：東京大学社会科学研究所 附属社会調査・データアーカイブ研究センター 学術専門職員"
  },
  {
    "objectID": "profile.html#学歴",
    "href": "profile.html#学歴",
    "title": "Profile",
    "section": "学歴",
    "text": "学歴\n\n2023年4月 - 現在：東京大学大学院教育学研究科比較教育社会学コース 博士課程\n2021年4月 - 2023年3月：東京大学大学院教育学研究科 比較教育社会学コース 修士課程\n2019年4月 - 2021年3月：東京大学教育学部 比較教育社会学コース"
  }
]